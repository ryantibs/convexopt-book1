\chapter{Duality correspondences}
\label{chap:duality_correspondences}

In this chapter we explore various topics which connect to our treatment of 
Lagrangian duality and the KKT conditions in the last two chapters.

\section{Dual norms}
\label{sec:dual_norms}

Given a norm $\|\cdot\|$ on $\R^d$, its \emph{dual norm} is denoted
$\|\cdot\|_*$, and defined by 
\index{dual norm}  
\begin{equation}
\label{eq:dual_norm}
\|y\|_* = \sup_{\|x\| \leq 1} \, y^\T x.
\end{equation}
It is not hard to check that the dual norm $\|\cdot\|_*$ is itself a norm on
$\R^d$, that is, it satisfies the triangle inequality, absolute homogeneity, and
positive definiteness (Exercise \ref{ex:dual_norm_check}). 

A key property to record: the dual of the dual norm is the original norm,
$\|\cdot\|_{**} = \|\cdot\|$. This is traditionally verified using the
Hahn-Banach theorem (Exercise \ref{ex:dual_norm_dual1}), but can also be
derived using the theory of Lagrangian duality (Exercise
\ref{ex:dual_norm_dual2}), or even convex conjugacy (Exercise
\ref{ex:dual_norm_dual3}). In any case (however it is proved), this fact allows 
us to write    
\begin{equation}
\label{eq:norm_dual2}
\|x\| = \sup_{\|y\|_* \leq 1} \, x^\T y.
\end{equation}

Directly from \eqref{eq:dual_norm}, we may infer that $x^\T y \leq \|x\|$ for
all $x$ and $\|y\|_* \leq 1$, or     
\index{H{\"o}lder's inequality}
\begin{equation}
\label{eq:holder_inequality}
x^\T y \leq \|x\| \|y\|_*, \quad \text{for all $x,y$}.
\end{equation}
This may be seen as a generalization of \emph{H{\"o}lder's inequality} for
$\ell_p$ norms. When does equality hold in \eqref{eq:holder_inequality}? The
representations \eqref{eq:dual_norm}, \eqref{eq:norm_dual2}, combined with the
fact about subgradients of norms in \eqref{eq:norm_subgradients}, lead to the 
following equivalent conditions (Exercise \ref{ex:dual_norm_subgradients}).     

\index{norm!subgradients}
\index{dual norm!subgradients}
\begin{Theorem}
\label{thm:dual_norm_subgradients}
For any norm $\|\cdot\|$ and its dual $\|\cdot\|_*$, and any $x \not= 0$ and $y 
\not= 0$, the following statements are equivalent:
\begin{enumerate}[label=(\roman*)]
\item $x^\T y = \|x\| \|y\|_*$;
\item $x/\|x\| \in \partial \|y\|_*$; 
\item $y/\|y\|_* \in \partial \|x\|$;
\item $x/\|x\| \in \argmax_{\|s\| \leq 1} \, y^\T s$; 
\item $y/\|y\|_* \in \argmax_{\|s\|_* \leq 1} \, x^\T s$. 
\end{enumerate}
\end{Theorem}

The next example confirms that \eqref{eq:holder_inequality} is indeed a strict 
generalization of H{\"o}lder's inequality for $\ell_p$ norms, as it shows the
dual of the $\ell_p$ norm is the $\ell_q$ norm for a conjugate pair $p,q$.      

\begin{Example}
The following are examples of dual norms for common norms, which can be verified
by direct calculation (Exercises \ref{ex:lp_norm_dual}--\ref{ex:trace_norm_dual}). 

\begin{enumerate}[label=\alph*., ref=\alph*]
\index{lp norm@$\ell_p$ norm!dual norm}
\item \parlab{xa:lp_norm_dual} 
For the $\ell_p$ norm $\|\cdot\|_p$ with $p \geq 1$, its dual norm is
$\|\cdot\|_q$ where $q$ satisfies $1/p + 1/q = 1$.  

\item \parlab{xa:scaled_euclidean_norm_dual}  
For the scaled Euclidean norm $\|\cdot\|_A$ in a given positive definite
matrix $A$, which recall is defined by \smash{$\|x\|_H = \sqrt{x^\T A x}$},
its dual norm is \smash{$\|\cdot\|_{A^{-1}}$}.  

\index{trace norm!dual norm}
\index{operator norm!dual norm}
\item \parlab{xa:trace_norm_dual}  
For the trace norm $\|\cdot\|_{\tr}$, its dual norm is the operator norm
$\|\cdot\|_{\op}$.   
\end{enumerate}
\end{Example}

Dual norms bear interesting connections to conjugacy. As already shown in
Example \parref{xa:norm_conjugate}, the conjugate of a norm is the
characteristic function of the dual norm unit ball:  
\index{norm!conjugate}
\begin{equation}
\label{eq:norm_conjugate}
(\|\cdot\|)^* = I_{\{u : \|u\|_* \leq 1\}}.
\end{equation}
Meanwhile, an arguably even simpler relationship holds for the squared norm; the
convex conjugate of one half times a norm squared is one half times its dual
norm squared (Exercise \ref{ex:norm_squared_conjugate}):       
\begin{equation}
\label{eq:norm_squared_conjugate}
\Big( \frac{1}{2} \|\cdot\|^2 \Big)^{\hspace{-2pt}*} = \frac{1}{2}
\|\cdot\|_*^2.   
\end{equation}
These facts are especially useful in the context of Fenchel duality, which we
cover next.  

\section{Fenchel duality}

For $f,g : \R^d \to \R$ and $A \in \R^{k \times d}$, consider the optimization
problem  
\begin{equation}
\label{eq:fenchel_primal}
\minimize_x \quad f(Ax) + g(x).
\end{equation}
Introducing an auxiliary variable (and equality constraint), this becomes
\[
\minimize_{x,z} \quad f(z) + g(x) \quad \st \quad z = Ax,
\]
whose Lagrangian is $L(x,z,u) = f(z) + g(x) + u^\T (z - Ax)$. Observe that  
\begin{align*}
\inf_{x,z} \, \Big\{ f(z) + g(x) + u^\T (z - Ax) \Big\} 
&= -\sup_z  \,\Big\{ -u^\T z - f(z) \Big\} - \sup_x \, \Big\{ u^\T Ax - g(x)  
  \Big\} \\ 
&= -f^*(-u) - g^*(A^\T u),
\end{align*}
where $f^*,g^*$ denote the convex conjugates of $f,g$, respectively. The
associated dual problem is thus
\index{Fenchel duality}
\begin{equation}
\label{eq:fenchel_dual}
\maximize_u \quad -f^*(-u) - g^*(A^\T u),
\end{equation}
which is often called the \emph{Fenchel dual} of
\eqref{eq:fenchel_primal}. Applying Slater's condition (Theorem
\ref{thm:slater_condition}) and the KKT stationarity condition
\eqref{eq:kkt_stationarity} now leads to the following result.   

\begin{Corollary}
\label{cor:fenchel_dual}
For convex functions $f,g$ such that there exists a point $x \in
\relint(\dom(g))$ and $Ax \in \relint(\dom(f))$, the problems 
\eqref{eq:fenchel_primal}, \eqref{eq:fenchel_dual} are strongly
dual. Also, the conditions  
\[
A^\T u \in \partial g(x), \quad u \in - \partial f(Ax)
\]
are sufficient for primal-dual optimality, and necessary under strong duality.   
\end{Corollary}

Fenchel duality provides a succinct relationship between primal and dual
problems, in terms of conjugates. This can often be useful in deriving dual
problems.

\begin{Example}
The following two examples can be derived directly from Fenchel duality
\eqref{eq:fenchel_primal}, \eqref{eq:fenchel_dual}, and facts about conjugates
of norms \eqref{eq:norm_conjugate}, \eqref{eq:norm_squared_conjugate}.

\begin{enumerate}[label=\alph*., ref=\alph*]
\item \parlab{xa:fenchel_norm}
For a norm $\|\cdot\|$ and tuning parameter $\lambda \geq 0$, the problem    
\[
\minimize_x \quad f(Ax) + \lambda \|x\| 
\]
has Fenchel dual 
\[
\maximize_u \quad -f^*(-u) \quad \st \quad \|A^\T u\|_* \leq \lambda.        
\]
This reproduces, as a special case, the lasso dual \eqref{eq:lasso_dual} (see
Exercise \ref{ex:lasso_fenchel_dual}). 

\item \parlab{xa:fenchel_norm_squared}
Again for a norm $\|\cdot\|$ and parameter $\lambda \geq 0$, the problem   
\[
\minimize_x \quad f(Ax) + \frac{\lambda}{2} \|x\|^2
\]
has Fenchel dual 
\[
\maximize_u \quad -f^*(-u) - \frac{1}{2\lambda} \|A^\T u\|_*^2. 
\]
This reproduces, as a special case, the SVM dual \eqref{eq:svm_dual} (see
Exercise \ref{ex:svm_fenchel_dual}). 
\end{enumerate}
\end{Example}

\section{Dual cones*}
\label{sec:dual_cones}

In this section, we examine dual cones. Recall (as studied in Chapter
\ref{sec:cones}), a cone is a set $K$ such that $x \in K \implies t x \in K$,
for all $t \geq 0$. A convex cone $K$ is a cone that is also convex, which can
be characterized by the property: $x, y \in K \implies s x + t y \in K$, for all
$s, t \geq 0$. Now, given a cone $K$, its \emph{dual cone} is defined as
\label{dual cone}
\begin{equation}
\label{eq:dual_cone}
K^* = \{ y : y^\T x \geq 0, \; x \in K \}.
\end{equation}
It is straightforward to check that $K^*$ is always a closed convex cone,
regardless of the convexity of $K$. Furthermore, if $K$ is itself a closed
convex cone, then the dual of its dual cone is 
\begin{equation}
\label{eq:dual_cone_dual}
K^{**} = K,
\end{equation}
which will be verified shortly, when we discuss polarity. In general, we can
interpret elements of $K^*$ as normal vectors to halfspaces containing $K$,
since $y \in K^* \iff \{ x : y^\T x \geq 0 \} \supseteq K$. Figure
\ref{fig:dual_cone} gives an illustration.      

\begin{Example}
The following are examples of duals of common convex cones. The first two are
straightforward to verify, and the third is verified in Exercise
\ref{ex:psd_cone_self_dual}. 

\begin{enumerate}[label=\alph*., ref=\alph*]
\item For a linear subspace $L$, its dual cone is $L^* = L^\perp$, the
  orthogonal complement of $L$; in other words, for a matrix $A$, it holds that 
  $(\nul(A))^* = \row(A)$.  
  
\item For the norm cone $K = \{ (x,t) \in \R^d \times \R : \|x\| \leq t \}$ in a
  norm $\|\cdot\|$, its dual cone is 
  \index{norm!cone}  
  \[
  K^* = \{ (y,s) \in \R^d \times \R : \|y\|_* \leq s \},
  \]
  the norm cone in the dual norm $\|\cdot\|_*$.

\index{positive semidefinite cone!dual cone}
\item For the positive semidefinite cone $K = \SS_+^d = \{X \in \SS^d : X
  \succeq 0 \}$, it is its own dual cone, $K^* = \SS_+^d$. Such cones are called
  \emph{self-dual}.  
\end{enumerate}
\end{Example}

\begin{figure}[tb]
\centering
\includegraphics[width=0.7\textwidth]{fig/dual_cone.pdf}
\caption{A cone $K$ and its negative dual cone $-K^*$ (equivalently, its polar
  cone $K^\circ$), which can be easier to visualize. The set $-K^*$ is
  characterized by the normal vectors $y$ to all halfspaces $\{y : y^\T x \leq
  0\}$ containing $K$.}     
\label{fig:dual_cone}
\end{figure}

\section{Dual polyhedra*}
\label{sec:dual_polyhedra}

Recall (as covered in Chapter \ref{sec:polyhedra}), a polyhedron is an
intersection of halfspaces, $P = \{x : Ax \leq b\}$, for a matrix-vector pair 
$A,b$ of compatible dimensions. The \emph{dual polyhedron} to $P$ is defined as    
\index{polyhedron!dual polyhedron}
\index{polytope!dual polytope}
\begin{equation}
\label{eq:dual_polyhedron}
P^* = \{ y : y^\T x \leq 1, \; x \in P \}.
\end{equation}
The set $P^*$ is indeed a polyhedron (an intersection of halfspaces),  
% For example, Corollary 19.2.2 of Rockafellar (1970)
although this may be nonobvious at face value. The developments below will make
it clear that $P^*$ is polyhedral in the case that $P$ is bounded. The unbounded
case is discussed in Exercise \ref{ex:minkowski_weyl}. 
 
Let us assume henceforth that $P$ is bounded polyhedron, which recall we call a
polytope, with $0 \in \interior(P)$. While Theorem \ref{thm:hv_representation},
the main representation theorem for polytopes, can be established directly using 
algebraic arguments, we demonstrate in this section how it can be explained from 
the perspective of polyhedral duality. 

To this end, suppose that we have the H-representation $P = \{ x : Ax \leq
b\}$. Then by definition, 
\begin{align}
\nonumber
P^* &= \{ y : y^\T x \leq 1, \; \text{for all $x$ with $Ax \leq b$} \} \\
\label{eq:v_representation_dual}
&= \bigg\{ \sum_{i=1}^m t_i \frac{a_i}{b_i} : \text{$t_i \geq 0$, for
  $i=1,\dots,m$, and $\sum_{i=1}^m t_i = 1$} \bigg\}.
\end{align}
The second step above can be verified using Farkas' lemma (Exercise
\ref{ex:v_representation_dual}), where $a_1,\dots,a_m \in \R^d$ denote the rows
of $A$, and $b_1,\dots,b_m > 0$ (as $0 \in \interior(P)$). Defining $y_i =
a_i/b_i$, $i = 1,\dots,m$, we have thus produced the dual V-representation $P^*
= \conv\{y_1,\dots,y_m\}$.       

Suppose instead that we have the V-representation $P =
\conv\{x_1,\dots,x_n\}$. Again by definition,  
\begin{align}
\nonumber
P^* &= \big\{ y : y^\T x \leq 1, \; x \in \conv\{x_1,\dots,x_n\} \big\} \\
\label{eq:h_representation_dual}
&= \{ y : y^\T x_i \leq 1, \; i = 1,\dots,n \},
\end{align}
where the second step holds by convexity (Exercise
\ref{ex:h_representation_dual}). In other words, we have produced the dual
H-representation $P^* = \{ y : Xy \leq 1\}$, where $X$ is the matrix with rows
$x_1,\dots,x_n \in \R^d$.    

We have therefore established that the H-representation and V-representation for
polytopes are dual to one another: facets (halfspaces in an H-representation) of
$P$ correspond to vertices (points in a V-representation) of $P^*$, and vice 
versa. This dual relationship can actually be used as a key step in proving
Theorem \ref{thm:hv_representation} in the first place; see Exercise 
\ref{ex:hv_representation_dual}.    

There is in fact even more to be said about the relationship between $P$ and
$P^*$: not only vertices and facets, but all faces of $P$ and $P^*$ obey a
precise one-to-one correspondence. 

\begin{Theorem}
\label{thm:face_duality}
Let $P \subseteq \R^d$ be a polytope with $0 \in \interior(P)$, and let $P^*$ be
its dual polytope in \eqref{eq:dual_polyhedron}. Define a map $\Psi$ from faces
$\cF(P)$ of $P$ to faces $\cF(P^*)$ of $P^*$ by  
\[
\Psi(F) = \{ y \in P^* : y^\T x = 1, \; x \in F \}.
\]
Then $\Psi$ is a bijection. Furthermore, it is inclusion-reversing: $F \subseteq
G \implies \Psi(F) \supseteq \Psi(G)$, and it satisfies $\dim(F) + \dim(\Psi(F))
= d-1$ for all $F \in \cF(P)$. 
\end{Theorem}

This theorem, whose proof is outlined in Exercise \ref{ex:face_duality}, gives
us a rich way of understanding the relation between a polytope $P$ and its dual
$P^*$. Duality provides a representation map $\Psi$, which maps vertices of $P$
to facets of $P^*$, edges (1-faces) of $P$ to ridges ($(d-2)$-faces) of $P^*$,
and so on. In short, the  entire facial structure of $P$ is determined by $P^*$,
and vice versa. A canonical example of a dual polytope pair is given by the
$\ell_1$ and $\ell_\infty$ balls, explored in Exercise
\ref{ex:l1_linf_ball_duality}.    

\section{Polar sets*}
\label{sec:polar_sets}

Given a set $C$, its \emph{polar set} is defined by 
\index{polar set}
\begin{equation}
\label{eq:polar_set}
C^\circ = \{ y : y^\T x \leq 1, \; x \in C \}.
\end{equation}
For a polyhedron $P$, its polar set is simply another name for its dual
polyhedron \eqref{eq:dual_polyhedron}, $P^\circ = P^*$. Moreover, for a cone
$K$, its polar set is in fact the negative dual cone \eqref{eq:dual_cone},
$K^\circ = -K^*$: clearly we have $-K^* \subseteq K^\circ$, and for the opposite
inclusion, take any $y \in K^\circ$, $x \in K$, $t>0$, then observe that $y^\T x  
\leq 1 \implies y^\T (tx) \leq 1 \iff y^\T x \leq 1/t$, thus sending $t \to
\infty$ gives $y^\T x \leq 0$.          

One can check that the polar $C^\circ$ is always a closed convex set. When $C$
itself a closed convex set containing 0, the polar of its polar set is
\begin{equation}
\label{eq:polar_set_polar}
C^{\circ\circ} = C.
\end{equation}
As polarity generalizes duality for both polyhedra and cones, the above fact
implies $P^{**} = P$ for a polyhedron containing the origin, and also $K^{**} =
K$ for a closed convex cone, as claimed earlier in
\eqref{eq:dual_cone_dual}. The fact \eqref{eq:polar_set_polar} can be verified
using the separating hyperplane theorem (Exercise \ref{ex:polar_set_polar}). 

\begin{Example}
The \emph{tangent cone} to a set $C$ at a point $x \in C$ is defined as 
\index{normal cone!dual cone}
\index{tangent cone}
\begin{equation}
\label{eq:tangent_cone}
T_C(x) = \closure \Big\{ y : \text{there exists $\beta>0$ such that $x + \alpha
  y \in C$, for all $\alpha \in [0,\beta]$} \Big\}.  
\end{equation}
This is a closed cone, but need not be convex in general. However, if $C$ is
convex then $T_C(x)$ is convex too, and in this case it is actually the polar to
the normal cone, $(N_C(x))^\circ = T_C(x)$. Exercise \ref{ex:tangent_cone}
explores these and related details.     
\end{Example}

Polarity and conjugacy share a connection for characteristic functions. By
direct inspection, the conjugate of $I_C$, which recall (Example
\parref{xa:characteristic_function_conjugate}) is the support function $h_C =
I^*_C$, satisfies        
\[
h_C(y) \leq 1 \iff y \in C^\circ.
\]
Moreover, if $C$ is a cone, then we can see from the above that $h_C$ can
only take the values 0 and $\infty$ (since $h_C(y) \leq 1 \implies h_C(ty) = t
h_C(y) \leq 1$ for all $y \in C^\circ$ and $t>0$); more precisely, $h_C$ is 0 
on $C^\circ$ and $\infty$ otherwise, which leads to 
\begin{equation}
\label{eq:support_function_cone}
h_C = I_{C^\circ}.
\end{equation}
That is, the conjugate (support function) of the characteristic function of a
cone is the characteristic function of its polar cone. This fact will be useful
in derivations involving conic duality. 
  
\section{Conic duality*}
\label{sec:conic_duality}

We are now prepared to develop a theory of conic duality, which recall was
alluded to in Chapter \ref{sec:sdp_duality} (in particular, Corollary
\ref{cor:slater_sdp}). We will study an optimization problem said to be in
\emph{conic form}:     
% Note: convenient to write it this way because we are effectively separating
% out affine and nonaffine inequality constraints ahead of time ...
\begin{equation}
\label{eq:primal_conic}
\begin{alignedat}{2}
&\minimize_{x \in D} \quad && f(x) \\ 
&\st \quad && h(x) \leq_K 0 \\
& && Ax \leq b,
\end{alignedat}
\end{equation}
where $D \subseteq \R^d$ (viewed as the intersection of effective domains of
the functions in the optimization problem, which is typically implicit, but is
explicit here for convenience in what follows), $f : D \to \R$, $h : D \to 
\R^m$, $K \subseteq \R^m$ is a cone, $A \in \R^{k \times d}$, and $b \in
\R^k$. The relation $\leq_K$ in \eqref{eq:primal_conic} is defined by
\[
a \leq_K b \iff b-a \in K.
\]
When $K$ is a \emph{regular} cone $K$, which is a closed convex cone with
nonempty interior, that is \emph{pointed}: $K \cap -K = \{0\}$, one can check
that $\leq_K$ is indeed a partial ordering.\footnote{This means that it is
  reflexive: $a \leq_K a$, for all $a$; antisymmetric: $a \leq_K b$ and $b
  \leq_K a$ implies $a = b$; and transitive: $a \leq_K b$ and $b \leq_K c$
  implies $a \leq_K c$.}    
Notable examples are given by $K = \R_+^d$, the nonnegative orthant, where
$\leq_K$ reduces to the usual (coordinatewise) ordering $\leq$ on vectors, and
$K = \SS_+^d$, the positive semidefinite cone, where $\leq_K$ reduces to the
positive semidefinite ordering $\preceq$ on matrices.

\index{cone program}
As $h(x) \leq_K 0 \iff -h(x) \in K$, note that we can take $f$ to be linear and
$h$ to be affine in order to recover the cone program in \eqref{eq:cone_program}
from Chapter \ref{sec:cone_programs}. Thus, the general problem in conic form 
\eqref{eq:primal_conic} encompasses cone programs, and hence SDPs. Furthermore,
when we take $K = \R_+^d$, then problem \eqref{eq:primal_conic} is able to
recover standard convex programs, which were the focus of Chapter
\ref{chap:lagrangian_duality}. In this sense, the strong duality theorem below
both generalizes the previous one for standard convex programs (Theorem
\ref{thm:slater_condition}), and implies the previously claimed result for SDPs
(Corollary \ref{cor:slater_sdp}).

Defining \smash{$g(u,v) = \inf_{x \in D} \, \{ f(x) + u^\T h(x) + v^\T (Ax - b) 
  \}$}, the dual of \eqref{eq:primal_conic} is  
\begin{equation}
\label{eq:dual_conic}
\begin{alignedat}{2}
&\maximize_{u,v} \quad && g(u,v) \\ 
&\st \quad && u \geq_{K^*} 0 \\
& && v \geq 0,
\end{alignedat}
\end{equation}
which can be verified using familiar calculations in Lagrangian duality, along
with property \eqref{eq:support_function_cone} (Exercise
\ref{ex:dual_conic}). The following generalizes Slater's condition to problems
of conic form.  

\index{Slater's condition}
\index{strong duality}
\begin{Theorem}
\label{thm:slater_conic}
Consider the optimization problem \eqref{eq:primal_conic}. Assume the following.           

\begin{enumerate}[label=(\roman*)]
\item The set $D$ is convex, $K$ is a regular (closed, convex, pointed, with a
  nonempty interior) cone, $f : D \to \R$ is convex, and $h : D \to \R^m$ is
  $K$-convex, which means          
  \[
  h \big(t x + (1-t) y \big) \leq_K t h(x) + (1-t) h(y),  \quad \text{for all $x,y
  \in D$ and $t \in [0,1]$}.
  \]

\item There exists $x \in \relint(D)$, such that $Ax \leq b$ and $h(x) <_K 0
  \iff -h(x) \in \interior(K)$.
\end{enumerate}

Then strong duality holds: the optimal values $f^\star$ in
\eqref{eq:primal_conic} and $g^\star$ in the dual \eqref{eq:dual_conic} satisfy
$f^\star = g^\star$. Furthermore, when this common optimal value is finite, then
a dual solution exists (the optimal value in \eqref{eq:dual_conic} is attained).      
\end{Theorem}

This result is an extension and refinement of Slater's condition from Theorem 
\ref{thm:slater_condition}, and its proof proceeds similarly, outlined in
Exercises \ref{ex:convex_theorem_alternatives_conic} and \ref{ex:slater_conic}.  
Recall, applying Theorem \ref{thm:slater_conic} to SDPs yields Corollary 
\ref{cor:slater_sdp}, once we realize that we can learn from applying it to the
primal and the dual separately (the dual of the dual SDP being the primal SDP). 
A similar conclusion can be reached for cone programs (with $f,h$ being linear)
which we cover in Exercise \ref{ex:slater_cone_program}.

\section{Minimax theorems*}
\label{sec:minimax_theorems}

Now we broaden our perspective and study a real-valued function of a block
variable $(x,y) \in X \times Y$, denoted $L : X \times Y \to \R$. This can be
viewed as a generalization of the Lagrangian function featured in convex duality 
and the KKT conditions, with $x$ and $y$ representing the primal and dual
variables, constrained to lie in subsets $X \subseteq \R^d$ and $Y \subseteq  
\R^p$, respectively. We will investigate conditions under which $L$ admits a 
saddle point. What distinguishes our study here from that in previous chapters
is that the function $L$ is not restricted to be linear in $y$ (all Lagrangians
are linear in $u,v$).        

To be clear, as before, a point \smash{$(\bar{x}, \bar{y}) \in X \times Y$} is
said to be a saddle point of $L$ provided that  
\index{saddle point}
\begin{equation}
\label{eq:saddle_point2}
L(\bar{x}, y) \leq L(\bar{x}, \bar{y}) \leq L(x, \bar{y}) \quad \text{for any
  $x \in X$ and any $y \in Y$}. 
\end{equation}
An equivalent definition is 
\begin{equation}
\label{eq:saddle_point3}
\sup_{y \in Y} \, L(\bar{x}, y) \leq L(\bar{x}, \bar{y}) \leq \inf_{x \in X}
L(x, \bar{y}). 
\end{equation}
Defining the functions
\begin{equation}
\label{eq:saddle_functions}
f(x) = \sup_{y \in Y} \, L(x,y) \quad \text{and} \quad 
g(y) = \inf_{x \in X} \, L(x,y),
\end{equation}
we observe directly that $g(y) \leq L(x,y) \leq f(x)$, for any $x \in X$ and any 
$y \in Y$, whereas \eqref{eq:saddle_point3} calls for \smash{$(\bar{x},
  \bar{y})$} to satisfy \smash{$g(\bar{y}) \geq  L(\bar{x}, \bar{y}) \geq
  f(\bar{x})$}. This leads to the following conclusion (a generalization of
Theorem \ref{thm:saddle_point_optimality} for Lagrangian functions). 

\begin{Theorem}
\label{thm:saddle_point_optimality2}
For any function $L : X \times Y \to \R$, a point \smash{$(\bar{x}, \bar{y}) \in
  X \times Y$} is a saddle point as in \eqref{eq:saddle_point2} if and only if 
\smash{$\bar{x}$} minimizes $f$ over $X$, \smash{$\bar{y}$} maximizes $g$ 
over $Y$, and \smash{$f(\bar{x}) = g(\bar{y})$}, with $f$ and $g$ defined as in   
\eqref{eq:saddle_functions}. 
\end{Theorem}

When \smash{$(\bar{x}, \bar{y}) \in X \times Y$} is a saddle point, the
statement \smash{$f(\bar{x}) = g(\bar{y})$} is equivalent to 
\begin{equation}
\label{eq:saddle_point4}
\inf_{x \in X} \, \sup_{y \in Y} \, L(x,y) = \sup_{y \in Y} \, \inf_{x \in X} \,
L(x,y).
\end{equation}
The existence of a saddle point is therefore, according to
\eqref{eq:saddle_point4}, the assertion that ``minimax'' (min over $x$ and max
over $y$) and ``maximin'' problems (max over $y$ and min over $y$) are one in
the same. Conditions which guarantee the existence of saddle points are
thus often called \emph{minimax theorems}. 

We have already seen minimax theorems in Chapters \ref{sec:slater_condition},
\ref{sec:sdp_duality}, and \ref{sec:conic_duality}: these are (successively more  
general) versions of Slater's condition, which guarantees strong duality---and 
hence a saddle point of the associated Lagrangian $L$---in convex programs.   

The first and arguably the most famous minimax theorem is known as \emph{von
  Neumann's minimax theorem}, which concerns bilinear functions, of the form
$L(x,y) = x^\T P y$ (and assumes that $X,Y$ are probability simplices). Below is
a more general minimax result, known as \emph{Sion's minimax theorem}, for 
functions that are convex in their first argument and concave in their second.      

\index{Sion's minimax theorem}
\begin{Theorem}
\label{thm:sion_minimax}
Let $X \subseteq \R^d$ and $Y \subseteq \R^p$ be compact and let $L : X \times Y
\to \R$ be continuous. Assume that $L(\cdot,y)$ is convex for each fixed $y \in
Y$ and $L(x,\cdot)$ is concave for each fixed $x \in X$. Then there exists a
saddle point \smash{$(\bar{x}, \bar{y}) \in X \times Y$} of $L$, and thus
\eqref{eq:saddle_point4} holds. 
\end{Theorem}

Theorem \ref{thm:sion_minimax} is more general than (say) Theorem
\ref{thm:slater_condition} because $L$ is allowed to be concave (not
necessarily linear) in the argument $y$ representing the dual variable; at the
same time, it is also less general in that the sets $X,Y$ are assumed to be 
compact. Its proof is developed in Exercise \ref{ex:sion_minimax}.   

\section{Existence of minima, revisited*}
\label{sec:duality_minima}
\index{optimization problem!existence of solutions}

We once again revisit the existence of solutions in convex optimization
problems, now through the lens of duality. The refined form of Slater's
condition given in Theorem \ref{thm:slater_conic} (the conic formulation aside) 
showed that certain strict feasibility conditions on the primal problem imply
the existence of dual solutions (attainment of the dual optimal
value). Likewise, for a problem in which the dual of the dual is the primal, we
can apply Slater's condition to the dual problem in order to conclude the
existence of primal solutions (this was done for SDPs in Exercise
\ref{ex:lp_sdp_differences} parts a and b, and for cone programs in Exercise
\ref{ex:slater_cone_program}). 

Motivated to derive and extend the results in Theorem \ref{thm:existence_glms}
of Chapter \ref{sec:maximum_likelihood} on generalized linear models (GLMs), we
will study a problem of the form       
\begin{equation}
\label{eq:primal_legendre}
\minimize_\beta \quad f(X\beta) + h_C(\beta),
\end{equation}
where $f : \R^n \to (-\infty,\infty]$ is closed and convex, $X \in \R^{n \times
  d}$, and \smash{$h_C(\beta) = \sup_{z \in C} \, \beta^\T z$} is the support
function of a closed, convex, and nonempty set $C \subseteq \R^d$. The Fenchel
dual of \eqref{eq:primal_legendre} is
\begin{equation}
\label{eq:dual_legendre}
\maximize_u \quad -f^*(-u) \quad \st \quad X^\T u \in C,
\end{equation}
where we have used the fact that $h^*_C = I_C$ (by Example
\parref{xa:support_function_conjugate}). It is straightforward to check (again
using Fenchel duality, the fact that $f^{**} = f$ by
\eqref{eq:double_conjugate_identity}, and that $I^*_C = h_C$ by Example
\parref{xa:characteristic_function_conjugate}) that the dual of
\eqref{eq:dual_legendre} is \eqref{eq:primal_legendre}. Therefore, to reiterate
our strategy, we will investigate Slater's condition for
\eqref{eq:dual_legendre} in order to assert existence of solutions in
\eqref{eq:primal_legendre}.

To this end, let us assume that $f$ is of Legendre type (essentially smooth and
essentially strictly convex, as in Chapter \ref{sec:conjugate_smoothness}), as
this will enable us to easily relate the domain of its conjugate $f^*$ to $f$.
In particular, by Theorem \ref{thm:conjugate_legendre}, the gradient map $\nabla
f$ is a bijection from $\interior(\dom(f))$ to $\interior(\dom(f^*))$, which
means $\interior(\dom(f^*)) = \interior(\ran(\nabla f))$, where $\ran(\nabla f)$
denotes the range of $\nabla f$. Now, Slater's condition for
\eqref{eq:dual_legendre} requires the existence of $-u \in \interior(\ran(\nabla
f))$ and $X^\T u \in \relint(C)$, or
\begin{equation}
\label{eq:slater_dual_legendre}
\interior(\ran(\nabla f)) \cap (X^\T)^{-1} (\relint(-C)) \not= \emptyset,
\end{equation}
where $(X^\T)^{-1}$ denotes the inverse image under $X$. By Theorem
\ref{thm:slater_conic}, the condition \eqref{eq:slater_dual_legendre} guarantees
strong duality between \eqref{eq:primal_legendre}, \eqref{eq:dual_legendre}. To
ensure the existence of a primal solution, it remains to ensure that the optimal
dual value is finite, which happens if and only if the primal is feasible. This
leads us to the following result.

\begin{Theorem}
\label{thm:duality_minima}
Consider the optimization problem \eqref{eq:primal_legendre}, assumed to be
feasible, where $f$ is a closed and convex function of Legendre type, and $C$ is
a closed, convex, and nonempty set. Then, under condition
\eqref{eq:slater_dual_legendre}, a solution exists.
\end{Theorem}

To specialize to GLMs, where $X \in \R^{n \times d}$ is the feature matrix, we
take $f$ to be of the form  
\begin{equation}
\label{eq:glm_loss}
f(\eta) = -y^\T \eta + \Psi(\eta),
\end{equation}
where $y \in \R^n$ is the response vector, and $\Psi : \R^n \to (-\infty,
\infty]$ is the log-partition function, assumed to be itself of Legendre
type. Recall that for $f(X\beta)$ to represent the negative log likelihood in
linear regression, we take \smash{$\Psi(\eta) = \sum_{i=1}^n \eta_i^2/2$}; for
logistic regression, we take \smash{$\Psi(\eta) = \sum_{i=1}^n
  \log(1+e^{\eta_i})$}; and for Poisson regression, we take \smash{$\Psi(\eta) =
  \sum_{i=1}^n e^{\eta_i}$}. We thus see that \eqref{eq:primal_legendre} can 
represent regularized maximum likelihood in a GLM where the support function
$h_C$ acts as the regularizer. For example, if $C = \{ z : \|z\|_* \leq \lambda
\}$, then $h_C(\beta) = \lambda \|\beta\|$ is a norm penalty. Further, as
$\nabla f(\eta) = -y + \nabla \Psi(\eta)$, the condition
\eqref{eq:slater_dual_legendre} reduces to        
\begin{equation}
\label{eq:slater_dual_regularized_glm}
y \in \interior(\ran(\nabla \Psi)) + (X^\T)^{-1} (\relint(C)).
\end{equation}
We therefore arrive at the following conclusion for regularized GLMs. 

\index{generalized linear model!existence of solutions}
\begin{Corollary}
\label{cor:existence_regularized_glms}
Consider the regularized GLM problem \eqref{eq:primal_legendre}, assumed to be 
feasible, for $f$ as in \eqref{eq:glm_loss}, with $\Psi$ a closed and convex
function of Legendre type, and $C$ a closed, convex, and nonempty set. Then,
under condition \eqref{eq:slater_dual_regularized_glm}, a regularized GLM
solution exists.    

\setlength{\parindent}{\normalparindent}
Note in particular that when $C = \{0\}$, the condition in
\eqref{eq:slater_dual_glm} further reduces to  
\begin{equation}
\label{eq:slater_dual_glm}
y \in \interior(\ran(\nabla \Psi)) + \nul(X^\T),
\end{equation}
which guarantees the existence of an ordinary (unregularized) GLM solution.  
\end{Corollary}

For Poisson regression, recalling \smash{$\nabla_i \Psi(\eta) = e^{\eta_i}$}, $i
= 1,\dots,n$, we can see that $\interior(\ran(\nabla \Psi)) = \R_{++}^n$ is the 
positive orthant. In this case, it is clear that \eqref{eq:slater_dual_glm} is
equivalent to \eqref{eq:existence_poisson}, which verifies part (iii) of Theorem
\ref{thm:existence_glms}. For logistic regression, showing that
\eqref{eq:slater_dual_glm} is equivalent to \eqref{eq:existence_logistic} in
part (ii) of the same theorem requires a more nuanced argument, where a theorem
of the alternative (known as Stiemke's lemma) in needed to recast
\eqref{eq:slater_dual_glm}. This argument is outlined in Exercises
\ref{ex:stiemke_lemma}--\ref{ex:existence_logistic}. The case of a norm
penalty is studied in Exercise \ref{ex:existence_regularized_glms}. 

\SkipTocEntry\section*{Chapter notes}

This chapter has covered a wide range of topics related to duality, with the
title inspired by \cite{rockafellar1970convex} (Part III: Chapters 11--16). 
Fenchel duality is named after the developments in Fenchel's influential lecture
notes \cite{fenchel1951convex}, and was further developed by
\cite{rockafellar1963convex}. For more detail, we refer to
\cite{rockafellar1970convex} (Chapter 31). The geometry associated with convex 
duality---dual cones, dual polyhedra, and polar sets---is in it of itself a rich
topic. To learn more, we recommend \cite{rockafellar1970convex} (Chapters 14,
19), \cite{bertsekas2009convex} (Chapters 3.2, 3.3), \cite{grunbaum2003convex}  
(Chapters 3.1, 3.4), and \cite{ziegler1995lectures} (Chapters 2.3, 2.4).       

Our treatment of conic duality is based on \cite{bental2023convex} (Chapter
3.2), whose extension of Slater's condition for problems in conic form is
presented in our Theorem \ref{thm:slater_conic}. In addition, Exercises
\ref{ex:dubovitski_milutin_lemma}, \ref{ex:farkas_variations_conic}, 
\ref{ex:convex_theorem_alternatives_conic}, \ref{ex:slater_conic}, and   
\ref{ex:slater_cone_program} are all based the masterful treatment in
\cite{bental2023convex}.  

Sion's minimax theorem is named after \cite{sion1958general}, who extended  
von Neumann's minimax theorem \cite{vonneumann1928theorie} beyond bilinear   
functions. Also inspired by von Neumann, John Nash developed the concept of
(what is now called) \emph{Nash equilibria} \cite{nash1950equilibrium,
  nash1951noncooperative}, generalizing von Neumann's theorem in a different   
direction---to games in which the utility functions used by the two players  
(corresponding to $x$ and $y$ variables) are asymmetric, called non-zero-sum
games. Von Neumman's minimax theorem, Sion's minimax theorem, and the existence
of Nash equilibria can each be proved using fixed-point theorems, typically,
Kakutani's fixed-point theorem \cite{kakutani1941generalization} for set-valued
mappings. Instead, the proof laid out in Exercise \ref{ex:sion_minimax} below, 
which is based on \cite{bental2023convex} (Section 3.4), proceeds in such a way
that reveals connections to convex duality.    

The existence of solutions in optimization problems of regularized GLM-type
\eqref{eq:dual_legendre}, based on duality arguments, was studied in
\cite{ali2019generalized}. The arguments presented here are similar to those
from that paper. The fact that this approach is able to reproduce well-known 
results on the existence of logistic and Poisson regression solutions
is also from \cite{ali2019generalized}.    

\clearpage

\begin{xcb}{Exercises}
\begin{enumerate}[label=\thechapter.\arabic*]
\settowidth{\leftmargini}{00.00.\hskip\labelsep}
\item \label{ex:dual_norm_check}
  Prove that the dual norm \eqref{eq:dual_norm} is indeed a norm on $\R^d$ by
  verifying the following properties:  
  \begin{itemize}
  \item triangle inequality: $\|x+y\|_* \leq \|x\|_* + \|y\|_*$ for all $x,y \in 
  \R^d$;
  \item absolute homogeneity: $\|ax\|_* = |a|\|x\|_*$ for all $a \in \R$ and $x 
  \in \R^d$; and 
\item positive definiteness: $\|x\|_* \geq 0$ for all $x \in \R^d$, with
  equality if and only if $x=0$. 
  \end{itemize}

\item \label{ex:dual_norm_dual1}
  In this exercise, we investigate the dual of the dual of a norm $\|\cdot\|$: 
  \begin{equation}
  \label{eq:dual_norm_dual}
  \|x\|_{**} = \sup_{\|y\|_* \leq 1} \, x^\T y.
  \end{equation}
  Fixing any $x \in \R^d$, we will argue that $\|x\|_{**} = \|x\|$, in two
  steps.  

\begin{enumerate}[label=\alph*.]
\item Prove $\|x\|_{**} \leq \|x\|$ using H{\"o}lder's inequality
  \eqref{eq:holder_inequality}.  

\item Prove $\|x\|_{**} \geq \|x\|$ using the Hahn-Banach theorem. Hint:
  denoting $X = \R^d$, viewed as a vector space equipped with the norm
  $\|\cdot\|$, note that \eqref{eq:dual_norm_dual} can be rewritten as            
  \[
  \|x\|_{**} = \sup \{ F(x) : F \in X^*, \, \|F\|_{\op} \leq 1 \}.
  \]
  Here $X^*$ denotes the dual space of bounded linear functionals (real-valued 
  functions) on $X$, equipped with the norm \smash{$\|F\|_{\op} = \sup_{x \not 
    =  0} \, |F(x)| / \|x\|$.} Now define $L = \{ ax : a \in \R\}$, a
  1-dimensional subspace of $X$, and define $f : L \to \R$ by 
  $f(ax) = a\|x\|$. Show that $f$ is bounded and linear, with unit operator
  norm, hence by the Hahn-Banach theorem there exists a bounded linear
  functional $F$ on $X$ which agrees with $f$ on $L$, and itself has unit
  operator norm. Use this $F$ to show that $\|x\|_{**} \geq \|x\|$.  
\end{enumerate}

\index{norm!subgradients}
\index{dual norm!subgradients}
\item \label{ex:dual_norm_subgradients}
  In this exercise, we will prove Theorem \ref{thm:dual_norm_subgradients}. 

\begin{enumerate}[label=\alph*.] 
\item Prove that $\text{(i)} \iff \text{(iv)}$ using \eqref{eq:dual_norm}.
\item Prove that $\text{(i)} \iff \text{(v)}$ using \eqref{eq:norm_dual2}.
\item Prove that $\text{(ii)} \iff \text{(iv)}$ using
  \eqref{eq:norm_subgradients}, which comes from applying the Danskin-Bertsekas
  theorem \eqref{eq:danskin_bertsekas} to \eqref{eq:dual_norm}.
\item Prove that $\text{(iii)} \iff \text{(v)}$ using
  \begin{equation}
  \label{eq:dual_norm_subgradients}
  \partial \|y\|_* = \{ x : \|x\| \leq 1, \, x^\T y = \|y\|_* \},
  \end{equation}
  which comes from applying the Danskin-Bertsekas theorem
  \eqref{eq:danskin_bertsekas} to \eqref{eq:norm_dual2}. 
\end{enumerate}

\index{lp norm@$\ell_p$ norm!dual norm}
\item \label{ex:lp_norm_dual}
  In this exercise, we prove $(\|\cdot\|_p)_* = \|\cdot\|_q$, as stated in
  Example \parref{xa:lp_norm_dual}, for $p,q \geq 1$ such that $1/p + 1/q = 1$. 
  It suffices to prove $x^\T y \leq \|x\|_p \|y\|_q$ for all $x,y$ (traditional
  H{\"o}lder's inequality, for $\ell_p$ norms), where equality can always be
  obtained for each $x$ (at some $y$). We consider the cases $p = 1$ and $p > 
  1$ separately.            

\begin{enumerate}[label=\alph*.] 
\item Argue directly that $x^\T y \leq \|x\|_1 \|y\|_\infty$, with equality if
  $y_i = \sign(x_i)$, $i = 1,\dots,d$.

\item When $p > 1$, argue first that we can take $\|x\|_p= \|y\|_q = 1$,  
  without any loss of generality. Then argue that $x_i y_i \leq |x_i|^p / p +
  |y_i|^q / q$, using Young's inequality, and sum this up over each $i =
  1,\dots,d$ to arrive at $x^\T y \leq \|x\|_p \|y\|_q$, with equality if
  $|x_i|^p = c |y_i|^q$ for $c > 0$. 
\end{enumerate}

\item \label{ex:scaled_euclidean_norm_dual}
  Consider the optimization problem
  \[
  \maximize \quad y^\T x \quad \st \quad x^\T A x \leq 1. 
  \]
  Prove that the supremum is attained at \smash{$x = A^{-1} y / \sqrt{y^\T
    A^{-1} y}$}, and so \smash{$(\|y\|_A)_* = \sqrt{y^\T A^{-1} y}$}, as stated
  in Exercise \parref{xa:scaled_euclidean_norm_dual}. 

\index{trace norm!dual norm}
\item \label{ex:trace_norm_dual}
  To prove $(\|\cdot\|_{\tr})_* = \|\cdot\|_{\op}$, as stated in Exercise
  \parref{xa:trace_norm_dual}, we will verify that $(\|\cdot\|_{\op})_* = 
  \|\cdot\|_{\tr}$, where to be clear,
  \[
  (\|X\|_{\op})_* = \sup_{\|Z\|_{\tr} \leq 1} \, \langle Z, X \rangle, 
  \]
  and $\langle Z, X \rangle = \tr(Z^\T X)$. We proceed in two steps; in what
  follows, let $X = U \Sigma V^\T$ be the SVD of $X \in \R^{k \times d}$.

\begin{enumerate}[label=\alph*.] 
\item Prove $(\|X\|_{\op})_* \leq \|X\|_{\tr}$ by direct calculation. Hint:
  given any $Z$ such that $\|Z\|_{\op} \leq 1$, argue that 
  \[
  \langle Z, X \rangle = \langle U^\T Z V, \Sigma \rangle = \sum_{i=1}^d u_i^\T
  Z v_i \sigma_{ii}, 
  \]
  where $u_i,v_i$ denote the $i\th$ columns of $U,V$, respectively, for $i =
  1,\dots,d$. Then argue that each $u_i^\T Z v_i \leq 1$, as $u_i,v_i$ have 
  unit $\ell_2$ norm and $\|Z\|_{\op} \leq 1$.

\item Prove $(\|X\|_{\op})_* \geq \|X\|_{\tr}$ by studying $\langle Z, X
  \rangle$ with $Z = U V^\T$. Hint: verify that now  
  \[
  \langle Z, X \rangle = \langle U^\T Z V, \Sigma \rangle = \sum_{i=1}^d
  \sigma_{ii}.
  \]
\end{enumerate}

% Recall that the Schatten $p$-norm on matrices is defined, for $p \geq 1$, by    
% \[
% \|X\|_p =  \|\sigma(X)\|_p,
% \]
% with the right-hand representing the usual $\ell_p$ norm of the vector
% $\sigma(X) = (\sigma_1(X), \dots, \sigma_n(X))$ of singular values of a matrix 
% $X \in \R^{n \times d}$. Consider
% \[
% (\|X\|_p)_* = \sup_{\|Z\|_p \leq 1} \, \langle Z, X \rangle,
% \]
% where $\langle Z, X \rangle = \tr(Z^\T X)$. Prove that $(\|X\|_p)_* =
% \|X\|_q$, where $1/p + 1/q = 1$.   

\item \label{ex:norm_squared_conjugate}
  We will derive some properties associated with the pair of set-valued
  operators   
  \begin{align*}
  d(x) &= \|x\| \cdot \partial \|x\|, \\
  d^*(y) &= \|y\|_* \cdot \partial \|y\|_*.
  \end{align*}

\begin{enumerate}[label=\alph*.] 
\item Prove that $\|x\| = \|y\|_*$, for $y \in d(x)$.
\item Prove that $x^\T y = \|x\|^2$, for $y \in d(x)$. 
\item For $f(x) = \frac{1}{2} \|x\|^2$, prove that $\partial f(x) = d(x)$. 
\item For $f^*(y) = \sup_x \, \{ y^\T x - f(x) \}$, prove that the supremum
  defining $f^*(y)$ is achieved for $y \in d(x)$. Plug this in, and use parts a
  and b, to verify \eqref{eq:norm_squared_conjugate}. 
\item Prove that $d(x) = \argmax_y \, \{ x^\T y - \frac{1}{2} \|y\|_*^2 \}$, and 
  $d^*(y) = \argmax_x \, \{ y^\T x - \frac{1}{2} \|x\|^2 \}$. 
\item Prove that $y \in d(x) \iff x \in d^*(y)$.
\end{enumerate}

\item \label{ex:dual_norm_dual2}
  Consider the optimization problem 
  \[
  \minimize_z \quad \|z\| \quad \st \quad z = x,
  \]
  whose optimal criterion value is $f^\star = \|x\|$. Show that its Lagrange 
  dual problem is 
  \[
  \maximize_u \quad u^\T x \quad \st \quad \|u\|_* \leq 1,
  \]
  whose optimal criterion value is $g^\star = \|x\|_{**}$. Argue that strong 
  duality holds, $f^\star = g^\star$, which translates to $\|x\|_{**} = \|x\|$. 

\item \label{ex:dual_norm_dual3}
  Use \eqref{eq:norm_squared_conjugate}, and the fact that $f^{**} = f$ for
  closed convex $f$, as in \eqref{eq:double_conjugate_identity}, to verify
  $\|x\|_{**} = \|x\|$.  

\index{lasso!dual problem}
\item \label{ex:lasso_fenchel_dual}
  For \smash{$f(z) = \frac{1}{2} \|y-z\|_2^2$}, prove either by direct
  calculation or calculus rules for conjugacy that \smash{$f^*(u) = \frac{1}{2}
    \|y+u\|_2^2 - \frac{1}{2} \|y\|_2^2$}. Use this to show that Fenchel
  duality, as given in Example \parref{xa:fenchel_norm}, reproduces the lasso 
  dual in \eqref{eq:lasso_dual}.

\index{support vector machine!hinge form}
\index{support vector machine!dual problem}
\item \label{ex:svm_fenchel_dual}
  Consider the SVM in hinge form, for labels $y_i \in \{ -1, 1\}$ and features
  $x_i \in \R^d$, $i=1,\dots,n$,     
  \begin{equation}
  \label{eq:svm_hinge2}
  \minimize_{\beta_0,\beta} \quad C \sum_{i=1}^n \big[1 - y_i(\beta_0 + x_i^\T 
  \beta)\big]_+ + \frac{1}{2} \|\beta\|_2^2.
  \end{equation}
  We simplify our study, in preparation for examining Fenchel duality, by
  omitting the intercept term $\beta_0$. Denote by $y \in \R^n$ the label
  vector, $X \in \R^{n \times d}$ the feature matrix (with $i\th$ row $x_i^\T$),
  and \smash{$\tilde{X} = \diag(y) X$}. Then the above problem (without
  intercept) can be recast as      
  \begin{equation}
  \label{eq:svm_hinge3}
  \minimize_\beta \quad f(\tilde{X} \beta) + \frac{1}{2} \|\beta\|_2^2, 
  \end{equation}
  where $f(z) = C \sum_{i=1}^n (1-z_i)_+$. To reproduce the dual
  \eqref{eq:svm_dual}, we proceed in steps. 

\begin{enumerate}[label=\alph*.] 
\item Prove by direct calculation that $f^*(u) = \one^\T u \cdot
  I_{[-C,0]^n}(u)$.   

\item Use Fenchel duality, as given in Example \parref{xa:fenchel_norm_squared}, 
  to prove that the dual of \eqref{eq:svm_hinge3} is
  \begin{alignat*}{2}
  &\maximize_u \quad && \one^\T u - \frac{1}{2} \|\tilde{X}^\T u\|_2^2 \\ 
  &\st \quad && u \in [0,C]^n.
  \end{alignat*}

\item Adapt the arguments in the previous parts to accommodate the intercept in
  the original problem \eqref{eq:svm_hinge2}, and show that the resulting
  Frenchel dual reproduces \eqref{eq:svm_dual}. Hint: the presence of intercept
  will require you to compute the conjugate of \smash{$(\beta_0, \beta) \mapsto 
    \frac{1}{2} \|\beta\|_2^2$}.    
\end{enumerate}

\item When is the dual of dual problem the primal problem? Describe a class of
  problems in which this holds, using Fenchel duality.   

\index{Dubovitski-Milutin lemma}
\item \label{ex:dubovitski_milutin_lemma} 
  Let $K_1,\dots,K_m$ be cones, and $K = K_1 \cap \dots \cap K_m$. 

\begin{enumerate}[label=\alph*.] 
\item Prove that $K^* \supseteq \closure(K_1^* + \cdots + K_m^*)$. 
\item Assuming additionally that $K_1,\dots,K_m$ are closed, prove that
  $K^* = \closure(K_1^* + \cdots + K_m^*)$. Hint: use the strict separating
  hyperplane theorem, from Exercise \ref{ex:farkas_lemma}.      
  % Proposition 1.2.6 in Ben-Tal and Nemirovski (2023) 

\item Assuming additionally that $K_1,\dots,K_m$ are convex (altogether, these
  are closed convex cones), and $K_1 \cap \interior(K_2) \cap \cdots \cap
  \interior(K_m) \not= \emptyset$, prove that $K_1 ^* + \cdots + K_m^*$ is
  closed, thus 
  \[
  K^* = K_1^* + \cdots + K_m^*.
  \] 
  This is known as the \emph{Dubovitski-Milutin lemma}. Hint: consider a
  sequence $y_j = \sum_{i=1}^k y_{ij}$, with $y_{ij} \in K_i^*$ for $j =
  1,2,3,\dots$ and each $i$, where $y_j \to y$ as $j \to \infty$. It remains to
  show that $y \in K_1^* + \cdots + K_m^*$. For this, use the existence of $x
  \in K_1 \cap \interior(K_2) \cap \cdots \cap \interior(K_m)$ to show that
  $y_{ij}$, $j = 1,2,3,\dots$ is a bounded sequence for each $i$, and therefore,
  passing to a subsequence if needed, it has a subsequence converging to a point
  in $K_i^*$.  
  % Proposition 1.2.7 in Ben-Tal and Nemirovski (2023) 
\end{enumerate}

\index{Farkas' lemma}
\item \label{ex:farkas_variations_conic} 
  In this exercise, we will show that the Dubovitski-Milutin lemma, studied in
  the last exercise, leads to an important variation on Farkas' lemma. This
  result will generalize the result from Exercise \ref{ex:farkas_variations}
  part b, which was a key tool used in proving the convex theorem of
  alternatives from Exercise \ref{ex:convex_theorem_alternatives}. Given $A \in
  \R^{k \times d}$, $b \in \R^k$, $g \in \R^d$, $h \in \R$, and a convex set $D
  \subseteq \R^d$, with $b \geq 0$ and $0 \in \interior(D)$, consider two
  statements:        
  \begin{itemize}
  \item for all $x \in D$, it holds that $Ax \leq b \implies g^\T x \leq h$;  
  \item there exists $\mu \in \R^k$ such that $\mu \geq 0$ and $\mu^\T (Ax - b) 
    \geq g^\T x - h$ for all $x \in D$.
  \end{itemize}
  We will prove that these two statements are equivalent in what follows.
  % Following Lemma 3.2.1 in Ben-Tal and Nemirovski (2023)
  
\begin{enumerate}[label=\alph*.]
\item Prove that the second statement implies the first.

\item To prove that the first statement implies the second, start by defining 
  \[
  K_1 = \closure \Big\{ (x,t) \in \R^d \times \R: t > 0, \, x/t \in D \Big\}
  \quad \text{and} \quad K_2 = \Big\{ (x,t) \in \R^d \times \R: Ax \leq tb
  \Big\}.  
  \]
  Show that these are closed convex cones, with $\interior(K_1) \cap K_2 \not= 
  \emptyset$. 

\item Define $f = (g,-h) \in \R^{d+1}$, and show that $f^\T (x,t) \leq 0$ for
  all $(x,t) \in K_1 \cap K_2$.  

\item Use Exercise \ref{ex:dubovitski_milutin_lemma} part c to argue that we can
  write $f = \psi + \phi$, where $\psi^\T(x,t) \leq 0$ for all $x \in K_1$, and
  $\phi^\T (x,t) \leq 0$ for all $x \in K_2$. 

\item Use Exercise \ref{ex:farkas_variations} part a to argue that there exists
  $\mu \geq 0$ with $[A \, -b]^\T \mu = \phi$, in other words, $\phi^\T (x,t) =
  \mu^\T (Ax - tb)$ for all $x,t$. Use this to prove the desired result.        
\end{enumerate}
  
\item \label{ex:v_representation_dual}
  To verify \eqref{eq:v_representation_dual}, argue that the inclusion
  $\supseteq$ follows directly, whereas the inclusion $\subseteq$ follows using
  the Farkas' lemma variation from Exercise \ref{ex:farkas_variations} part b
  (or Exercise \ref{ex:farkas_variations_conic} with $D = \R^d$). Hint: fixing
  $y$ such that $Ax \leq b$ implies $y^\T x \leq 1$; we know that there exists  
  $\mu \geq 0$ such that $\mu^\T (Ax - b) \geq y^\T x - 1$ for all $x \in 
  \R^d$. Show that this implies $y = A^\T \mu$, and $\mu^\T b \leq 1$.
  We can then reparametrize, using $b_1,\dots,b_m > 0$, since $0 \in
  \interior(P)$, to obtain the desired statement. 

\item \label{ex:h_representation_dual}
  To verify \eqref{eq:h_representation_dual}, argue that the inclusion
  $\subseteq$ follows directly, whereas the inclusion $\supseteq$ follows
  by convexity of the set $\{x : y^\T x \leq 1\}$ for a given $y$.

\item \label{ex:hv_representation_dual}
  In this exercise, we step through a proof of the main representation theorem,
  Theorem \ref{thm:hv_representation}, for polytopes. We treat each direction 
  separately. 

\begin{enumerate}[label=\alph*.]
\item Suppose $P = \{x : Ax \leq b\}$ is an H-representation and $P$ is
  bounded. Use the fact that $P$ has a finite number of faces (Exercise
  \ref{ex:face_structure} part f), along with the Krein-Milman theorem
  (Exercise \ref{ex:extreme_points} part b), to show that $P =
  \conv\{x_1,\dots,x_n\}$ for vertices $x_1,\dots,x_n$.

\item Suppose $P = \conv\{x_1,\dots,x_n\}$ is a V-representation. Show that the
  dual polytope $P^*$ has an H-representation, then use the argument from part a
  applied to $P^*$, and duality once again along with $P^{**} = P$ by
  \eqref{eq:polar_set_polar}, to form an H-representation for $P$.    
\end{enumerate}

\item \label{ex:face_duality}
  This exercise outlines the proof of Theorem \ref{thm:face_duality}. It is
  trivial to check that $\Psi(\emptyset) = P^*$ and $\Psi(P) = \emptyset$, and
  we restrict our attention to a proper face $F \in \cF(P)$.  
% Following Theorem 4 in Chapter 3.4 of Grunbaum (2003)

\begin{enumerate}[label=\alph*.]
\item Let $x_0 \in \relint(F)$, and $F^* = \{ y \in P^* : y^\T x_0 = 1 \}$.
  Prove that $\Psi(F) = F^*$, and so $\Psi(F)$ is indeed a face of
  $P^*$. Hint: suppose that $\Psi(F) \not= F^*$, and take $y_0 \in F^* \setminus
  \Psi(F)$. Argue there exists $x_1 \in F$ such that $y_0^\T x_1 < 1$. Use $x_0
  \in \relint(F)$ to argue that $y_0^\T x_0 < 1$. 

\item Prove the inclusion-reversing property: given $G \supseteq F$, we have
  $\Psi(G) \subseteq \Psi(G)$.  

\item Prove the dimensionality property: $\dim(F) + \dim(\Psi(F)) = d-1$.  
\end{enumerate}

\item \label{ex:l1_linf_ball_duality}
  Consider the $\ell_1$ and $\ell_\infty$ balls in $\R^d$,
  \[
  P = \bigg\{ x : \sum_{i=1}^d |x_i| \leq 1 \bigg\} \quad \text{and} \quad 
  Q = \bigg\{ x : \max_{i=1,\dots,d} \, |x_i| \leq 1 \bigg\}. 
  \]
  In geometry these are often referred to as the \emph{cross-polytope} and
  \emph{hypercube}, respectively.  

\begin{enumerate}[label=\alph*.]
\item Prove directly with a counting argument that the number of $k$-faces of
  $P$ is $2^{k+1} {d \choose k+1}$.  

\item Prove directly with a counting argument that the number of $k$-faces of
  $Q$ is $2^{d-k} {d \choose d-k}$. 

\item Prove that $P^* = Q$, and check that the relationship between the faces of
  $P$ and $Q$ from Theorem \ref{thm:face_duality} is consistent with parts a and
  b above. 
\end{enumerate}

\item \label{ex:minkowski_weyl}
  We say that a set $C \subseteq \R^d$ is \emph{finitely generated} provided
  there exists $x_1,\dots,x_n \in \R^d$ and $k \leq n$ such that $C = 
  \conv\{x_1,\dots,x_k\} + \cone\{x_{k+1},\dots,x_n\}$, or in other words  
  \[
  C = \bigg\{
  \sum_{i=1}^n t_i x_i : \text{$t_i \geq 0$, for $i=1,\dots,n$, and
    $\sum_{i=1}^k t_i = 1$} \bigg\}.  
  \]
  When $k = n$, note that $C$ reduces to the convex hull of $x_1,\dots,x_n$,
  which is bounded, but when $k < n$, the set $C$ will be unbounded. A 
  generalization of Theorem \ref{thm:hv_representation}, sometimes referred to
  as the \emph{Minkowski-Weyl theorem}, is as follows: 
  \index{Minkowski-Weyl theorem}
  \index{polyhedron!dual polyhedron}
  \begin{equation}
  \label{eq:hv_representation_generalized}
  \text{a set $P$ is a polyhedron if and only if it is finitely generated.}
  \end{equation}
  % For example, Theorem 19.1 in Rockafellar (1970)
  In what follows, we will use this generalization to prove certain facts about
  polyhedra. Denote by $P = \{x : Ax \leq b\} \subseteq \R^d$ a polyhedron, and
  $f : \R^d \to \R^m$ a linear transformation.     

\begin{enumerate}[label=\alph*.]
\item Show that the preimage $f^{-1}(P)$ of $P$ under $F$ is a polyhedron. 

\item Show that the image $f(P)$ of $P$ under $F$ is a polyhedron. Hint: use
  \eqref{eq:hv_representation_generalized}. 

\item Show that the dual $P^*$ defined in \eqref{eq:dual_polyhedron} is a 
  polyhedron. Hint: use \eqref{eq:hv_representation_generalized}.
\end{enumerate}

\item \label{ex:polar_set_polar}
  Given an arbitrary set $C$, consider the polar of its polar set:
  \[
  C^{\circ\circ} = \{ x : x^\T y \leq 1, \; y \in C^\circ \}.
  \]
  In this exercise, we verify \eqref{eq:polar_set_polar}, assuming $C$ is closed
  and convex with $0 \in C$.
  
\begin{enumerate}[label=\alph*.] 
\item Prove that $C \subseteq C^{\circ\circ}$.
\item Prove that $C \supseteq C^{\circ\circ}$. Hint: fix $x_0 \notin C$, and
  argue that $x_0 \notin C^{\circ\circ}$ using the strict version of the
  separating hyperplane theorem given in Exercise \ref{ex:farkas_lemma}. Use $0
  \in C$ to show that the separating hyperplane can be expressed as $y^\T x_0 >
  1$ and $y^\T x < 1$ for all $x \in C$.
\end{enumerate}

\index{normal cone!dual cone}
\index{tangent cone}
\item \label{ex:tangent_cone}
  This exercise studies the tangent cone \eqref{eq:tangent_cone} to a set $C$ at
  a point $x \in C$. 
% See Theorems 6.5 and 6.9 of Rockafellar and Wets (2009), although our
% presentation is slightly different, but equivalent for convex sets 

\begin{enumerate}[label=\alph*.] 
\item Give an example to show that $T_C(x)$ can be nonconvex.
\item For convex $C$, prove that $T_C(x)$ is convex. 
\item For convex $C$, prove that $N_C(x) = (T_C(x))^\circ$ and thus
  $(N_C(x))^\circ = T_C(x)$. Hint: for the first claim, establish each inclusion
  ($\subseteq$ and $\supseteq$) separately. Only one requires convexity.        
\end{enumerate}

\item \label{ex:convex_theorem_alternatives_conic}
  Given a set $D$, cone $K$, functions $f,h$, and matrix-vector pair $A,b$,
  consider the problems:
  \begin{align}
  \label{eq:feasibility_conic1}
  &\begin{alignedat}{2}  
  &\find \quad && x \\  
  &\st \quad && f(x) < c \\
  & && h(x) \leq_K 0 \\
  & && Ax \leq b \\
  & && x \in D,
  \end{alignedat} \\[10pt]
  \label{eq:feasibility_conic2}
  &\begin{alignedat}{2}  
  &\find \quad && u,v \\ 
  &\st \quad && \inf_{x \in D} \, \big\{ f(x) + u^\T h(x) + v^\T (Ax - b) \big\}
  \geq c \\  
  & && u \geq_{K^*} 0 \\ 
  & && v \geq 0.
  \end{alignedat}
  \end{align}
  The value of $c$ here is fixed and arbitrary. Following Theorem
  3.2.2 in \cite{bental2023convex}, we will prove what these authors call the
  \emph{convex theorem of alternatives} for problems in conic form, which
  generalizes the result in Exercise \ref{ex:convex_theorem_alternatives}. This
  theorem says that under conditions (i) and (ii) in Theorem
  \ref{thm:slater_conic}, problem \eqref{eq:feasibility_conic2} is feasible if 
  and only if \eqref{eq:feasibility_conic1} is infeasible.    
  
\begin{enumerate}[label=\alph*.]
\item Assume that problem \eqref{eq:feasibility_conic2} is feasible. Prove that
  problem \eqref{eq:feasibility_conic1} is infeasible. Hint: argue the
  contrapositive, recalling that $K^*$ is the dual cone, and \smash{$u
    \geq_{K^*} 0 \iff u \in K^*$}. Note that this part of the argument does not
  use conic Slater's condition.   
  
\item The other direction---that infeasibility of \eqref{eq:feasibility_conic2}
  implies feasibility of \eqref{eq:feasibility_conic1}---is much more involved,
  and we will prove this in several steps, following closely the structure of
  arguments in Exercise \ref{ex:convex_theorem_alternatives}. We assume without
  a loss of generality that $c = 0$, and that $x = 0$ satisfies condition (ii)
  in Theorem \ref{thm:slater_conic}. Define the sets         
  \begin{align*}
  S &= \Big\{ (t_0, t_1) \in \R \times \R^m : t_0 < 0, \, t_1 \leq_K 0 \Big\},
    \\    
  T &= \Big\{ (t_0, t_1) \in \R \times \R^m : \text{there exists $x \in D$ such
      that $f(x) \leq t_0$, $h(x) \leq_K t_1$, and $Ax \leq b$} \Big\}.  
  \end{align*}
  Assume that \eqref{eq:feasibility_conic1} is infeasible. Prove that there
  exists a nonzero vector $a = (a_0, a_1) \in \R \times \R^m$, with $a_0 \geq 0$
  and \smash{$a_1 \geq_{K^*} 0$}, such that    
  \[
  \inf_{t \in T} \, a^\T t \geq 0.  
  \]
  Hint: apply the separating hyperplane theorem, Theorem
  \ref{thm:separating_hyperplane}, to $S,T$. Then argue that we must have $a_0
  \geq 0$, \smash{$a_1 \geq_{K^*} 0$}, and $b = 0$ in the supporting hyperplane.    

\item Prove that $a_0 > 0$. Hint: assume in order to achieve a contradiction 
  that $a_0 = 0$. Then use $(f(0), h(0)) \in T$, and the fact that $x = 0$
  satisfies condition (ii) in Theorem \ref{thm:slater_conic}, by assumption,
  to conclude $a_1 = 0$.

\item Define $\alpha = a_1 / a_0 \in \R^m$, and  
  \[
  F(x) = f(x) + \alpha^\T h(x),
  \]
  where $h(x) = (h_1(x), \dots, h_m(x))$. Noting that \smash{$\alpha \geq_{K^*}
    0$} (as $a_0 > 0$ and \smash{$a_1 \geq_{K^*} 0$}), prove 
  \[
 \text{for all $x \in D$}, \quad Ax \leq b \implies F(x) \geq 0.
  \]
  Hint: for any $x \in D$ with $Ax \leq b$, we have $t = (f(x), h(x)) \in T$, so
  that $a^\T t \geq 0$.

\item In what remains, we seek to show that there exists $\beta \in \R^k$ such
  that $\beta \geq 0$ and  
  \begin{equation}
  \label{eq:feasibility_conic3}
  F(x) + \beta^\T (Ax - b) \geq 0, \quad \text{for all $x \in D$}. 
  \end{equation}
  Notice that this would complete the proof as it would prove that 
  \eqref{eq:feasibility_conic2} is feasible, for $u = \alpha$ and $v = \beta$
  (recall that we have taken $c = 0$). Towards verifying the existence of such a
  vector $\beta$, define the sets     
  \begin{align*}
  G &= \Big\{ (x, z) \in \R^d \times \R : x \in D, \, Ax \leq b, \, z < 0
      \Big\}, \\    
  H &= \Big\{ (x, z) \in \R^d \times \R : x \in D, \, F(x) \leq z \Big\},  
  \end{align*}
  Prove that there exists a nonzero vector $(w, \eta) \in \R^d \times \R$ such
  that  
  \[
  \sup \, \Big\{ w^\T x + \eta z : x \in D, \, Ax \leq b, \, z < 0 \Big\} \leq 
  \inf \, \Big\{ w^\T x + \eta z : x \in D, \, F(x) \leq z \Big\}. 
  \]
  Hint: apply the separating hyperplane theorem again, this time to $G,H$. 

\item Argue that $\eta \geq 0$, so that 
  \[
  \sup \Big\{ w^\T x : x \in D, \, Ax \leq b \Big\} \leq \inf \Big\{ w^\T x +
  \eta F(x) : x \in D \Big\}.
  \]

\item Argue further that $\eta > 0$, so that by defining $\rho = w / z$, we
  have  
  \[
  \sup \Big\{ \rho^\T x : x \in D, \, Ax \leq b \Big\} \leq \inf \Big\{
  \rho^\T x + F(x) : x \in D \Big\}.
  \]
  Hint: assume for the sake of contradiction that $\eta = 0$. Then as the origin
  $x = 0$ satisfies condition (ii) from Theorem \ref{thm:slater_conic}, the
  left-hand side in the display from part f is at least zero, so that $w^\T x
  \geq 0$ for all $x \in D$, which can only happen (recalling $0 \in
  \interior(D)$, and assuming without a loss of generality that $D$ is
  full-dimensional) if $w = 0$.

\item Let $\theta = \sup \, \{\rho^\T x : x \in D, \, Ax \leq b\}$. Then $x \in
  D$ and $Ax \leq b \implies \rho^\T x \leq \theta$, hence by Exercise 
  \ref{ex:farkas_variations_conic}, there exists a vector $\beta \geq 0$ such that
  $\beta^\T (Ax - b) \geq \rho^\T x - \theta$ for all $x \in D$. Use this along 
  with part g to establish \eqref{eq:feasibility_conic3} and complete the proof.    
\end{enumerate}

\index{Slater's condition}
\item \label{ex:slater_conic}
  In this exercise, we prove the conic generalization of Slater's condition for
  strong duality, in Theorem \ref{thm:slater_conic}. Let $f^\star, g^\star$ be
  the optimal values in \eqref{eq:primal_conic} and \eqref{eq:dual_conic},
  respectively. Note that if $f^\star = -\infty$, then the desired conclusion
  $f^\star = g^\star$ is already implied by weak duality. Hence, assume that
  $f^\star > -\infty$, and use the result in Exercise
  \ref{ex:convex_theorem_alternatives_conic} to prove $f^\star = g^\star$, and
  there exists dual feasible $u,v$ which achieve the optimal criterion
  value. Hint: set $c = f^\star$.      

\item \label{ex:dual_conic}
  We derive the dual of problem \eqref{eq:primal_conic}. Show that we can
  rewrite this as  
  \begin{alignat*}{2}
  &\minimize_{x,z} \quad && f(x) + I_K(z) \\ 
  &\st \quad && h(x) = -z \\ 
  & && Ax \leq b,
  \end{alignat*} 
  whose Lagrangian is
  \[
  L(x,z,u,v) = f(x) + I_K(z) + u^\T (h(x) + z) + v^\T (Ax - b).
  \]
  Then, by minimizing over $x,z$, and using $I^*_K = I_{K^\circ} = I_{-K^*}$
  from \eqref{eq:support_function_cone}, show that the dual problem is as
  claimed in \eqref{eq:dual_conic}.  

\index{cone program!dual problem}
\item \label{ex:slater_cone_program}
  Recall that a cone program is a special case of \eqref{eq:primal_conic} where 
  $f(x) = c^\T x$ and $h(x) = Px - q$, for $c \in \R^d$, $P \in \R^{m \times d}$,
  and $q \in \R^m$, explicitly   
  \begin{equation}
  \label{eq:primal_cone_program}
  \begin{alignedat}{2}
  &\minimize_x \quad && c^\T x \\ 
  &\st \quad && Px \leq_K q \\
  & && Ax \leq b.
  \end{alignedat}
  \end{equation}
  We examine duality and optimality in \eqref{eq:primal_cone_program},
  generalizing previous conclusions for SDPs. 

\begin{enumerate}[label=\alph*.]
\item Prove that the dual of \eqref{eq:primal_cone_program} is 
  \begin{equation}
  \label{eq:dual_cone_program}
  \begin{alignedat}{2}
  &\maximize_{u,v} \quad && -q^\T u -b^\T v \\
  &\st \quad && P^\T u + A^\T v = -c \\
  & && u \geq_{K*} 0 \\
  & && v \geq 0.
  \end{alignedat}
  \end{equation}

\item Prove that the dual of \eqref{eq:dual_cone_program} is equivalent to
  \eqref{eq:primal_cone_program}. 

\item Prove that the dual cone $K^*$ is regular if $K$ is regular. 

\item When $K$ is a regular cone, conclude based on Theorem
  \ref{thm:slater_conic} that the following holds for problems 
  \eqref{eq:primal_cone_program}, \eqref{eq:dual_cone_program}.
  
  \begin{enumerate}[label=(\roman*)]  
  \item If there exists $x$ such that $Ax \leq b$ and $Px - q <_K 0 \iff q - Px 
    \in \interior(K)$, then strong duality holds. Furthermore, $f^\star >
    -\infty$ if and only if the dual is feasible, in which case a dual solution 
    exists.  
  \item If there exists $u$ and $v$ such that $P^\T u + A^\T v = -c$ and $u
    >_{K^*} 0 \iff u \in \interior(K^*)$, then strong duality
    holds. Furthermore, $g^\star < \infty$ if and only if the primal is
    feasible, in which case a primal solution exists. 
  \end{enumerate}
\end{enumerate}

\index{Sion's minimax theorem}
\item \label{ex:sion_minimax}
  This exercise traces out the proof of Sion's minimax theorem, in Theorem 
  \ref{thm:sion_minimax}. Let us define 
  \[
  \alpha = \inf_{x \in X} \, \sup_{y \in Y} \, L(x,y) \quad \text{and} \quad  
  \beta = \sup_{y \in Y} \, \inf_{x \in X} \, L(x,y),
  \]
  each of which are finite because $X,Y$ are compact and $L$ is continuous.
  Recall by the general inf-sup inequality (Exercise \ref{ex:inf_sup_rules}
  part d), we know that $\beta \leq \alpha$. Thus, it remains to prove the
  opposite inequality, that $\alpha \leq \beta$.  

\begin{enumerate}[label=\alph*.]
\item Define $X(y) = \{x \in X : L(x,y) \leq \beta\}$. Prove that for each $y
  \in Y$, the set $X(y)$ is closed, convex, and nonempty. 
  
\item Prove that to show $\alpha \leq \beta$, it suffices to show that the sets
  $X(y)$, $y \in Y$ have a point in common (have nonempty intersection).    

\item Fix any $y_i \in Y$, $i=1,\dots,n$, and abbreviate $f_i = L(\cdot,
  y_i)$, $i=1,\dots,n$. Notice that these are convex continuous functions on 
  $X$. Prove using strong duality (Slater's condition) that there exists $u \geq
  0$ with \smash{$\sum_{i=1}^n u_i = 1$} such that       
  \[
  \inf_{x \in X} \, \max_{i=1,\dots,n} \, f_i(x) = \inf_{x \in X} \,
  \sum_{i=1}^n u_i f_i(x). 
  \]

\item Prove that \smash{$\sum_{i=1}^n u_i f_i(x) \leq \beta$} for all $x \in
  X$. Hint: use the concavity of $L$ in its second argument, and the definition
  of $\beta$. 

\item Prove based on parts c and d that any finite number of sets $X(y_i)$, $i =
  1,\dots,n$ have a nonempty intersection. The collection $X(y)$, $y \in Y$ is
  therefore said to satisfy the \emph{finite intersection property}. 

\item Complete the proof of Sion's minimax theorem by concluding that $X(y)$, $y
  \in Y$ have a point in common. Hint: you may use the fact that since $X$ is
  compact, if a collection of subsets of $X$ satisfies the finite intersection
  property, then this collection must itself have a nonempty intersection. This
  statement is equivalent to the usual topological definition of compactness 
  (which can be seen via manipulation using De Morgan's laws).      
\end{enumerate}

\item \label{ex:stiemke_lemma}
  Use the separating hyperplane theorem (as in Theorem
  \ref{thm:separating_hyperplane}---notably, not the strict version) to prove  
  \emph{Stiemke's lemma}: given $A \in \R^{k \times d}$, exactly one of the
  following statements is true:            
  \index{Stiemke's lemma}
  \begin{itemize}
  \item there exists $x \in \R^d$ such that $Ax=0$, $x > 0$;
  \item there exists $y \in \R^k$ such that $A^\T y \geq 0$, $A^\T y \not=0$. 
  \end{itemize}
  Hint: in the separating hyperplane application, take $C = \R_{++}^d$ and $D = 
  \nul(A)$.

\index{generalized linear model!existence of solutions}
\item \label{ex:existence_logistic}
  Consider the log-partition function for logistic regression, 
  \smash{$\Psi(\eta) = \sum_{i=1}^n \log(1+e^{\eta_i})$}, where we have
  $\interior(\ran(\nabla \Psi)) = (0,1)^n$. Let $y = (y_1,\dots,y_n) \in
  \{0,1\}^n$ denote the response vector.  

\begin{enumerate}[label=\alph*.]
\item Prove that \eqref{eq:slater_dual_glm} is equivalent to the existence of $a
  \in (0,1)^n$ such that $X^\T (y - a) = 0$.

\item Prove that the condition from part a is equivalent to $\nul(X^\T 
  D_Y) \cap \R_{++}^n \not= \emptyset$, where we define $D_Y =
  \diag(Y_1,\dots,Y_n)$, for $Y = (Y_1,\dots,Y_n)$ with each $Y_i = 2y_i - 1 \in
  \{-1,1\}$.

\item Prove that the condition from part b is itself equivalent to $\col(D_Y X)
  \cap \R_+^n = \{0\}$. Hint: use Stiemke's lemma from Exercise 
  \ref{ex:stiemke_lemma}, with $A = X^\T D_Y$. 

\item Prove that the condition from part c is equivalent to
  \eqref{eq:existence_logistic}, concluding that part (ii) of Theorem
  \ref{thm:existence_glms} follows from Corollary
  \ref{cor:existence_regularized_glms}.  
\end{enumerate}

\item \label{ex:existence_regularized_glms}
  In the context of Corollary \ref{cor:existence_regularized_glms}, consider the
  case $h_C(\beta) = \lambda \|\beta\|$ for a norm $\|\cdot\|$ and $\lambda >
  0$.  

\begin{enumerate}[label=\alph*.]
\item For logistic regression, where \smash{$\Psi(\eta) = \sum_{i=1}^n
    \log(1+e^{\eta_i})$}, prove that \eqref{eq:slater_dual_glm} always holds,
  thus a regularized logistic regression solution always exists. Hint: decompose
  $y = z + \delta$, with $z \in (0,1)^n$ and $\delta$ having arbitrarily small 
  $\|\cdot\|$-norm. Then argue that \eqref{eq:slater_dual_glm} holds if
  $(X^\T)^{-1} (\relint(C)) = \{ u : \|X^\T u\|_* < \lambda\}$ contains a
  $\|\cdot\|$-norm ball of arbitrarily small radius centered at the origin.  

\item For Poisson regression, where \smash{$\Psi(\eta) = \sum_{i=1}^n e^{\eta_i}$},
  prove that \eqref{eq:slater_dual_glm} always holds, thus a regularized Poisson
  regression solution always exists. Hint: similarly, let $y = z + \delta$, with
  $z \in \R_{++}^n$ and $\delta$ having small $\|\cdot\|$-norm; then complete
  the argument as in part a.  
\end{enumerate}

% Ideas for unwritten exercises? (we have way too much already ...)
% - Interpret the existence condition for logistic regression
% - Describe gauge functions and their properties
% - Relationship between duals, polars, gauges
% - Investigate plurality of dual problems

\end{enumerate}
\end{xcb}