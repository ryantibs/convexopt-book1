\chapter{Duality revisited}
\label{chap:duality_revisited}

\section{Dual norms}
\label{sec:dual_norms}

\section{Dual cones}
\label{sec:dual_cones}
Also give conic duality

\section{Polar sets}
\label{sec:polar_sets}

Dual in terms of gauge of polar set 

\section{Fenchel duality}

Fenchel duality: gives general relationship between primal and dual in terms of
conjugates 

when is dual of dual the primal? Answer with f** = f. Pull back to QPs? 

\section{Conic duality*}
\label{sec:conic_duality}

How to write in a unified way?

Write cone program as follows in a way that allows Fenchel duality to be applied
elegantly. For example, min $c^\T x$ subject $Ax + b \in K$

See Theorems 3.2.2 and 3.2.5 in Ben Tal and Nemirovski. This is more general and 
it will be helpful to write this as: min $f(x)$ subject to $Ax = b$, $Gx \leq
h$, $h$ is $K$-convex where $K$ is pointed. MAKE SURE TO GIVE STATEMENTS OF
ATTAINMENT SO WE CAN USE THAT IN AN EARLIER EXERCISE. 

\index{Slater's condition}
\index{strong duality}
\begin{Theorem}
\label{thm:slater_conic}
TODO
% Consider the optimization problem \eqref{eq:primal_problem}, where after 
% relabeling, if needed, we take $h_1, \dots, h_r$ to be inequality constraint
% functions which are affine ($r = 0$ if none are affine). Assume the following.         

% \begin{enumerate}[label=(\roman*)]
% \item The functions $f$ and $h_i$, $i=1,\dots,m$ are convex, and $\ell_j$,
%   $j=1,\dots,k$ are affine; in other words, problem \eqref{eq:primal_problem} is
%   convex, and we can write its equality constraints as $Ax = b$.      

% \item There exists $x \in \relint(D)$, where \smash{$D = \dom(f) \cap
%     \bigcap_{i=r+1}^m \dom(h_i)$} denotes the common effective domain, such that      
%   \begin{equation}
%   \label{eq:slater_condition}
%   h_i(x) \leq 0 \;\, \text{for all $i \leq r$}, \quad
%   h_i(x) < 0 \;\, \text{for all $i \geq r+1$}, \quad 
%   Ax = b.
%   \end{equation}
% \end{enumerate}

% Then strong duality holds: the optimal values $f^\star$ in problem
% \eqref{eq:primal_problem} and $g^\star$ in the corresponding dual problem  
% \eqref{eq:dual_problem} satisfy $f^\star = g^\star$.
\end{Theorem}

\section{Saddle point theorems*}
\label{sec:saddle_point_theorems}

?? consider Bert chap 5.5 or BTN chap 3.4.2 

\section{Existence of minima revisited*}
\label{sec:existence_minima_revisited}

discuss 0 being in int(dom(f)) being equivalent to f* having no directions
of recession.  give nice proposition/theorem about existence of solutions of 
$$
\minimize_\theta \quad g(X \theta) + h_C(x),
$$
where $C$ is a convex set. prove in exercises. specialize to generalized linear
model case, interpret sufficient conditions, and prove theorems in Chapter
\ref{sec:maximum_likelihood} about existence of logistic and Poisson regression
solutions. 

MAKE Sure to talk about interpretation of logistic interpretation


?? 

\section{Plurality of dual problems*}
\label{sec:plurality_dual_problems}

\begin{xcb}{Exercises}
\begin{enumerate}[label=\thechapter.\arabic*]
\settowidth{\leftmargini}{00.00.\hskip\labelsep}
\item \label{ex:trace_norm_semidefinite} 
 
% \begin{enumerate}
% \item[(a, 5pts)]
% Show that computing the trace norm of a matrix, i.e., computing $\| X \|_{\tr}$, can be expressed as the following (convex) optimization problem:
% \begin{equation}
% \begin{array}{ll}
% \maximizewrt{Y \in \mathbb{R}^{m \times n}} & \tr(X^T Y) \\
% \subjectto & 
% \left[
% \begin{array}{cc}
% I_m & Y \\
% Y^T & I_n
% \end{array}
% \right]
% \succeq 0,
% \end{array}
% \label{eq:aa:primal}
% \end{equation}
% where $I_p$ is the $p \times p$ identity matrix.  (By the way, problem \eqref{eq:aa:primal} is a semidefinite program; more on this in part (d) below.)

% Hint: think about using the ``Schur complement'' somewhere here.  A good reference for this might be Section A.5.5 in the ``Convex Optimization'' book, by Stephen Boyd and Lieven Vandenberghe.

% \item[(b, 5pts)]
% Show that the dual problem associated with \eqref{eq:aa:primal} can be expressed as
% %\begin{equation}
% %\begin{array}{ll}
% %\minimizewrt{\substack{W_{1} \in \symm^{m}, \\ W_{2} \in \symm^{n}}} & (1/2) ( \tr(I_m W_1) + \tr(I_n W_2) ) \\
% %\subjectto &
% %\left[
% %\begin{array}{cc}
% %W_1 & X \\
% %X^T & W_2
% %\end{array}
% %\right]
% %\succeq 0,
% %\end{array}
% %\label{eq:aa:dual}
% %\end{equation}
% \begin{equation}
% \begin{array}{ll}
% \minimizewrt{\substack{W_{1} \in \symm^{m}, \\ W_{2} \in \symm^{n}}} & \tr(W_{1}) + \tr(W_{2}) \\
% \subjectto & 
% \left[
% \begin{array}{cc}
% W_{1} & (1/2) X \\
% (1/2) X^T & W_{2}
% \end{array}
% \right]
% \succeq 0,
% \end{array}
% \label{eq:aa:dual}
% \end{equation}
% where, just to remind you, $\symm^p$ is the space of $p \times p$ real, symmetric matrices.

% \item[(c, 2pts)]
% Show that the optimal values for problems \eqref{eq:aa:primal} and \eqref{eq:aa:dual} are equal to each other, and that both optimal values are attained.

% \item[(d, 5pts)]
% In the \textit{matrix completion problem}, we want to find a matrix $X \in \reals^{m \times n}$ of low rank that is close, in a squared error sense, to some observed matrix $Z \in \reals^{m \times n}$.  We do not assume that all of the entries of $Z$ are observed, so we will look at the squared error over $Z$'s observed entries only, which we store in a set $\Omega$ of (observed) row and column indices.  Putting all this together leads us to the following (convex) optimization problem:
% \begin{equation}
% \begin{array}{ll}
% \minimizewrt{X \in \mathbb{R}^{m \times n}} & \sum_{(i,j) \in \Omega} ( X_{ij} - Z_{ij} )^2 + \lambda \| X \|_{\tr},
% \end{array}
% \label{eq:aa:mtxcomp}
% \end{equation}
% with tuning parameter $\lambda > 0$.

% Show that problem \eqref{eq:aa:mtxcomp} can be expressed as a semidefinite program of the form
% \begin{equation*}
% \begin{array}{ll}
% \minimizewrt{x \in \mathbb{R}^p} & c^T x \\
% \subjectto & x_1 A_1 + \cdots + x_p A_p \preceq B,
% \end{array}
% \end{equation*}
% for some fixed $c, B, A_i, \; i=1,\ldots,p$.

% Hint: you will probably need to use each of the above parts (in different ways) here.

\end{enumerate}
\end{xcb}