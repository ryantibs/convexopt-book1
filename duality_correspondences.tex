\chapter{Duality correspondences}
\label{chap:duality_correspondences}

\section{Dual norms}
\label{sec:dual_norms}

\section{Fenchel duality}

Fenchel duality: gives general relationship between primal and dual in terms of
conjugates 

when is dual of dual the primal? Answer with f** = f. Pull back to QPs? 

\section{Dual cones*}
\label{sec:dual_cones}

\section{Dual polyhedra*}
\label{sec:dual_polyhedra}

\section{Polar sets*}
\label{sec:polar_sets}

Make sure to mention that conjugate of indicator of convex cone is indicator of
its polar cone. 

Also where do we want to prove conjugate of sum of indicators is conjugate of
indicator of intersection?

Dual in terms of gauge of polar set 

\section{Conic duality*}
\label{sec:conic_duality}

How to write in a unified way?

Write cone program as follows in a way that allows Fenchel duality to be applied
elegantly. For example, min $c^\T x$ subject $Ax + b \in K$

See Theorems 3.2.2 and 3.2.5 in Ben Tal and Nemirovski. This is more general and 
it will be helpful to write this as: min $f(x)$ subject to $h(x) \leq 0$, $Ax =
b$, where $h$ is $K$-convex and $K$ is a regular cone (regular means: closed,
convex, nonempty interior, pointed)

\begin{equation}
\label{eq:primal_conic}
\begin{alignedat}{2}
&\minimize_x \quad && f(x) \\
&\st \quad && h(x) \leq_K 0 \\
& && Ax = b.
\end{alignedat}
\end{equation}

\index{Slater's condition}
\index{strong duality}
\begin{Theorem}
\label{thm:slater_conic}
TODO. MAKE SURE TO GIVE STATEMENTS OF ATTAINMENT SO WE CAN USE THAT IN AN
EARLIER EXERCISE.    

State as in the earlier Slater, that we get strong duality. Then say,
furthermore, when the common optimal value is finite, that this is attained in 
the dual, i.e., dual solution exists. 

% Consider the optimization problem \eqref{eq:primal_problem}, where after 
% relabeling, if needed, we take $h_1, \dots, h_r$ to be inequality constraint
% functions which are affine ($r = 0$ if none are affine). Assume the following.         

% \begin{enumerate}[label=(\roman*)]
% \item The functions $f$ and $h_i$, $i=1,\dots,m$ are convex, and $\ell_j$,
%   $j=1,\dots,k$ are affine; in other words, problem \eqref{eq:primal_problem} is
%   convex, and we can write its equality constraints as $Ax = b$.      

% \item There exists $x \in \relint(D)$, where \smash{$D = \dom(f) \cap
%     \bigcap_{i=r+1}^m \dom(h_i)$} denotes the common effective domain, such that      
%   \begin{equation}
%   \label{eq:slater_condition}
%   h_i(x) \leq 0 \;\, \text{for all $i \leq r$}, \quad
%   h_i(x) < 0 \;\, \text{for all $i \geq r+1$}, \quad 
%   Ax = b.
%   \end{equation}
% \end{enumerate}

% Then strong duality holds: the optimal values $f^\star$ in problem
% \eqref{eq:primal_problem} and $g^\star$ in the corresponding dual problem  
% \eqref{eq:dual_problem} satisfy $f^\star = g^\star$.
\end{Theorem}

\section{Minimax theorems*}
\label{sec:minimax_theorems}

?? consider Bert chap 5.5 or BTN chap 3.4.2 

\section{Existence of minima revisited*}
\label{sec:existence_minima_revisited}

discuss 0 being in int(dom(f)) being equivalent to f* having no directions
of recession.  give nice proposition/theorem about existence of solutions of 
$$
\minimize_\theta \quad g(X \theta) + h_C(x),
$$
where $C$ is a convex set. prove in exercises. specialize to generalized linear
model case, interpret sufficient conditions, and prove theorems in Chapter
\ref{sec:maximum_likelihood} about existence of logistic and Poisson regression
solutions. 

MAKE Sure to talk about interpretation of logistic interpretation

\SkipTocEntry\section*{Chapter notes}

Exercises \ref{ex:farkas_variations_conic} and
\ref{ex:convex_theorem_alternatives_conic} are based on \cite{bental2023convex}.           

\begin{xcb}{Exercises}
\begin{enumerate}[label=\thechapter.\arabic*]
\settowidth{\leftmargini}{00.00.\hskip\labelsep}

\item \label{ex:dubovitski_milutin}
Dubovitski-Milutin lemma

\item \label{ex:farkas_variations_conic} 
  In this exercise, we will show that the Dubovitski-Milutin lemma, studied in
  the last exercise, leads to an important variation on Farkas' lemma. This will 
  slightly generalize the result from Exercise \ref{ex:farkas_variations} part
  b, which was a key tool used in proving the convex theorem of alternatives
  from Exercise \ref{ex:convex_theorem_alternatives}. Given $A \in \R^{k \times
    d}$, $b \in \R^k$, $g \in \R^d$, $h \in \R$, and a convex set $D \subseteq
  \R^d$, with $b \geq 0$ and $0 \in \interior(D)$, consider two statements:       
  \begin{itemize}
  \item for all $x \in D$, it holds that $Ax \leq b \implies g^\T x \leq h$;  
  \item there exists $\mu \in \R^k$ such that $\mu \geq 0$ and $\mu^\T (Ax - b) 
    \geq g^\T x - h$ for all $x \in D$.
  \end{itemize}
  We will prove that these two statements are equivalent in what follows.

\begin{enumerate}[label=\alph*.]
\item Prove that the second statement implies the first.

\item To prove that the first statement implies the second, start by defining 
  \[
  K_1 = \closure \Big\{ (x,t) \in \R^d \times \R: t > 0, \, x/t \in D \Big\}
  \quad \text{and} \quad K_2 = \Big\{ (x,t) \in \R^d \times \R: Ax \leq tb
  \Big\}.  
  \]
  Show that these are convex cones, whose relative interiors have nonempty  
  intersection.

\item Define $f = (g,-h) \in \R^{d+1}$, and show that $f^\T (x,t) \leq 0$ for
  all $(x,t) \in K_1 \cap K_2$.  

\item Use Exercise \ref{ex:dubovitski_milutin} to argue that we can decompose
  $f = \psi + \phi$, where $\psi^\T(x,t) \leq 0$ for all $x \in K_1$, and
  $\phi^\T (x,t) \leq 0$ for all $x \in K_2$.

\item Use Exercise \ref{ex:farkas_variations} part a to show that there exists
  $\mu \geq 0$ such that $[A \, -b]^\T \mu = \phi$, in other words, $\phi^\T
  (x,t) = \mu^\T (Ax - tb)$ for all $x,t$. Use this to prove the desired result.       
\end{enumerate}

\item \label{ex:convex_theorem_alternatives_conic}
  just guide them through the conic theorem of alternatives from 3.2.2 in BTN
  and the conic slater condition in the main text, by generalizing earlier
  proofs, and using the above Farkas lemma variation

\item \label{ex:trace_norm_semidefinite} 
 



% \begin{enumerate}
% \item[(a, 5pts)]
% Show that computing the trace norm of a matrix, i.e., computing $\| X \|_{\tr}$, can be expressed as the following (convex) optimization problem:
% \begin{equation}
% \begin{array}{ll}
% \maximizewrt{Y \in \mathbb{R}^{m \times n}} & \tr(X^T Y) \\
% \subjectto & 
% \left[
% \begin{array}{cc}
% I_m & Y \\
% Y^T & I_n
% \end{array}
% \right]
% \succeq 0,
% \end{array}
% \label{eq:aa:primal}
% \end{equation}
% where $I_p$ is the $p \times p$ identity matrix.  (By the way, problem \eqref{eq:aa:primal} is a semidefinite program; more on this in part (d) below.)

% Hint: think about using the ``Schur complement'' somewhere here.  A good reference for this might be Section A.5.5 in the ``Convex Optimization'' book, by Stephen Boyd and Lieven Vandenberghe.

% \item[(b, 5pts)]
% Show that the dual problem associated with \eqref{eq:aa:primal} can be expressed as
% %\begin{equation}
% %\begin{array}{ll}
% %\minimizewrt{\substack{W_{1} \in \symm^{m}, \\ W_{2} \in \symm^{n}}} & (1/2) ( \tr(I_m W_1) + \tr(I_n W_2) ) \\
% %\subjectto &
% %\left[
% %\begin{array}{cc}
% %W_1 & X \\
% %X^T & W_2
% %\end{array}
% %\right]
% %\succeq 0,
% %\end{array}
% %\label{eq:aa:dual}
% %\end{equation}
% \begin{equation}
% \begin{array}{ll}
% \minimizewrt{\substack{W_{1} \in \symm^{m}, \\ W_{2} \in \symm^{n}}} & \tr(W_{1}) + \tr(W_{2}) \\
% \subjectto & 
% \left[
% \begin{array}{cc}
% W_{1} & (1/2) X \\
% (1/2) X^T & W_{2}
% \end{array}
% \right]
% \succeq 0,
% \end{array}
% \label{eq:aa:dual}
% \end{equation}
% where, just to remind you, $\symm^p$ is the space of $p \times p$ real, symmetric matrices.

% \item[(c, 2pts)]
% Show that the optimal values for problems \eqref{eq:aa:primal} and \eqref{eq:aa:dual} are equal to each other, and that both optimal values are attained.

% \item[(d, 5pts)]
% In the \textit{matrix completion problem}, we want to find a matrix $X \in \reals^{m \times n}$ of low rank that is close, in a squared error sense, to some observed matrix $Z \in \reals^{m \times n}$.  We do not assume that all of the entries of $Z$ are observed, so we will look at the squared error over $Z$'s observed entries only, which we store in a set $\Omega$ of (observed) row and column indices.  Putting all this together leads us to the following (convex) optimization problem:
% \begin{equation}
% \begin{array}{ll}
% \minimizewrt{X \in \mathbb{R}^{m \times n}} & \sum_{(i,j) \in \Omega} ( X_{ij} - Z_{ij} )^2 + \lambda \| X \|_{\tr},
% \end{array}
% \label{eq:aa:mtxcomp}
% \end{equation}
% with tuning parameter $\lambda > 0$.

% Show that problem \eqref{eq:aa:mtxcomp} can be expressed as a semidefinite program of the form
% \begin{equation*}
% \begin{array}{ll}
% \minimizewrt{x \in \mathbb{R}^p} & c^T x \\
% \subjectto & x_1 A_1 + \cdots + x_p A_p \preceq B,
% \end{array}
% \end{equation*}
% for some fixed $c, B, A_i, \; i=1,\ldots,p$.

% Hint: you will probably need to use each of the above parts (in different ways) here.

\item Exercise on plurality of dual problems?

\end{enumerate}
\end{xcb}