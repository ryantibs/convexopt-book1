\documentclass[oneside]{gsm-l} % Remove oneside for printing
%    For the GSM series, theorem heads are set in bold and not indented.
%    To use the standard "amsbook" style for theorem heads instead,
%    comment out the previous \documentclass line and use this one:
%\documentclass[theoremb]{gsm-l}

%    For use when working on individual chapters
%\includeonly{}

%    Include other referenced packages here.
\input{macros}
\numberwithin{section}{chapter}
\numberwithin{equation}{chapter}
\numberwithin{figure}{chapter}

%    For a single index; for multiple indexes, see the manual
%    "AMS Author Handbook, Monograph Classes", included in the 
%    author package).
\makeindex

\begin{document}

\frontmatter

\title{Convex Optimization for Statistics and Machine Learning, Volume I:
  Analysis} 
\author{Ryan J.\ Tibshirani}

%    The 2010 edition of the Mathematics Subject Classification is
%    the current definitive version. 
%\subjclass[2010]{Primary }
%\keywords{}

\maketitle

%    Dedication.  If the dedication is longer than a few lines,
%    remove the centering instructions and the line break.
%\cleardoublepage
%\thispagestyle{empty}
%\vspace*{13.5pc}
%\begin{center}
%  Dedication text (use \\[2pt] for line break if necessary)
%\end{center}
%\cleardoublepage

\setcounter{page}{5} % Change page number to 7 if a dedication is present
\setcounter{tocdepth}{1} % Depth 1 means include sections
\tableofcontents

%    Include unnumbered chapters (preface, acknowledgments, etc.) here.
%\include{}

\mainmatter
%    Include main chapters here.

\part{Introduction}
\include{why_read_this_book}
\include{how_read_this_book}
%\include{} % what about deep learning

\part{Fundamentals}
\include{convex_sets_functions}
\include{optimization_basics}
\include{canonical_problems}

\part{Subdifferential theory}
\include{subgradients}
\include{proximal_mappings}
\include{convex_conjugates} 

\part{Duality and optimality}
\include{lagrangian_duality}
\include{kkt_conditions}
\include{duality_correspondences}

%\part{Case studies}
%\include{svm}

\part{Advanced topics}
\include{bregman_divergences} % See sketch in bregman_notes
\include{lasso}

% Uniqueness without strict convexity?
% - This could be a home for my result with general polyhedral penalties. 

% Caratheodory theorems on sparsity?
% - Lasso: see Q1 on my Statistical Learning Stat 241B Homework 3: 
% https://www.stat.berkeley.edu/~ryantibs/statlearn-s24/homeworks/homework3.pdf 
% - Group lasso: see Thm 12 here: https://arxiv.org/abs/2305.16534
% - Locally adaptive regression splines, RTV splines, etc.

% Monotone operators?

% Something on pertubation/sensitivity analysis? MAYBE if I can find enough?
% - Chapter 5.6 of BV gives KKT interpretation.
% - My own proof of continuity of solution paths?
% - Do something basic from variational analysis?

% Parametric convex programming? PROBABLY NO. Plan to cover in lasso chapter.
% Optimization in Hilbert space? NO. That's been covered abundantly elsewhere.

\appendix
\include{app_point_set_topology}
\include{app_multivariate_calculus}
\include{app_linear_algebra}
% Numerical linear algebra?

\backmatter
%    Bibliographies can be prepared with BibTeX using amsplain,
%    amsalpha, or (for author-year style) natbib.
{\RaggedRight
\bibliographystyle{amsalpha}
\bibliography{general}}

%    See note above about multiple indexes.
\input{cross_references}
\printindex

\end{document}