\chapter{Duality correspondences}
\label{chap:duality_correspondences}

In this chapter we explore various topics which are adjacent to our treatment of
Lagrangian duality and the KKT conditions in the last two chapters, and describe
connections as well as implications brought about by new perspectives.    

\section{Dual norms}
\label{sec:dual_norms}

Given a norm $\|\cdot\|$ on $\R^d$, its \emph{dual norm} is denoted
$\|\cdot\|_*$, and defined by 
\index{dual norm}  
\begin{equation}
\label{eq:dual_norm}
\|z\|_* = \sup_{\|x\| \leq 1} \, x^\T z.
\end{equation}
It is not hard to check that the dual norm $\|\cdot\|_*$ is itself a norm on
$\R^d$, that is, it satisfies the triangle inequality, absolute homogeneity, and
positive definiteness (Exercise \ref{ex:dual_norm_check}). 

As a first property to record, the dual of the dual norm is the original norm:
$\|\cdot\|_{**} = \|\cdot\|$, which can be verified using the theory of
Lagrangian duality (Exercise \ref{ex:dual_norm_dual}). This allows us to write  
\begin{equation}
\label{eq:norm_dual2}
\|x\| = \sup_{\|z\|_* \leq 1} \, z^\T x.
\end{equation}
The representations \eqref{eq:dual_norm}, \eqref{eq:norm_dual2}, combined with
the Danskin-Bertsekas theorem and its translation for subgradients of norms
\eqref{eq:norm_subgradients}, leads to the following simple but elementary
facts (Exercise \ref{ex:dual_norm_subgradients}).

\index{norm!subgradients}
\index{dual norm!subgradients}
\index{H{\"o}lder's inequality}
\begin{Theorem}
\label{thm:dual_norm_subgradients}
For a norm $\|\cdot\|$ and dual its $\|\cdot\|_*$, it holds that $x^\T z \leq
\|x\| \|z\|_*$ for any $x,y$. Furthermore, for any $x \not= 0$ and $z \not= 0$,
the following statements are equivalent:
\begin{enumerate}[label=(\roman*)]
\item $x^\T z = \|x\| \|z\|_*$;
\item $x/\|x\| \in \partial \|z\|_*$; 
\item $z/\|z\|_* \in \partial \|x\|$;
\item $x/\|x\| \in \argmax_{\|u\| \leq 1} \, u^\T z$; 
\item $z/\|z\|_* \in \argmax_{\|u\|_* \leq 1} \, u^\T x$. 
\end{enumerate}
\end{Theorem}

The conclusion given above, that $x^\T z \leq \|x\| \|z\|_*$ for any $x,y$, can
be seen as a generalization of \emph{H{\"o}lder's inequality}, which is
typically stated for  $\ell_p$ norms. The examples below confirm that this is
indeed a strict generalization (as the dual of the $\ell_p$ norm is the $\ell_q$
norm for conjugate pairs $p,q$).   

\begin{Example}
The following are examples of dual norms for common norms, which can be verified
by direct calculation (Exercises \ref{ex:lp_norm_dual}--\ref{ex:trace_norm_dual}). 

\begin{enumerate}[label=\alph*., ref=\alph*]
\index{lp norm@$\ell_p$ norm!dual}
\item \parlab{xa:lp_norm_dual} 
  For the $\ell_p$ norm $\|\cdot\|_p$ with $p \geq 1$, its dual norm is
  $\|\cdot\|_q$ for $q \geq 1$ such that $1/p + 1/q = 1$.  

\item \parlab{xa:scaled_euclidean_norm_dual}  
  For the scaled Euclidean norm $\|\cdot\|_A$ in a given positive definite
  matrix $A$, which recall is defined by \smash{$\|x\|_H = \sqrt{x^\T A x}$},
  its dual norm is \smash{$\|\cdot\|_{A^{-1}}$}.  

\index{trace norm!dual}
\index{operator norm!dual}
\item \parlab{xa:trace_norm_dual}  
  For the trace norm $\|\cdot\|_{\tr}$, its dual norm is the operator norm
  $\|\cdot\|_{\op}$.   
\end{enumerate}
\end{Example}

Dual norms bear an interesting connection to conjugacy. The convex conjugate of
a norm is the characteristic function the dual norm unit ball (Exercise 
\ref{ex:dual_norm_conjugate}): 
\index{dual norm!conjugate}
\begin{equation}
\label{eq:dual_norm_conjugate}
\text{for $f(x) = \|x\|$, we have $f^*(u) = I_{\{z : \|z\|_* \leq 1\}}(u)$}.
\end{equation}
Meanwhile, an arguably even simpler relationship holds for the squared
norm---the convex conjugate of one half times a norm squared is one half times
its dual norm squared (Exercise \ref{ex:dual_norm2_conjugate}):    
\begin{equation}
\label{eq:dual_norm2_conjugate}
\text{for $f(x) = \frac{1}{2} \|x\|^2$, we have $f^*(u) = \frac{1}{2}
  \|u\|_*^2$}. 
\end{equation}
These facts are especially useful in the context of Fenchel duality, which we
cover next.  

\section{Fenchel duality}

Fenchel duality: gives general relationship between primal and dual in terms of
conjugates 

when is dual of dual the primal? Answer with f** = f. Pull back to QPs? 

\section{Dual cones*}
\label{sec:dual_cones}

\section{Dual polyhedra*}
\label{sec:dual_polyhedra}

\section{Polar sets*}
\label{sec:polar_sets}

Make sure to mention that conjugate of indicator of convex cone is indicator of
its polar cone. 

Also where do we want to prove conjugate of sum of indicators is conjugate of
indicator of intersection?

Dual in terms of gauge of polar set 

\section{Conic duality*}
\label{sec:conic_duality}

How to write in a unified way?

Write cone program as follows in a way that allows Fenchel duality to be applied
elegantly. For example, min $c^\T x$ subject $Ax + b \in K$

See Theorems 3.2.2 and 3.2.5 in Ben Tal and Nemirovski. This is more general and 
it will be helpful to write this as: min $f(x)$ subject to $h(x) \leq 0$, $Ax =
b$, where $h$ is $K$-convex and $K$ is a regular cone (regular means: closed,
convex, nonempty interior, pointed)

\begin{equation}
\label{eq:primal_conic}
\begin{alignedat}{2}
&\minimize_x \quad && f(x) \\
&\st \quad && h(x) \leq_K 0 \\
& && Ax = b.
\end{alignedat}
\end{equation}

\index{Slater's condition}
\index{strong duality}
\begin{Theorem}
\label{thm:slater_conic}
TODO. MAKE SURE TO GIVE STATEMENTS OF ATTAINMENT SO WE CAN USE THAT IN AN
EARLIER EXERCISE.    

State as in the earlier Slater, that we get strong duality. Then say,
furthermore, when the common optimal value is finite, that this is attained in 
the dual, i.e., dual solution exists. 

% Consider the optimization problem \eqref{eq:primal_problem}, where after 
% relabeling, if needed, we take $h_1, \dots, h_r$ to be inequality constraint
% functions which are affine ($r = 0$ if none are affine). Assume the following.         

% \begin{enumerate}[label=(\roman*)]
% \item The functions $f$ and $h_i$, $i=1,\dots,m$ are convex, and $\ell_j$,
%   $j=1,\dots,k$ are affine; in other words, problem \eqref{eq:primal_problem} is
%   convex, and we can write its equality constraints as $Ax = b$.      

% \item There exists $x \in \relint(D)$, where \smash{$D = \dom(f) \cap
%     \bigcap_{i=r+1}^m \dom(h_i)$} denotes the common effective domain, such that      
%   \begin{equation}
%   \label{eq:slater_condition}
%   h_i(x) \leq 0 \;\, \text{for all $i \leq r$}, \quad
%   h_i(x) < 0 \;\, \text{for all $i \geq r+1$}, \quad 
%   Ax = b.
%   \end{equation}
% \end{enumerate}

% Then strong duality holds: the optimal values $f^\star$ in problem
% \eqref{eq:primal_problem} and $g^\star$ in the corresponding dual problem  
% \eqref{eq:dual_problem} satisfy $f^\star = g^\star$.
\end{Theorem}

\section{Minimax theorems*}
\label{sec:minimax_theorems}

?? consider Bert chap 5.5 or BTN chap 3.4.2 

\section{Existence of minima revisited*}
\label{sec:existence_minima_revisited}

discuss 0 being in int(dom(f)) being equivalent to f* having no directions
of recession.  give nice proposition/theorem about existence of solutions of 
$$
\minimize_\theta \quad g(X \theta) + h_C(x),
$$
where $C$ is a convex set. prove in exercises. specialize to generalized linear
model case, interpret sufficient conditions, and prove theorems in Chapter
\ref{sec:maximum_likelihood} about existence of logistic and Poisson regression
solutions. 

MAKE Sure to talk about interpretation of logistic interpretation

\SkipTocEntry\section*{Chapter notes}

Exercises \ref{ex:farkas_variations_conic} and
\ref{ex:convex_theorem_alternatives_conic} are based on \cite{bental2023convex}.           

\begin{xcb}{Exercises}
\begin{enumerate}[label=\thechapter.\arabic*]
\settowidth{\leftmargini}{00.00.\hskip\labelsep}
\item \label{ex:dual_norm_check}
Dual norm is a norm. 

\item \label{ex:dual_norm_dual}
Dual of dual norm is original.

\item \label{ex:dual_norm_subgradients}
Prove Theorem \ref{thm:dual_norm_subgradients}. Can show each thing is
equivalent to (i) directly. 

\item \label{ex:lp_norm_dual}
\item \label{ex:scaled_euclidean_norm_dual}
\item \label{ex:trace_norm_dual}

For the Schatten $p$-norm $\|\cdot\|_p$, which recall is a norm on
  matrices defined by $\|X\|_p = \|\sigma(X)\|_p$

\item \label{ex:dual_norm_conjugate}
\item \label{ex:dual_norm2_conjugate}

Can prove this via Theorem \ref{thm:dual_norm_subgradients}. Can rearrange the
statement of the theorem similar to how we wrote it in the coordinate flow
notes: define the set-valued operator  
\[
d(x) = \|x\|_* \cdot \partial \|x\|_*,
\]
and its dual operator 
\[
d_*(y) = \|y\| \cdot \partial \|y\|.
\]

\begin{lemma}
The following holds for any norm $\|\cdot\|$ and its dual norm
$\|\cdot\|_*$. 

\begin{enumerate}[label=(\roman*)]
% \item Positive homogeneity: $d(ax) = a \cdot d(x)$ for $a \geq 0$. 
\item Norm equality: $\|x\|_* = \|y\|$, for $y \in d(x)$.
\item Inner product equality: $x^\T y = \|x\|_*^2$, for $y \in d(x)$.
\item Dual representation: $y \in d(x) \iff x \in d_*(y)$.
\item Conjugate representation: $d(x) = \argmax_y \, x^\T y - \frac{1}{2}
  \|y\|^2$. 
\end{enumerate}
\end{lemma}

\begin{proof}
Facts (i) and (ii) follow directly by definition of $d$ and $\|\cdot\|_*$. Next
define \smash{$g(x) = \frac{1}{2} \|x\|_*^2$}. One can verify that $\partial
g(x) = d(x)$. Moreover, the conjugate of $g$ is  
\[
g^*(y) = \sup_x \, y^\T x - \frac{1}{2} \|x\|_*^2.
\]
By taking a subgradient and setting this equal to zero, we see that the supremum
is achieved for $y \in d(x)$. Plugging this in, and using facts (ii) and (i) gives 
\[
g^*(y) = \frac{1}{2} \|y\|^2.
\]
Therefore facts (iii) and (iv) follow from the conjugate relation $y \in
\partial g(x) \iff x \in \partial g^*(x)$.   
\end{proof} 

\item \label{ex:dubovitski_milutin}
Dubovitski-Milutin lemma

\item \label{ex:farkas_variations_conic} 
  In this exercise, we will show that the Dubovitski-Milutin lemma, studied in
  the last exercise, leads to an important variation on Farkas' lemma. This will 
  slightly generalize the result from Exercise \ref{ex:farkas_variations} part
  b, which was a key tool used in proving the convex theorem of alternatives
  from Exercise \ref{ex:convex_theorem_alternatives}. Given $A \in \R^{k \times
    d}$, $b \in \R^k$, $g \in \R^d$, $h \in \R$, and a convex set $D \subseteq
  \R^d$, with $b \geq 0$ and $0 \in \interior(D)$, consider two statements:       
  \begin{itemize}
  \item for all $x \in D$, it holds that $Ax \leq b \implies g^\T x \leq h$;  
  \item there exists $\mu \in \R^k$ such that $\mu \geq 0$ and $\mu^\T (Ax - b) 
    \geq g^\T x - h$ for all $x \in D$.
  \end{itemize}
  We will prove that these two statements are equivalent in what follows.

\begin{enumerate}[label=\alph*.]
\item Prove that the second statement implies the first.

\item To prove that the first statement implies the second, start by defining 
  \[
  K_1 = \closure \Big\{ (x,t) \in \R^d \times \R: t > 0, \, x/t \in D \Big\}
  \quad \text{and} \quad K_2 = \Big\{ (x,t) \in \R^d \times \R: Ax \leq tb
  \Big\}.  
  \]
  Show that these are convex cones, whose relative interiors have nonempty  
  intersection.

\item Define $f = (g,-h) \in \R^{d+1}$, and show that $f^\T (x,t) \leq 0$ for
  all $(x,t) \in K_1 \cap K_2$.  

\item Use Exercise \ref{ex:dubovitski_milutin} to argue that we can decompose
  $f = \psi + \phi$, where $\psi^\T(x,t) \leq 0$ for all $x \in K_1$, and
  $\phi^\T (x,t) \leq 0$ for all $x \in K_2$.

\item Use Exercise \ref{ex:farkas_variations} part a to show that there exists
  $\mu \geq 0$ such that $[A \, -b]^\T \mu = \phi$, in other words, $\phi^\T
  (x,t) = \mu^\T (Ax - tb)$ for all $x,t$. Use this to prove the desired result.       
\end{enumerate}

\item \label{ex:convex_theorem_alternatives_conic}
  just guide them through the conic theorem of alternatives from 3.2.2 in BTN
  and the conic slater condition in the main text, by generalizing earlier
  proofs, and using the above Farkas lemma variation

\item \label{ex:trace_norm_semidefinite} 
 



% \begin{enumerate}
% \item[(a, 5pts)]
% Show that computing the trace norm of a matrix, i.e., computing $\| X \|_{\tr}$, can be expressed as the following (convex) optimization problem:
% \begin{equation}
% \begin{array}{ll}
% \maximizewrt{Y \in \mathbb{R}^{m \times n}} & \tr(X^T Y) \\
% \subjectto & 
% \left[
% \begin{array}{cc}
% I_m & Y \\
% Y^T & I_n
% \end{array}
% \right]
% \succeq 0,
% \end{array}
% \label{eq:aa:primal}
% \end{equation}
% where $I_p$ is the $p \times p$ identity matrix.  (By the way, problem \eqref{eq:aa:primal} is a semidefinite program; more on this in part (d) below.)

% Hint: think about using the ``Schur complement'' somewhere here.  A good reference for this might be Section A.5.5 in the ``Convex Optimization'' book, by Stephen Boyd and Lieven Vandenberghe.

% \item[(b, 5pts)]
% Show that the dual problem associated with \eqref{eq:aa:primal} can be expressed as
% %\begin{equation}
% %\begin{array}{ll}
% %\minimizewrt{\substack{W_{1} \in \symm^{m}, \\ W_{2} \in \symm^{n}}} & (1/2) ( \tr(I_m W_1) + \tr(I_n W_2) ) \\
% %\subjectto &
% %\left[
% %\begin{array}{cc}
% %W_1 & X \\
% %X^T & W_2
% %\end{array}
% %\right]
% %\succeq 0,
% %\end{array}
% %\label{eq:aa:dual}
% %\end{equation}
% \begin{equation}
% \begin{array}{ll}
% \minimizewrt{\substack{W_{1} \in \symm^{m}, \\ W_{2} \in \symm^{n}}} & \tr(W_{1}) + \tr(W_{2}) \\
% \subjectto & 
% \left[
% \begin{array}{cc}
% W_{1} & (1/2) X \\
% (1/2) X^T & W_{2}
% \end{array}
% \right]
% \succeq 0,
% \end{array}
% \label{eq:aa:dual}
% \end{equation}
% where, just to remind you, $\symm^p$ is the space of $p \times p$ real, symmetric matrices.

% \item[(c, 2pts)]
% Show that the optimal values for problems \eqref{eq:aa:primal} and \eqref{eq:aa:dual} are equal to each other, and that both optimal values are attained.

% \item[(d, 5pts)]
% In the \textit{matrix completion problem}, we want to find a matrix $X \in \reals^{m \times n}$ of low rank that is close, in a squared error sense, to some observed matrix $Z \in \reals^{m \times n}$.  We do not assume that all of the entries of $Z$ are observed, so we will look at the squared error over $Z$'s observed entries only, which we store in a set $\Omega$ of (observed) row and column indices.  Putting all this together leads us to the following (convex) optimization problem:
% \begin{equation}
% \begin{array}{ll}
% \minimizewrt{X \in \mathbb{R}^{m \times n}} & \sum_{(i,j) \in \Omega} ( X_{ij} - Z_{ij} )^2 + \lambda \| X \|_{\tr},
% \end{array}
% \label{eq:aa:mtxcomp}
% \end{equation}
% with tuning parameter $\lambda > 0$.

% Show that problem \eqref{eq:aa:mtxcomp} can be expressed as a semidefinite program of the form
% \begin{equation*}
% \begin{array}{ll}
% \minimizewrt{x \in \mathbb{R}^p} & c^T x \\
% \subjectto & x_1 A_1 + \cdots + x_p A_p \preceq B,
% \end{array}
% \end{equation*}
% for some fixed $c, B, A_i, \; i=1,\ldots,p$.

% Hint: you will probably need to use each of the above parts (in different ways) here.

\item Exercise on plurality of dual problems?

\end{enumerate}
\end{xcb}