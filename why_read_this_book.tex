\chapter{Why read this book?}
\label{chap:why_this_book}

\section{Why (convex) optimization?}

% Optimization lies at the heart of a lot of the methodology in machine learning 
% and computational statistics. A solid working knowledge of optimization is 
% important for students and researchers in machine learning. Clearly this would 
% help for computational purposes (knowing what optimization algorithms are
% applicable for any given problem, and why we might expect some to perform
% better or worse than others). Perhaps less expected, as we will see in later
% chapters in this book, knowledge of optimization will also help us to better
% understand the properties of machine learning methods (and even derive
% advanced statistical theory for them). For example, it would be hard to
% understand the basic properties of the lasso (least absolute shrinkage and
% selection operator) without understanding subgradients and the subgradient
% optimality condition; and it would be hard to understand support vector
% machines without understanding duality and the KKT (Karush-Kuhn-Tucker)
% conditions. 

% What makes one optimization problem hard (intractable) and another easy
% (tractable)?  Of course, it is not possible to answer this question in full
% generality, but \emph{convexity} is now widely recognized as a critical
% attribute that can distinguish a tractable optimization problem from an
% intractable one. We must be careful not to oversell this perspective: there
% certainly are some easy nonconvex problems, and some hard convex ones. But, if
% were to pick just a single problem attribute on which to place our bets, then
% convexity is probably the right one. 

\paragraph{What about deep learning?}

\section{Why another book?}

\paragraph{What about algorithms?}

% Infrequently touched on here, but hinted at in several places. Algorithms will
% be at the center of Part II...