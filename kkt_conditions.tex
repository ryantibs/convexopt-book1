\chapter{Karush-Kuhn-Tucker conditions}
\label{chap:kkt_conditions}

\section{Saddle point condition}
\label{sec:saddle_point_condition}

In this chapter, we will explore conditions which characterize solutions in
constrained optimization problems, the most well-known being a set of conditions
known as the Karush-Kuhn-Tucker (KKT) conditions. This flows naturally from our 
study of Lagrange duality in the last chapter. In fact, the last chapter already
established a critical relationship between primal and dual solutions and what
are known as saddle points of the Lagrangian. These arguments were given in 
passing, and here we revisit them, as they will lay the foundation for the KKT 
conditions, which we cover next.           

As in the last chapter, consider a primal problem
\begin{equation}
\label{eq:primal_problem2}
\begin{alignedat}{2}
&\minimize_x \quad && f(x) \\
&\st \quad && h_i(x) \leq 0, \; i=1,\dots,m \\ 
& && \ell_j(x) = 0, \; j=1,\dots,k,
\end{alignedat}
\end{equation}
whose associated Lagrangian and dual function are
\begin{align*}
L(x,u,v) &= f(x) + \sum_{i=1}^m u_i h_i(x) + \sum_{j=1}^k v_j \ell_j(x), \\ 
g(u,v) &= \inf_x \, L(x,u,v), 
\end{align*}
and whose associated dual problem is 
\begin{equation}
\label{eq:dual_problem2}
\maximize_{u,v} \quad g(u,v) \quad \st \quad u \geq 0.
\end{equation}
We do not assume that \eqref{eq:primal_problem2} is convex, though recall, the 
dual problem \eqref{eq:dual_problem2} is always convex. In general, we say that
a triplet \smash{$\bar{x}, \bar{u}, \bar{v}$} is a \emph{saddle point} of the
Lagrangian if  
\index{Lagrangian function!saddle point} 
\begin{equation}
\label{eq:lagrangian_saddle_point2}
L(\bar{x}, u, v) \leq L(\bar{x}, \bar{u}, \bar{v}) \leq L(x, \bar{u}, \bar{v}),
\quad \text{for any $x$ and any $u \geq 0, \, v$}.
\end{equation}
In other words, starting at \smash{$\bar{x}, \bar{u}, \bar{v}$}, the condition
says: if we move the primal variable away from \smash{$\bar{x}$} then $L$ can
only increase; if we move the dual variables away from \smash{$\bar{u},
  \bar{v}$}, then $L$ can only decrease.    

The following is a useful characterization of primal and dual solutions via
saddle points.  

\begin{Theorem}
\label{thm:saddle_point_optimality}
For any primal problem \eqref{eq:primal_problem2} and its dual
\eqref{eq:dual_problem2}, and any primal feasible \smash{$\bar{x}$} and dual
feasible \smash{$\bar{u}, \bar{v}$}, the triplet \smash{$\bar{x}, \bar{u},
  \bar{v}$} is a saddle point of the Lagrangian
\eqref{eq:lagrangian_saddle_point2} if and only if \smash{$\bar{x}$} is primal
optimal, \smash{$\bar{u}, \bar{v}$} are dual optimal, and strong duality holds. 
\end{Theorem}

Though the arguments behind this result were already given in Chapter 
\ref{chap:lagrangian_duality}, it will be helpful to consolidate them here.
First, note that the saddle point condition
\eqref{eq:lagrangian_saddle_point2} has a couple of equivalent forms:
\begin{equation}
\label{eq:lagrangian_saddle_point3}
\sup_{u \geq 0, \, v} \, L(\bar{x}, u, v) = L(\bar{x}, \bar{u}, \bar{v}) =
\inf_x \, L(x, \bar{u}, \bar{v}), 
\end{equation}
as well as
\begin{equation}
\label{eq:lagrangian_saddle_point4}
f(\bar{x}) = L(\bar{x}, \bar{u}, \bar{v}) = g(\bar{u}, \bar{v}).
\end{equation}
The right-hand side in the above display is due to the definition of the dual
function $g$, whereas the left-hand side uses the representation of $f$ as a
supremum of the Lagrangian, from Property \parref{par:max_min}. To prove    
Theorem \ref{thm:saddle_point_optimality} we simply observe that 
\eqref{eq:lagrangian_saddle_point4} is equivalent to \smash{$\bar{x}, \bar{u}, 
  \bar{v}$} being primal and dual solutions with zero duality gap. 

\section{Karush-Kuhn-Tucker conditions}
\label{sec:kkt_conditions}

We say that a triplet \smash{$\bar{x}, \bar{u}, \bar{v}$} is a
\emph{Karush-Kuhn-Tucker (KKT) point} associated with the primal and 
dual pair \eqref{eq:primal_problem2} and \eqref{eq:dual_problem2} if it
satisfies the following conditions: 
\index{KKT conditions}
\begin{alignat}{2}
\label{eq:kkt_stationarity}
&0 \in \partial_x L(\bar{x}, \bar{u}, \bar{v}) \quad
&& \text{(stationarity)} \\
\label{eq:kkt_complementary_slackness} 
&\bar{u}_i h_i(\bar{x}) = 0, \; i = 1,\dots,m \quad 
&& \text{(complementary slackness)} \\
\label{eq:kkt_primal_feasibility}
&h_i(\bar{x}) \leq 0, \; i = 1,\dots,m, \; \text{and} \; 
\ell_j(\bar{x}) = 0, \; j = 1,\dots,k \quad 
&& \text{(primal feasibility)} \\ 
\label{eq:kkt_dual_feasibility}
&\bar{u}_i \geq 0, \; i = 1,\dots,m. \quad
&& \text{(dual feasibility)}
\end{alignat}
To be clear, in \eqref{eq:kkt_stationarity}, called the stationarity condition,
the subdifferential is taken with respect to the $x$ component of the
Lagrangian.  

The next result shows that the KKT conditions offer yet another equivalent
characterization of saddle point condition (together with feasibility).  

\begin{Lemma}
\label{lem:saddle_point_kkt}
For any primal feasible \smash{$\bar{x}$} and dual feasible \smash{$\bar{u}, 
  \bar{v}$}, the saddle point condition \eqref{eq:lagrangian_saddle_point2} is
equivalent to stationarity and complementary slackness,
\eqref{eq:kkt_stationarity} and \eqref{eq:kkt_complementary_slackness}.
\end{Lemma}

The proof is straightforward, if we use the equivalent form of the saddle point
condition \eqref{eq:lagrangian_saddle_point4}. The second equality in
\eqref{eq:lagrangian_saddle_point4} is equivalent to the fact that
\smash{$\bar{x}$} minimizes \smash{$L(\cdot, \bar{u}, \bar{v})$}, which is
equivalent to stationarity \eqref{eq:kkt_stationarity}. The first equality in
\eqref{eq:lagrangian_saddle_point4} is implied by complementary slackness
\eqref{eq:kkt_complementary_slackness}:
\[
L(\bar{x}, \bar{u}, \bar{v}) = f(\bar{x}) + 
\sum_{i=1}^m \underbrace{\bar{u}_i h_i(\bar{x})}_{=\,0} + 
\sum_{j=1}^k \underbrace{\bar{v}_j \ell_j(\bar{x})}_{=\,0} 
= f(\bar{x}),
\]
where we have used primal feasibility. Furthermore, the first equality in
\eqref{eq:lagrangian_saddle_point4} also implies complementary slackness: if
\smash{$f(\bar{x}) = L(\bar{x}, \bar{u}, \bar{v})$}, then (again using primal
and dual feasibility) we conclude that all summands must be zero in the middle
expression of the last display, which leads to
\eqref{eq:kkt_complementary_slackness}. This completes the proof the theorem.

Combining Theorem \ref{thm:saddle_point_optimality} and Lemma
\ref{lem:saddle_point_kkt} yields the following important result. 

\index{KKT conditions!optimality}
\begin{Theorem}
\label{thm:kkt_optimality}
For any primal problem \eqref{eq:primal_problem2} and its dual
\eqref{eq:dual_problem2}, a triplet \smash{$\bar{x}, \bar{u}, \bar{v}$}
satisfies the KKT conditions
\eqref{eq:kkt_stationarity}--\eqref{eq:kkt_dual_feasibility} if and only if 
\smash{$\bar{x}$} is primal optimal, \smash{$\bar{u}, \bar{v}$} are dual
optimal, and strong duality holds. In other words, the KKT conditions are always
sufficient for optimality, and necessary under strong duality.   
\end{Theorem}

\index{subgradient!optimality condition}
\index{Lagrange multiplier condition} 
It is worth noting that the KKT conditions generalize the subgradient optimality
condition \eqref{eq:subgradient_optimality}. That is, if problem
\eqref{eq:primal_problem2} has no constraints ($m = k = 0$), then the Lagrangian
is simply the primal criterion, $L = f$, and the only non-vacuous KKT condition
is stationarity \eqref{eq:kkt_stationarity}, which reduces to
\eqref{eq:subgradient_optimality}. Similarly, the KKT conditions generalize the
Lagrange multiplier condition \eqref{eq:lagrange_multiplier}, which can seen
either because the subgradient optimality condition does, or directly from
stationarity \eqref{eq:kkt_stationarity} with linear equality constraints of the
form $Ax = b$. 

\begin{Example}
The following examples discuss the KKT conditions for problems of interest. In
each case, the KKT conditions are necessary and sufficient for optimality, as
the problem in question is convex and strong duality holds by Slater's condition
(Theorem \ref{thm:slater_condition}). 

\begin{enumerate}[label=\alph*., ref=\alph*]
\item Consider a QP with equality constraints only, 
  \begin{alignat*}{2}
  &\minimize_x \quad && c^\T x + \frac{1}{2} x^\T Q x \\
  &\st && Ax = b,
  \end{alignat*}
  where $Q \succeq 0$. Its Lagrangian is 
  \[
  L(x,v) = c^\T x + \frac{1}{2} x^\T Q x + v^\T (Ax - b).
  \]
  Complementary slackness \eqref{eq:kkt_complementary_slackness} and dual
  feasibility \eqref{eq:kkt_dual_feasibility} are vacuous due to the lack of 
  inequality constraints. Stationarity \eqref{eq:kkt_stationarity} and primal
  feasibility \eqref{eq:kkt_primal_feasibility} are each linear equations, which
  can be assembled into one combined linear system:
  \index{Karush-Kuhn-Tucker matrix}
  \[
  \begin{bmatrix} Q & A^\T \\ A & 0 \end{bmatrix}  
  \begin{bmatrix} x \\ v \end{bmatrix} =
  \begin{bmatrix} -c \\ b \end{bmatrix}.
  \]
  The above matrix is often called the \emph{KKT matrix} associated with our
  original QP. 

\item \parlab{xa:lasso_kkt}
  Consider the lasso problem
  \begin{equation}
  \label{eq:lasso_primal2}
  \minimize_\beta \quad \frac{1}{2} \|y - X \beta\|_2^2 + \lambda \|\beta\|_1, 
  \end{equation}
  for a response vector $y \in \R^n$ and feature matrix $X \in \R^{n \times
    d}$. Since this problem has no constraints, the KKT conditions reduce to  
  stationarity (which, as discussed above, is just the subgradient optimality 
  condition):   
  \index{lasso!optimality conditions}
  \begin{equation}
  \label{eq:lasso_optimality}
  X^\T (y - X\beta) = \lambda s, \quad \text{where $s \in \partial
    \|\beta\|_1$}. 
  \end{equation}
  This condition characterizes lasso solutions. We will see later, in Chapter
  \ref{sec:lasso_structure}, that it provides a basis for better understanding
  the structure of lasso solutions.   
  
\item \parlab{xa:svm_kkt}
  Consider the SVM problem
  \begin{equation}
  \label{eq:svm_primal2}
  \begin{alignedat}{2}
  &\minimize_{\beta_0,\beta,\xi} \quad
  && \frac{1}{2} \|\beta\|_2^2 + C \sum_{i=1}^n \xi_i \\ 
  &\st \quad && y_i (\beta_0 + x_i^\T \beta) \geq 1-\xi_i, \;  i=1,\dots,n \\ 
  & && \xi \geq 0,
  \end{alignedat}
  \end{equation}
  for labels $y_i \in \{ -1, 1\}$ and features $x_i \in \R^d$, $i=1,\dots,n$. 
  Its Lagrangian is
  \[
   L(\beta,\beta_0,\xi,u,v) = 
   \frac{1}{2} \|\beta\|_2^2 + C \sum_{i=1}^n \xi_i +  \sum_{i=1}^n u_i 
   \big(1-\xi_i - y_i (\beta_0 + x_i^\T \beta) \big) - v^\T \xi. 
   \]
  The stationarity condition \eqref{eq:kkt_stationarity} becomes
  \index{svm!optimality conditions}
  \begin{equation}
  \label{eq:svm_stationarity}
  \sum_{i=1}^n u_i y_i = 0, \quad 
  \sum_{i=1}^n u_i y_i x_i = \beta, \quad
  u_i + v_i = C, \quad i = 1,\dots,n.
  \end{equation}
  Complementary slackness \eqref{eq:kkt_complementary_slackness} becomes   
  \begin{equation}
  \label{eq:svm_complementary_slackness}
    u_i \big(1-\xi_i - y_i (\beta_0 + x_i^\T \beta) \big) = 0, \quad v_i \xi_i = 
    0, \quad i=1,\dots,n. 
  \end{equation}
  Together with primal and dual feasibility, the above two conditions
  characterize SVM solutions. We will see later, in Chapter
  \ref{sec:svm_structure}, that these conditions provide a basis for better 
  understanding the structure of SVM solutions.  
\end{enumerate}
\end{Example}

\section{Primal-dual relationships}

For a primal-dual pair \eqref{eq:primal_problem2}, \eqref{eq:dual_problem2}
which exhibit strong duality, we know (Theorem \ref{thm:kkt_optimality}) that
the KKT conditions are necessary and sufficient for optimality. The stationarity
condition \eqref{eq:kkt_stationarity} allows us to relate primal $x^\star$ and
dual $u^\star, v^\star$ solutions. Rephrased, this condition says that 
\index{KKT conditions!primal-dual relationship}
\begin{equation}
\label{eq:kkt_stationarity2}
\text{$x^\star$ minimizes $L(\cdot, u^\star, v^\star)$}.
\end{equation}
As demonstrated in the examples that follow, we can often rearrange
\eqref{eq:kkt_stationarity2} to form a \emph{primal-dual relationship}, which is
an equation that relates primal and dual solutions.  Further, if $L(\cdot,
u^\star, v^\star)$ has a unique minimizer, then this relationship uniquely
specifies $x^\star$ as a function of $u^\star, v^\star$.

Primal-dual relationships can be useful for two main reasons. First, in the case
that it uniquely specifies the primal solution $x^\star$, a primal-dual
relationship enables us to solve the primal \eqref{eq:primal_problem2} via the
dual \eqref{eq:dual_problem2}, which is useful in practice when the latter is
easier solve than the former. Second, a primal-dual relationship can provide an
avenue for inferring properties of solutions, such as those described next.   

\begin{Example}
We revisit the lasso and SVM problems from Examples \parref{xa:lasso_kkt} and
\parref{xa:svm_kkt}. 

\begin{enumerate}[label=\alph*., ref=\alph*]
\item \parlab{xa:lasso_primal_dual}
  The lasso problem \eqref{eq:lasso_primal2} has no constraints and thus no 
  associated dual variables. In its original form, the stationarity condition 
  \eqref{eq:lasso_optimality} relates the primal solution to itself, and not to
  dual variables. To make progress toward a primal-dual relationship, we can 
  introduce auxiliary equality constraints in the lasso problem
  \eqref{eq:lasso_primal2}, as in Exercise \ref{ex:lasso_dual}. This allows us
  to write its Lagrangian    
  as     
  \[
  L(\beta,z,u) = \frac{1}{2} \|y - z\|_2^2 + \lambda \|\beta\|_1 + u^\T (z - X
  \beta), 
  \]
  where $z$ is an additional primal variable, subject to the constraint $z =
  X \beta$, as in \eqref{eq:lasso_primal}. Denote by \smash{$\hbeta, \hat{z}$} 
  and \smash{$\hat{u}$} primal and dual solutions, respectively. The
  stationarity condition with respect to $z$ implies    
  \[
  y - \hat{z} = \hat{u}.
  \]
  Since \smash{$\hat{z} = X \hbeta$} at the solution, this gives the primal-dual
  relationship   
  \index{lasso!primal-dual relationship}
  \begin{equation}
  \label{eq:lasso_primal_dual}
  X \hbeta = y - \hat{u}.
  \end{equation}
  Any lasso solution \smash{$\hbeta$} in \eqref{eq:lasso_primal2} (which is not
  necessarily unique) and the dual solution \smash{$\hat{u}$} in
  \eqref{eq:lasso_dual} (which is unique, since the dual criterion is strictly
  concave) must together satisfy \eqref{eq:lasso_primal_dual}. Furthermore, the
  dual \eqref{eq:lasso_dual} is equivalent to \index{lasso!dual problem}  
  \begin{equation}
  \label{eq:lasso_dual2}
  \minimize_u \quad \|y - u\|_2^2 \quad \st \quad \|X^\T u\|_\infty \leq
  \lambda.  
  \end{equation}
  We can hence write the dual solution as \smash{$\hat{u} = P_C(y)$}, the
  projection of $y$ onto the convex set (in fact, polyhedron)
  \[
  C = \{ u : \|X^\T u\|_\infty \leq \lambda \}. 
  \]
  From \eqref{eq:lasso_primal_dual}, we infer that \smash{$X \hbeta =
    (I-P_C)(y)$}, the residual from projection onto $C$. As the residual 
  from projection onto a convex set is always nonexpansive
  \eqref{eq:projection_residual_nonexpansiveness}, writing \smash{$X
    \hbeta$}---which is called the lasso fitted vector (or simply lasso
  fit)---in this form allows us to deduce the following smoothness property. For
  any two response vectors $y, y' \in \R^n$ and their corresponding lasso fitted 
  vectors \smash{$X \hbeta(y), X \hbeta(y') \in \R^n$}, we have       
  \begin{equation}
  \label{eq:lasso_nonexpansive}
  \|X \hbeta(y) - X \hbeta(y')\|_2 \leq \|y - y'\|_2.
  \end{equation}
  That is, the lasso fit \smash{$X \hbeta(y)$} is Lipschitz continuous as a
  function of $y$ (with Lipschitz constant $L = 1$), thus differentiable almost
  everywhere (by Rademacher's theorem).   

\item \parlab{xa:svm_primal_dual}
\end{enumerate}
\end{Example}

examples revisiting lasso and SVM

we recall the form of the dual, credit past exercises, but print again here (in
the example). then we write down PD relationship. then we say what we learn. for
lasso, it's nonexpansiveness. for svm, it's ???

\section{Stationarity and convexity}

Talk about the role of convexity in interpreting the stationarity codition.
Can't split it up for nonconvex functions.

\section{Constraint qualification*}

should we cover constraint qualification? 
somehow try to cover lagrange multipliers as a consquence?

\section{Constrained and penalized forms*}

\begin{xcb}{Exercises}
\begin{enumerate}[label=\thechapter.\arabic*]
\settowidth{\leftmargini}{00.00.\hskip\labelsep}
\item 

\item \label{ex:lasso_dantzig}
Compare these two? And how about basis pursuit?

\item \label{ex:simplex_projection}

\end{enumerate}
\end{xcb}