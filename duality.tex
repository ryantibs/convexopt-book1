\chapter{Duality}
\label{chap:duality}

\section{LP duality*}
\label{sec:lp_duality}

Duality is fascinating topic in mathematical optimization: the basic arguments
used in the theory of duality are elementary and yet they can lead to powerful
and far-reaching conclusions.    

The story begins, for us, with linear programs (LPs). Consider a generic LP of
the form 
\begin{equation}
\label{eq:lp_primal}
\begin{alignedat}{2}
&\minimize_x \quad && c^\T x \\
&\st && Ax \leq b \\
& && Gx = h, 
\end{alignedat}
\end{equation}
where $c \in \R^d$, $A \in \R^{m \times d}$, $b \in \R^m$, $G \in \R^{k \times 
  d}$, $h \in \R^k$. The reason we start the chapter by studying LPs is that,   
in this problem class, we can build up dual problems ``constructively''; this 
constructive approach is not possible for general optimization problems, and
helps us appreciate the importance and elegance of Lagrange duality, which is
covered next.     

The fundamental question that underlies the study of duality is as follows: what
is the tightest lower bound we can form on the optimal criterion value $f^\star
= c^\T x^\star$ in \eqref{eq:lp_primal}? To address this, suppose that $u \in
\R^m$ and $v \in \R^k$ are arbitrary vectors---which we call \emph{dual 
  variables} in this context---with $u$ nonnegative in each component, $u \geq   
0$. Provided that $x \in \R^d$ is a feasible point for problem
\eqref{eq:lp_primal}, it holds that $u^\T (Ax - b) \leq 0$ and $v^\T (Gx - h) =
0$, and so, adding these together gives        
\begin{equation}
\label{eq:lp_dual_nonnegative}
u^\T (Ax - b) + v^\T (Gx - h) \leq 0.
\end{equation}
In order to obtain a lower bound on the criterion value $c^\T x$, we rearrange
the above into
\[
(-A^\T u - G^\T v)^\T x \geq -b^\T u - h^\T v.
\]
The key observation is that this provides the lower bound we desire provided
that the dual variables $u,v$ are chosen such that $-A^\T u - G^\T v= c$. This
is true for any feasible $x$, thus taking an infimum over all such $x$ gives   
\[
f^\star \geq -b^\T u - h^\T v, \quad \text{for any $u,v$ such that $-A^\T u -
  G^\T v = c$ and $u \geq 0$}.
\]
Finally, to make this lower bound as tight as possible, we maximize the
right-hand side above,  
\begin{equation}
\label{eq:lp_weak_duality}
f^\star \geq \, \underbrace{\sup \big\{ -b^\T u - h^\T v :  A^\T u + G^\T v =
  -c, \, u \geq 0 \big\}}_{g^\star}.
\end{equation}
Now the right-hand side in \eqref{eq:lp_weak_duality}, which we may denote as 
$g^\star = -b^\T u^\star - h^\T v^\star$, is itself the optimal criterion value
associated with an optimization problem, indeed an LP,     
\index{dual problem!linear program}
\begin{equation}
\label{eq:lp_dual}
\begin{alignedat}{2}
&\maximize_{u,v} \quad && b^\T u - h^\T v \\
&\st && A^\T u + G^\T v = -c \\
& && u \geq 0.
\end{alignedat}
\end{equation}
In this context, we call \eqref{eq:lp_dual} the \emph{dual LP} and the original 
problem \eqref{eq:lp_primal} the \emph{primal LP}. Note that, by construction 
\eqref{eq:lp_weak_duality}, we have $f^\star \geq g^\star$: the optimal
criterion in the dual problem is a lower bound on the optimal criterion in the  
primal problem.  

There are several natural follow-up questions that we can ask: for example, when
does equality hold in \eqref{eq:lp_weak_duality}, $f^\star = g^\star$? And, how 
do solutions $x^\star$ and $u^\star, v^\star$ in \eqref{eq:lp_primal} and
\eqref{eq:lp_dual}, respectively, relate? We will address these questions and
more, over this chapter and the next one. First, however, we must develop a
theory of duality beyond LPs. 

\begin{Remark}
In the attempt to move beyond LPs with the constructive approach to duality, we 
run into a shortcoming: when the criterion $f$ is nonlinear, but the constraint
functions are linear, we have no way in general to combine the
constraints---which are linear equalities and inequalities in $x$---to construct
a lower bound on $f(x)$. But there is another way: Lagrangian duality, as we
will see next, applies seamlessly to LPs and convex optimization more broadly
(and even some nonconvex problems).   
\end{Remark}

\section{Lagrangian duality}
\label{sec:lagrangian_duality}

Lagrangian duality (or Lagrange duality) starts with the same motivation as in
the last section, but cast in a more general setting: we seek a lower bound on
the optimal criterion value $f^\star$ in           
\begin{equation}
\label{eq:primal_problem}
\begin{alignedat}{2}
&\minimize_x \quad && f(x) \\
&\st \quad && h_i(x) \leq 0, \; i=1,\ldots,m \\ 
& && \ell_j(x) = 0, \; j=1,\ldots,k.
\end{alignedat}
\end{equation}
At the moment, we do not assume that criterion $f$ or the constraint functions
$h_i$, $i=1,\dots,m$, and $\ell_j$, $j=1,\dots,k$ are convex. Thus, to be clear,
we do not assume that \eqref{eq:primal_problem} is a convex problem. 

As before, let $u \in \R^m$ and $v \in \R^k$ be arbitrary vectors, with $u \geq
0$, which we call \emph{dual variables} in the current context. Using the same
basic idea as in \eqref{eq:lp_dual_nonnegative}, note that for any feasible $x$
for \eqref{eq:primal_problem}, 
\[
\sum_{i=1}^m u_i h_i(x) + \sum_{j=1}^r v_j \ell_j(x) \leq 0.
\]
We now add this quantity to the criterion $f(x)$ to define what we call the
\emph{Lagrangian function},   
\index{Lagrangian function}
\[
L(x,u,v) = f(x) + \sum_{i=1}^m u_i h_i(x) + \sum_{j=1}^r v_j \ell_j(x).
\] 
By construction, this provides a lower bound on the criterion,
\begin{equation}
\label{eq:lagrangian_bound1}
f(x) \geq L(x,u,v), \quad \text{for any feasible $x$ and $u,v$ such that $u
  \geq 0$},
\end{equation}
and minimizing over the feasible set, denoted $C \subseteq \R^d$, yields
\begin{equation}
\label{eq:lagrangian_bound2}
f^\star \geq \inf_{x \in C} \, L(x,u,v) \geq \inf_x \, L(x,u,v), \quad \text{for
  any $u,v$ such that $u \geq 0$}.  
\end{equation}

\begin{Remark}
The second inequality in \eqref{eq:lagrangian_bound2} is really the key idea
behind Lagrangian duality; had we stopped at the first inequality in 
\eqref{eq:lagrangian_bound2}, we would have not have (yet) gotten anywhere
practically interesting, since minimizing the Lagrangian over $x \in C$ is  
hard in general---it is no easier than minimizing $f$ over $x \in C$, which is
our original primal problem \eqref{eq:primal_problem}. Meanwhile, minimizing the
Lagrangian over $x \in \R^d$ is usually more tractable (as we will see in 
examples that follow), and still produces an effective lower bound.
\end{Remark}

It is convenient to denote the right-most quantity in
\eqref{eq:lagrangian_bound2} by 
\index{dual function}
\begin{equation}
\label{eq:dual_function}
g(u,v) = \inf_x \, L(x,u,v), 
\end{equation}
which we call the \emph{(Lagrange) dual function}. Then
\eqref{eq:lagrangian_bound2} becomes $f^\star \geq g(u,v)$ for any $u,v$ with $u
\geq 0$, and we can maximize $g$ to obtain the tightest lower bound, yielding  
\index{dual problem}
\begin{equation}
\label{eq:dual_problem}
\maximize_{u,v} \quad g(u,v) \quad \st \quad u \geq 0,
\end{equation}
which we call the \emph{(Lagrange) dual problem} associated with problem
\eqref{eq:primal_problem}. Figure \ref{fig:nonconvex_quartic}, given later and
discussed under Example \ref{xa:nonconvex_quartic}, illustrates the construction
of the Lagrangian and the dual problem.

\begin{Example}
In what follows, we work through the duals of canonical problems: LPs and
QPs. For the duals standard form LPs and QPs, see Exercises \ref{ex:lp_std_dual}
and \ref{ex:qp_std_dual}.   

\begin{enumerate}[label=\alph*., ref=\alph*]
\item \parlab{xa:lp_dual} 
For the linear program \eqref{eq:lp_primal}, the Lagrangian is
\begin{align*}
L(x,u,v) &= c^\T x + u^\T (Ax - b) + v^\T (Gx - h) \\
&= (c + A^\T u + G^\T v)^\T x - b^\T u - h^\T v.
\end{align*}
The dual function is obtained by taking an infimum over all $x$,
\[
g(u,v) = \begin{cases}
- b^\T u - h^\T v & A^\T u + G^\T v = -c \\
- \infty & \text{otherwise}.
\end{cases}
\]
The dual problem, which maximizes $g(u,v)$ subject to the constraint $u \geq
0$, is therefore precisely as derived earlier in \eqref{eq:lp_dual}. 

\item \parlab{xa:qp_dual}
For the quadratic program (QP): 
\begin{equation}
\label{eq:qp_primal}
\begin{alignedat}{2}
&\minimize_x \quad && \frac{1}{2} x^\T Q x + c^\T x\\ 
&\st && Ax \leq b \\
& && Gx = h,
\end{alignedat}
\end{equation}
the Lagrangian is
\begin{align*}
L(x,u,v) &= \frac{1}{2} x^\T Q x + c^\T x + u^\T (Ax - b) + v^\T (Gx - h) \\ 
&= \frac{1}{2} x^\T Q x + (c + A^\T u + G^\T v)^\T x - b^\T u - h^\T v. 
\end{align*}
Assume that $Q \succ 0$. We can minimize the Lagrangian over $x$ by setting its
gradient to zero and solving, which yields $x = - Q^{-1} (c + A^\T u + G^\T v)$,
and   
\[
g(u,v) = - \frac{1}{2} (c + A^\T u + G^\T v)^\T Q^{-1} (c + A^\T u + G^\T v) -
b^\T u - h^\T v.
\] 
The dual problem is thus
\index{dual problem!quadratic program}
\begin{equation}
\label{eq:qp_pd_dual}
\begin{alignedat}{2}
&\maximize_{u,v} \quad && - \frac{1}{2} (c + A^\T u + G^\T v)^\T Q^{-1} (c +
A^\T u + G^\T v) - b^\T u - h^\T v \\
&\st && u \geq 0.
\end{alignedat}
\end{equation}
This is itself a QP. If instead $Q \succeq 0$, then similar arguments (Exercise
\ref{ex:qp_psd_dual}) lead to  
\begin{equation}
\label{eq:qp_psd_dual}
\begin{alignedat}{2}
&\maximize_{u,v} \quad && - \frac{1}{2} (c + A^\T u + G^\T v)^\T Q^\pinv (c + 
A^\T u + G^\T v) - b^\T u - h^\T v \\
&\st && c + A^\T u + G^\T v \in \col(Q) \\ 
& && u \geq 0.
\end{alignedat}
\end{equation}
This is still a QP, because $c + A^\T u + G^\T v \in \col(Q)$ (where $\col(Q)$
denotes the column space of $Q$) is a linear constraint.    
\end{enumerate}
\end{Example}

\section{Properties}

Next we cover two important properties of Lagrange duality, which follow
more or less immediately from the development in the last section.

\paragraph{Weak duality.}
\parlab{par:weak_duality}

Let $g^\star$ denote the optimal value in the dual problem
\eqref{eq:dual_problem}. It follows from \eqref{eq:lagrangian_bound2} and the   
definition of the dual function in \eqref{eq:dual_function} that  
\index{weak duality}
\begin{equation}
\label{eq:weak_duality}
f^\star \geq g^\star.
\end{equation}
This property is called \emph{weak duality}, and it holds for \emph{any}
original (primal) problem \eqref{eq:primal_problem}, regardless of
convexity. Note that this generalizes what we found in
\eqref{eq:lp_weak_duality} for LPs. 

\paragraph{Convexity of dual problem.}
\parlab{par:convex_dual}

The dual optimization problem \eqref{eq:dual_problem} is \emph{always convex},
that is, it is a concave maximization problem. This is true for any original
(primal) problem \eqref{eq:primal_problem}, regardless of whether the primal
problem is itself convex. To see this, we can rewrite the dual function
\eqref{eq:dual_function} as   
\[
g(u,v) = - \, \underbrace{\sup_x \, \bigg\{ -f(x) - \sum_{i=1}^m u_i h_i(x) - 
  \sum_{j=1}^r v_j \ell_j(x) \bigg\}}_{\bar{g}(u,v)}.
\]
Observe that the function defined as \smash{$\bar{g}$} above is the pointwise 
supremum of affine---hence convex---functions in $u,v$. By the pointwise
supremum rule in Property \parref{par:function_supremum}, we see that
\smash{$\bar{g}$} is convex, and therefore the dual function \smash{$g =
  -\bar{g}$} is concave. This makes problem \eqref{eq:dual_problem} convex.     

\medskip

\begin{Example}
\label{xa:nonconvex_quartic}
The following example emphasizes how Property \parref{par:convex_dual} can be
surprising and nonobvious in practice. Consider the nonconvex optimization
problem 
\begin{equation}
\label{eq:nonconvex_quartic}
\minimize_x \quad \frac{1}{10} x^4 - 5x^2 + 10x + 125 \quad \st \quad x \geq -4.
\end{equation}
The left panel of Figure \ref{fig:nonconvex_quartic} plots its criterion, which
is a nonconvex quartic function. Despite such nonconvexity, we can calculate the
dual function for this problem explicitly because it reduces to solving for the
roots of a cubic equation, which can be done in closed-form. Some calculations
(Exercise \ref{ex:nonconvex_quartic}) lead to:
\begin{equation}
\label{eq:nonconvex_quartic_dual}
g(u) = \min_{j=1,2,3} \, \bigg\{ \frac{1}{10} R_j^4(u) - 5 R_j^2(u) + 10 R_j(u)
+ 125 - u R_j(u) - 4u \bigg\},
\end{equation}
where 
\[
R_j(u) = a_j \sqrt[3]{\sqrt{\frac{(u-25)^2}{4} - \frac{25^3}{27}} +
  \frac{u-25}{2}} + b_j \sqrt[3]{-\sqrt{\frac{(u-25)^2}{4} - \frac{25^3}{27}} +
  \frac{u-25}{2}}, \quad j = 1,2,3,   
\]
where here \smash{$\sqrt{\cdot}$} and \smash{$\sqrt[3]{\cdot}$} denote principal
values of the root function (the root with the largest real part), and the
coefficients are    
\[
(a_j, b_j) = \begin{cases}
(1, 1) & \text{if $j=1$} \\
(\epsilon_1, \epsilon_2) & \text{if $j=2$}  \\
(\epsilon_2, \epsilon_1) & \text{if $j=3$} ,
\end{cases}
\]
where \smash{$\epsilon_1 = (-1 + i \sqrt{3})/2$}, \smash{$\epsilon_2 = (-1 -  
  i \sqrt{3})/2$}, and we use $i$ for the imaginary unit. The right panel of
Figure \ref{fig:nonconvex_quartic} plots the dual criterion---that this function  
is concave, as guaranteed by Property \parref{par:convex_dual}, is not at all 
obvious from its analytic form!    
\end{Example}

\begin{figure}[tb]
\centering
\includegraphics[width=0.89\textwidth]{fig/nonconvex_quartic.pdf}
\caption{Left: the nonconvex quartic criterion in problem
  \eqref{eq:nonconvex_quartic}, drawn as a solid curve. The dashed lines show
  $uh(x)$ as a function of $x$ for different values of the dual variable $u$,
  where $h(x) = -x-4 \leq 0$ is the inequality constraint in
  \eqref{eq:nonconvex_quartic}. The thin solid curves show the Lagrangian
  $L(x,u)$ as a function of $x$, for the same values of $u$. Right: the
  associated dual function, as in \eqref{eq:nonconvex_quartic_dual}, which at
  each $u$ is given by minimizing $L(x,u)$ over all $x$. The dashed horizontal
  line marks the optimal primal value $f^\star$. We see that weak duality
  $f^\star \geq g^\star$ holds, and the inequality is strict.}  
\label{fig:nonconvex_quartic}
\end{figure}

\section{Interpretations}
\label{sec:duality_interpretations}

We discuss some interpretations of Lagrangian duality. The chapter notes 
provide further discussion and context. In this section, without loss of
generality, we will simply denote the dual variable by $u$, that is, we drop 
reference to the part $v$ corresponding to the equality constraints in the
primal \eqref{eq:primal_problem}. This is possible because equality constraints
can always be rewritten as inequality constraints (each $\ell_j(x) = 0$ can be 
rewritten as $\ell_j(x) \leq 0$ and $-\ell_j(x) \leq 0$). 

\paragraph{Max-min interpretation.}

Recall that, by definition \eqref{eq:dual_function}, for each $u \geq 0$,    
\[
\inf_x \, L(x,u) = g(u).
\]
Meanwhile, we also have the following symmetrical fact: for any $x$, 
\[
\sup_{u \geq 0} \, L(x,u) = f(x) + I_C(x),
\]
where $C = \{x : h_i(x) \leq 0, \, i=1,\dots,m\}$ is the primal feasible
set. To see this, note that we have the upper bound $L(x,u) \leq f(x) +
I_C(x)$ for all $x$ and $u \geq 0$. For $x \in C$, the upper bound is attained   
at $u = 0$, whereas for $x \notin C$, the upper bound is attained by sending 
$u_i \to \infty$, where $i$ is such that $h_i(x) > 0$. Therefore by the general
max-min inequality (see Exercise \ref{ex:inf_sup_rules} part d): 
\[
\underbrace{\inf_x \, \sup_{u \geq 0} \, L(x,u)}_{f^\star} \geq 
\underbrace{\sup_{u \geq 0} \, \inf_x \, L(x,u)}_{g^\star}.
\]
This is in fact precisely weak duality \eqref{eq:weak_duality}, since the 
left-hand side is \smash{$f^\star = \inf_x \{ f(x) + I_C(x) \}$} and the
right-hand side is \smash{$g^\star = \sup_{u \geq 0} \, g(u)$}. 

\paragraph{Saddle point interpretation.}

From \eqref{eq:lagrangian_bound1} and \eqref{eq:dual_function}, it follows that  
\[
f(x) \geq L(x,u) \geq g(u), \quad \text{for any primal feasible $x$ and dual
  feasible $u$}, 
\]
where by dual feasible we mean that $u \geq 0$. Suppose that $f^\star =
g^\star$, and $x^\star$ is a solution in the primal problem
\eqref{eq:primal_problem} and $u^\star$ is a solution in the dual problem
\eqref{eq:dual_problem}. Then the inequalities in the last display applied to
$x^\star,u^\star$ must be all equalities:  
\begin{equation}
\label{eq:lagrangian_equalities1}
f(x^\star) = L(x^\star, u^\star) = g(u^\star),
\end{equation}
or equivalently, 
\begin{equation}
\label{eq:lagrangian_equalities2}
\sup_{u \geq 0} \, L(x^\star, u) = L(x^\star, u^\star) = \inf_x \, L(x,
u^\star), 
\end{equation}
where used the definition of $g$ as an infimum, and the supremal representation
of $f$ established in the max-min interpretation. Observe that we can rewrite
\eqref{eq:lagrangian_equalities2} as
\index{Lagrangian function!saddle point}
\begin{equation}
\label{eq:lagrangian_saddle_point}
L(x^\star, u) \leq L(x^\star, u^\star) \leq L(x, u^\star), \quad \text{for any
  $x$ and $u \geq 0$},
\end{equation}
which says that $x^\star,u^\star$ is what is known as a \emph{saddle point} of 
the Lagrangian. 

\paragraph{Shadow price interpretation.}

Let us imagine that the primal variable $x$ in \eqref{eq:primal_problem}
represents a planning variable for a given business, and $f(x)$ is the cost 
incurred for operating according to $x$, that is, $-f(x)$ is the profit
earned. Each $h_i(x) \leq 0$ represents an operating constraint, for example, a
space constraint in the current warehouse used by the firm. (Recall, we are
dropping all equality constraints from \eqref{eq:primal_problem}, as we are  
considering the dual variable to be $u \geq 0$.)  Then in this language,
$f^\star$ is the minimal cost (maximum profit) when the firm operates according
to an optimal plan $x^\star$. 

The Lagrangian $L(x,u)$ in this context has the following interpretation. The
business is allowed to break the operating constraints, provided they pay
appropriately for violations. Each component $u_i \geq 0$ of the dual variable
corresponds to a price per unit violation of the constraint $h_i(x) \leq 0$,
that is, $u_i h_i(x)$ represents the cost to the business for violating the
$i\th$ constraint; likewise, $-u_i h_i(x)$ is the profit to the firm for slack
in the $i\th$ constraint. For example, if the firm needs more space, then they
can rent new warehouse space at a price per unit of $u_i$; and if they have
leftover space in their current warehouse, then they can rent it out at the same 
price. The value of the Lagrangian $L(x,u)$ thus represents the total cost
incurred when operating according to $x$, with prices according to $u$. The dual  
variable $u$ is often called the vector of \emph{shadow prices} in this
context. 
\index{Lagrangian function!shadow prices}

Weak duality \eqref{eq:weak_duality} is now the statement that the optimal cost
$g^\star$ when the firm is allowed to violate constraints and pay accordingly is
at most the optimal cost $f^\star$ when they must respect the constraints, even
at the worst-case prices. Furthermore, if indeed $f^\star = g^\star$, and
$u^\star$ is a dual solution, then $u^\star$ represents a set of prices for
which there is no advantage to violating the constraints versus respecting
them. 

\section{Duality gap}

In different interpretations in the last section, we discussed the case when
primal and dual optimal values match,   
\index{strong duality}
\begin{equation}
\label{eq:strong_duality}
f^\star = g^\star,
\end{equation}
which is a condition we call \emph{strong duality}. To be clear, unlike weak
duality \eqref{eq:weak_duality}, strong duality is \emph{not} guaranteed to hold
in general, for any given primal problem \eqref{eq:primal_problem} and its dual  
\eqref{eq:dual_problem}. (Recall, the example in Figure
\ref{fig:nonconvex_quartic} demonstrated a failure of strong duality for problem
\eqref{eq:nonconvex_quartic}.) However, it does hold for ``most'' convex
optimization problems, as the next section makes precise. 

A related concept, for primal feasible $x$ and dual feasible $u,v$, is the
quantity  
\index{duality gap}
\begin{equation}
\label{eq:duality_gap}
f(x) - g(u,v) \geq 0,
\end{equation}
which is called the \emph{duality gap} at $x,u,v$. Note that strong duality the
case in which the duality gap is zero at a primal solution $x^\star$ and dual
solution $u^\star, v^\star$ (assuming solutions exist, that is, assuming the
optimal values are achieved).  

Though it is a simple concept, the duality gap is a powerful tool, as the next
result shows.

\begin{Theorem}
\label{thm:duality_gap}
For any primal problem \eqref{eq:primal_problem} and its dual
\eqref{eq:dual_problem}, the following holds. 

\begin{enumerate}[label=(\roman*)]
\item If the duality gap at $x,u,v$ equals $\epsilon \geq 0$, then $f(x) -
  f^\star \leq \epsilon$, and $g^\star - g(u,v) \leq \epsilon$.  

\item If the duality gap at $x,u,v$ is zero, then $x$ is primal optimal and
  $u,v$ are dual optimal. 
\end{enumerate}
\end{Theorem}

The proof of the theorem is straightforward. If $f(x) - g(u,v) \leq \epsilon$, 
then by virtue of the fact that $f(x) \geq f^\star \geq g^\star \geq g(u,v)$, we
conclude that $f(x) - f^\star \leq \epsilon$ and $g^\star - g(u,v) \leq
\epsilon$, and this proves part (i). Meanwhile, part (ii) simply elucidates the  
special case with $\epsilon = 0$.

This theorem has numerous important implications, both in practice and in 
theory. For example, iterative algorithms which operate on the primal and the
dual simultaneously will be able to use the duality gap as a stopping criterion, 
and in doing so, will stop with a suboptimality guarantee on the primal and the 
dual criteria, by Theorem \ref{thm:duality_gap} part (i).  

As another example, Theorem \ref{thm:duality_gap} part (ii) can be used to
derive a converse of the saddle point property of the last section. There we
proved that if strong duality holds, and $x^\star,u^\star$ are primal and 
solutions, respectively (assuming no equality constraints, without loss of
generality), then $x^\star, u^\star$ is a saddle point of the Lagrangian, as in
\eqref{eq:lagrangian_saddle_point}. Conversely, if the saddle point property 
\eqref{eq:lagrangian_saddle_point} holds at a pair \smash{$\bar{x}, \bar{u}$},
then tracing back to \eqref{eq:lagrangian_equalities2} and
\eqref{eq:lagrangian_equalities1} shows that \smash{$\bar{x}, \bar{u}$} achieve
zero duality gap, which implies they must be optimal. We will revisit the saddle
point perspective in Chapter \ref{chap:kkt}.   
 
\section{Slater's condition}
\label{sec:slater_condition}

hi

\section{SDP duality*}
\label{sec:sdp_duality}

% for SDP duality: take a look at 
% https://www.mit.edu/~parrilo/cdc03_workshop/ejc03_comp.pdf
% https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-251j-introduction-to-mathematical-programming-fall-2009/readings/MIT6_251JF09_SDP.pdf

\SkipTocEntry\section*{Chapter notes}

describe history of duality? saddle point closely connected to zero-sum games. 

credit BV for interpretations

\clearpage

\begin{xcb}{Exercises}
\begin{enumerate}[label=\thechapter.\arabic*]
\settowidth{\leftmargini}{0.00.\hskip\labelsep}

\item \label{ex:lp_std_dual}
Standard form LP dual

\item \label{ex:qp_std_dual} 
Standard form QP dual

\item \label{ex:qp_psd_dual} 
PSD QP dual

\item \label{ex:nonconvex_quartic}
Nonconvex quartic
%https://en.wikipedia.org/wiki/Cubic_equation

\item 

\end{enumerate}
\end{xcb}
