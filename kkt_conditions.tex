\chapter{Karush-Kuhn-Tucker conditions}
\label{chap:kkt_conditions}

\section{Saddle point condition}
\label{sec:saddle_point_condition}

In this chapter, we will explore conditions which characterize solutions in
constrained optimization problems, the most well-known being a set of conditions
known as the Karush-Kuhn-Tucker (KKT) conditions. This flows naturally from our 
study of Lagrange duality in the last chapter. In fact, the last chapter already
established a critical relationship between primal and dual solutions and what
are known as saddle points of the Lagrangian. These arguments were given in 
passing, and here we revisit them, as they will lay the foundation for the KKT 
conditions, which we cover next.           

As in the last chapter, consider a primal problem
\begin{equation}
\label{eq:primal_problem2}
\begin{alignedat}{2}
&\minimize_x \quad && f(x) \\
&\st \quad && h_i(x) \leq 0, \; i=1,\dots,m \\ 
& && \ell_j(x) = 0, \; j=1,\dots,k,
\end{alignedat}
\end{equation}
whose associated Lagrangian and dual function are
\begin{align*}
L(x,u,v) &= f(x) + \sum_{i=1}^m u_i h_i(x) + \sum_{j=1}^k v_j \ell_j(x), \\ 
g(u,v) &= \inf_x \, L(x,u,v), 
\end{align*}
and whose associated dual problem is 
\begin{equation}
\label{eq:dual_problem2}
\maximize_{u,v} \quad g(u,v) \quad \st \quad u \geq 0.
\end{equation}
We do not assume that \eqref{eq:primal_problem2} is convex, though recall, the 
dual problem \eqref{eq:dual_problem2} is always convex. In general, we say that
a triplet \smash{$\bar{x}, \bar{u}, \bar{v}$} is a \emph{saddle point} of the
Lagrangian if  
\index{Lagrangian function!saddle point} 
\begin{equation}
\label{eq:lagrangian_saddle_point2}
L(\bar{x}, u, v) \leq L(\bar{x}, \bar{u}, \bar{v}) \leq L(x, \bar{u}, \bar{v}),
\quad \text{for any $x$ and any $u \geq 0, \, v$}.
\end{equation}
In other words, starting at \smash{$\bar{x}, \bar{u}, \bar{v}$}, the condition
says: if we move the primal variable away from \smash{$\bar{x}$} then $L$ can
only increase; if we move the dual variables away from \smash{$\bar{u},
  \bar{v}$}, then $L$ can only decrease.    

The following is a useful characterization of primal and dual solutions via
saddle points.  

\begin{Theorem}
\label{thm:saddle_point_optimality}
For any primal problem \eqref{eq:primal_problem2} and its dual
\eqref{eq:dual_problem2}, and any primal feasible \smash{$\bar{x}$} and dual
feasible \smash{$\bar{u}, \bar{v}$}, the triplet \smash{$\bar{x}, \bar{u},
  \bar{v}$} is a saddle point of the Lagrangian
\eqref{eq:lagrangian_saddle_point2} if and only if \smash{$\bar{x}$} is primal
optimal, \smash{$\bar{u}, \bar{v}$} are dual optimal, and strong duality holds. 
\end{Theorem}

Though the arguments behind this result were already given in Chapter 
\ref{chap:lagrangian_duality}, it will be helpful to consolidate them here.
First, note that the saddle point condition
\eqref{eq:lagrangian_saddle_point2} has a couple of equivalent forms:
\begin{equation}
\label{eq:lagrangian_saddle_point3}
\sup_{u \geq 0, \, v} \, L(\bar{x}, u, v) = L(\bar{x}, \bar{u}, \bar{v}) =
\inf_x \, L(x, \bar{u}, \bar{v}), 
\end{equation}
as well as
\begin{equation}
\label{eq:lagrangian_saddle_point4}
f(\bar{x}) = L(\bar{x}, \bar{u}, \bar{v}) = g(\bar{u}, \bar{v}).
\end{equation}
The right-hand side in the above display is due to the definition of the dual
function $g$, whereas the left-hand side uses the representation of $f$ as a
supremum of the Lagrangian, from Property \parref{par:max_min}. To prove    
Theorem \ref{thm:saddle_point_optimality} we simply observe that 
\eqref{eq:lagrangian_saddle_point4} is equivalent to \smash{$\bar{x}, \bar{u}, 
  \bar{v}$} being primal and dual solutions with zero duality gap. 

\section{Karush-Kuhn-Tucker conditions}
\label{sec:kkt_conditions}

We say that a triplet \smash{$\bar{x}, \bar{u}, \bar{v}$} is a
\emph{Karush-Kuhn-Tucker (KKT) point} associated with the primal and 
dual pair \eqref{eq:primal_problem2} and \eqref{eq:dual_problem2} if it
satisfies the following conditions: 
\index{KKT conditions}
\begin{alignat}{2}
\label{eq:kkt_stationarity}
&0 \in \partial_x L(\bar{x}, \bar{u}, \bar{v}) \quad
&& \text{(stationarity)} \\
\label{eq:kkt_complementary_slackness} 
&\bar{u}_i h_i(\bar{x}) = 0, \; i = 1,\dots,m \quad 
&& \text{(complementary slackness)} \\
\label{eq:kkt_primal_feasibility}
&h_i(\bar{x}) \leq 0, \; i = 1,\dots,m, \; \text{and} \; 
\ell_j(\bar{x}) = 0, \; j = 1,\dots,k \quad 
&& \text{(primal feasibility)} \\ 
\label{eq:kkt_dual_feasibility}
&\bar{u}_i \geq 0, \; i = 1,\dots,m. \quad
&& \text{(dual feasibility)}
\end{alignat}
To be clear, in \eqref{eq:kkt_stationarity}, called the stationarity condition,
the subdifferential is taken with respect to the $x$ component of the
Lagrangian.  

The next result shows that the KKT conditions offer yet another equivalent
characterization of saddle point condition (together with feasibility).  

\begin{Lemma}
\label{lem:saddle_point_kkt}
For any primal feasible \smash{$\bar{x}$} and dual feasible \smash{$\bar{u}, 
  \bar{v}$}, the saddle point condition \eqref{eq:lagrangian_saddle_point2} is
equivalent to stationarity and complementary slackness,
\eqref{eq:kkt_stationarity} and \eqref{eq:kkt_complementary_slackness}.
\end{Lemma}

The proof is straightforward, if we use the equivalent form of the saddle point
condition \eqref{eq:lagrangian_saddle_point4}. The second equality in
\eqref{eq:lagrangian_saddle_point4} is equivalent to the fact that
\smash{$\bar{x}$} minimizes \smash{$L(\cdot, \bar{u}, \bar{v})$}, which is
equivalent to stationarity \eqref{eq:kkt_stationarity}. The first equality in
\eqref{eq:lagrangian_saddle_point4} is implied by complementary slackness
\eqref{eq:kkt_complementary_slackness}:
\[
L(\bar{x}, \bar{u}, \bar{v}) = f(\bar{x}) + 
\sum_{i=1}^m \underbrace{\bar{u}_i h_i(\bar{x})}_{=\,0} + 
\sum_{j=1}^k \underbrace{\bar{v}_j \ell_j(\bar{x})}_{=\,0} 
= f(\bar{x}),
\]
where we have used primal feasibility. Furthermore, the first equality in
\eqref{eq:lagrangian_saddle_point4} also implies complementary slackness: if
\smash{$f(\bar{x}) = L(\bar{x}, \bar{u}, \bar{v})$}, then (again using primal
and dual feasibility) we conclude that all summands must be zero in the middle
expression of the last display, which leads to
\eqref{eq:kkt_complementary_slackness}. This completes the proof the theorem.

Combining Theorem \ref{thm:saddle_point_optimality} and Lemma
\ref{lem:saddle_point_kkt} yields the following important result. 

\index{KKT conditions!optimality}
\begin{Theorem}
\label{thm:kkt_optimality}
For any primal problem \eqref{eq:primal_problem2} and its dual
\eqref{eq:dual_problem2}, a triplet \smash{$\bar{x}, \bar{u}, \bar{v}$}
satisfies the KKT conditions
\eqref{eq:kkt_stationarity}--\eqref{eq:kkt_dual_feasibility} if and only if 
\smash{$\bar{x}$} is primal optimal, \smash{$\bar{u}, \bar{v}$} are dual
optimal, and strong duality holds. In other words, the KKT conditions are always
sufficient for optimality, and necessary under strong duality.   
\end{Theorem}

\index{subgradient!optimality condition}
\index{Lagrange multiplier condition} 
It is worth noting that the KKT conditions generalize the subgradient optimality
condition \eqref{eq:subgradient_optimality}. That is, if problem
\eqref{eq:primal_problem2} has no constraints ($m = k = 0$), then the Lagrangian
is simply the primal criterion, $L = f$, and the only non-vacuous KKT condition
is stationarity \eqref{eq:kkt_stationarity}, which reduces to
\eqref{eq:subgradient_optimality}. Similarly, the KKT conditions generalize the
Lagrange multiplier condition \eqref{eq:lagrange_multiplier}, which can seen
either because the subgradient optimality condition does, or directly from
stationarity \eqref{eq:kkt_stationarity} with linear equality constraints of the
form $Ax = b$. 

\begin{Example}
The following examples discuss the KKT conditions for problems of interest. In
each case, the KKT conditions are necessary and sufficient for optimality, as
the problem in question is convex and strong duality holds by Slater's condition
(Theorem \ref{thm:slater_condition}). 

\begin{enumerate}[label=\alph*., ref=\alph*]
\item Consider a QP with equality constraints only, 
  \begin{alignat*}{2}
  &\minimize_x \quad && c^\T x + \frac{1}{2} x^\T Q x \\
  &\st && Ax = b,
  \end{alignat*}
  where $Q \succeq 0$. Its Lagrangian is 
  \[
  L(x,v) = c^\T x + \frac{1}{2} x^\T Q x + v^\T (Ax - b).
  \]
  Complementary slackness \eqref{eq:kkt_complementary_slackness} and dual
  feasibility \eqref{eq:kkt_dual_feasibility} are vacuous due to the lack of 
  inequality constraints. Stationarity \eqref{eq:kkt_stationarity} and primal
  feasibility \eqref{eq:kkt_primal_feasibility} are each linear equations, which
  can be assembled into one combined linear system:
  \index{Karush-Kuhn-Tucker matrix}
  \[
  \begin{bmatrix} Q & A^\T \\ A & 0 \end{bmatrix}  
  \begin{bmatrix} x \\ v \end{bmatrix} =
  \begin{bmatrix} -c \\ b \end{bmatrix}.
  \]
  The above matrix is often called the \emph{KKT matrix} associated with our
  original QP. 

\item \parlab{xa:lasso_kkt}
  Consider the lasso problem
  \begin{equation}
  \label{eq:lasso_primal2}
  \minimize_\beta \quad \frac{1}{2} \|y - X \beta\|_2^2 + \lambda \|\beta\|_1, 
  \end{equation}
  for a response vector $y \in \R^n$ and feature matrix $X \in \R^{n \times
    d}$. Since this problem has no constraints, the KKT conditions reduce to  
  stationarity (which, as discussed above, is just the subgradient optimality 
  condition):   
  \index{lasso!optimality conditions}
  \begin{equation}
  \label{eq:lasso_optimality}
  X^\T (y - X\beta) = \lambda s, \quad \text{where $s \in \partial
    \|\beta\|_1$}. 
  \end{equation}
  This condition characterizes lasso solutions. We will see later, in Chapter
  \ref{sec:lasso_structure}, that it provides a way to understand the structure
  of lasso solutions.    
  
\item \parlab{xa:svm_kkt}
  Consider the SVM problem
  \begin{equation}
  \label{eq:svm_primal2}
  \begin{alignedat}{2}
  &\minimize_{\beta_0,\beta,\xi} \quad
  && \frac{1}{2} \|\beta\|_2^2 + C \sum_{i=1}^n \xi_i \\ 
  &\st \quad && y_i (\beta_0 + x_i^\T \beta) \geq 1-\xi_i, \;  i=1,\dots,n \\ 
  & && \xi \geq 0,
  \end{alignedat}
  \end{equation}
  for labels $y_i \in \{ -1, 1\}$ and features $x_i \in \R^d$, $i=1,\dots,n$. 
  Its Lagrangian is
  \[
   L(\beta,\beta_0,\xi,\alpha,u) = 
   \frac{1}{2} \|\beta\|_2^2 + C \sum_{i=1}^n \xi_i +  \sum_{i=1}^n \alpha_i 
   \big(1-\xi_i - y_i (\beta_0 + x_i^\T \beta) \big) - u^\T \xi. 
   \]
  The stationarity condition \eqref{eq:kkt_stationarity} reads
  \index{support vector machine!optimality conditions}
  \begin{equation}
  \label{eq:svm_stationarity}
  \sum_{i=1}^n \alpha_i y_i = 0, \quad 
  \sum_{i=1}^n \alpha_i y_i x_i = \beta, \quad
  \alpha_i + u_i = C, \quad i = 1,\dots,n.
  \end{equation}
  Complementary slackness \eqref{eq:kkt_complementary_slackness} reads   
  \begin{equation}
  \label{eq:svm_complementary_slackness}
    \alpha_i \big(1-\xi_i - y_i (\beta_0 + x_i^\T \beta) \big) = 0, \quad
    u_i \xi_i = 0, \quad i=1,\dots,n. 
  \end{equation}
  Together with primal and dual feasibility, the above two conditions
  characterize SVM solutions. We will see later, in Chapter
  \ref{sec:svm_structure}, that these conditions provide a basis for
  understanding the structure of SVM solutions.   
\end{enumerate}
\end{Example}

\section{Primal-dual relationships}

For a primal-dual pair \eqref{eq:primal_problem2}, \eqref{eq:dual_problem2}
which exhibit strong duality, we know (Theorem \ref{thm:kkt_optimality}) that
the KKT conditions are necessary and sufficient for optimality. The stationarity
condition \eqref{eq:kkt_stationarity} allows us to relate primal $x^\star$ and
dual $u^\star, v^\star$ solutions. Rephrased, this condition says that 
\index{KKT conditions!primal-dual relationship}
\begin{equation}
\label{eq:kkt_stationarity2}
\text{$x^\star$ minimizes $L(\cdot, u^\star, v^\star)$}.
\end{equation}
As demonstrated in the examples that follow, we can often rearrange
\eqref{eq:kkt_stationarity2} to form a \emph{primal-dual relationship}, which is
an equation that relates primal and dual solutions. Further, if $L(\cdot,
u^\star, v^\star)$ has a unique minimizer, then this relationship uniquely
specifies $x^\star$ as a function of $u^\star, v^\star$. 

Primal-dual relationships can be useful for two main reasons. First, in the case
that it uniquely specifies the primal solution $x^\star$, a primal-dual
relationship enables us to solve the primal \eqref{eq:primal_problem2} via the
dual \eqref{eq:dual_problem2}, which is useful in practice when the latter is
easier solve than the former. Second, a primal-dual relationship can provide an
avenue for inferring properties of solutions.

\index{support vector machine!primal-dual relationship}
A clear example of this is given by SVM duality, as studied in Example
\parref{xa:svm_kkt}. The stationarity condition \eqref{eq:svm_stationarity}
immediately reveals that the primal solution \smash{$\hbeta$} can be written in
terms of the dual solution via \smash{$\hbeta = \sum_{i=1}^n \halpha_i y_i
  x_i$}. Generally, some coordinates of the dual solution will be equal to
zero, \smash{$\halpha_i = 0$}, by the nonnegativity constraint in
\eqref{eq:svm_dual}; the corresponding feature vectors $x_i$ and labels $y_i$
play no role in shaping \smash{$\hbeta$}, which determines the orientation of
the linear SVM classifier. Meanwhile, the coordinates for which
\smash{$\halpha_i \not= 0$} index what are called \emph{support points} $x_i$
and $y_i$ of the classifier; we revisit this in more detail in Chapter
\ref{sec:svm_structure}.      

For the lasso, as studied in Example \parref{xa:lasso_kkt}, the problem
\eqref{eq:lasso_primal2} has no constraints and thus no associated dual   
variables. As written, the stationarity condition \eqref{eq:lasso_optimality}
relates the primal solution to itself, and not to dual variables. In order to
make progress toward a primal-dual relationship, we can introduce auxiliary
equality constraints in the lasso problem.

\begin{Example}
As in Exercise \ref{ex:lasso_dual}, consider writing the lasso problem
\eqref{eq:lasso_primal2} as 
\[
\minimize_{\beta,z} \quad \frac{1}{2} \|y - z\|_2^2 + \lambda \|\beta\|_1  
\quad \st \quad z = X \beta.
\]
This allows us to write its Lagrangian as     
\[
L(\beta,z,u) = \frac{1}{2} \|y - z\|_2^2 + \lambda \|\beta\|_1 + u^\T (z - X
\beta).
\]
The stationarity condition \eqref{eq:kkt_stationarity} now reads
\[
z = y-u, \quad X^\T u \in \lambda \partial \|\beta\|_1.
\]
By the first equation above, as \smash{$\hat{z} = X \hbeta$} at the solution, 
we have the primal-dual relationship    
\index{lasso!primal-dual relationship}
\begin{equation}
\label{eq:lasso_primal_dual}
X \hbeta = y - \hat{u}.
\end{equation}
Any lasso solution \smash{$\hbeta$} in \eqref{eq:lasso_primal2} (which is not
necessarily unique, if $X$ has linearly dependent columns) and the dual solution 
\smash{$\hat{u}$} in \eqref{eq:lasso_dual} (which is unique since the dual
criterion is strictly concave) must together satisfy
\eqref{eq:lasso_primal_dual}. Furthermore, by straightforward rearrangement, the
dual problem \eqref{eq:lasso_dual} is equivalent to  
\index{lasso!dual problem} 
\[
\minimize_u \quad \|y - u\|_2^2 \quad \st \quad \|X^\T u\|_\infty \leq \lambda.
\]
We can hence write the dual solution as \smash{$\hat{u} = P_C(y)$}, the
projection of $y$ onto the convex set
\[
C = \{ u : \|X^\T u\|_\infty \leq \lambda \}. 
\]
From \eqref{eq:lasso_primal_dual}, we infer that \smash{$X \hbeta =
  (I-P_C)(y)$}, which is the residual from projection onto $C$. As the residual 
from projection onto a convex set is nonexpansive
\eqref{eq:projection_residual_nonexpansiveness}, writing \smash{$X 
\hbeta$}---called the lasso \emph{fitted vector} (or simply the lasso
\emph{fit})---in this form leads to the following conclusion about
smoothness. For any two response vectors $y, y' \in \R^n$ and the corresponding
lasso fitted vectors \smash{$X \hbeta(y), X \hbeta(y') \in \R^n$}, we have
\begin{equation}
\label{eq:lasso_nonexpansive}
\|X \hbeta(y) - X \hbeta(y')\|_2 \leq \|y - y'\|_2.
\end{equation}
That is, the lasso fit \smash{$X \hbeta(y)$} is Lipschitz continuous as a
function of $y$ (with Lipschitz constant $L = 1$), and therefore differentiable
almost everywhere (by Rademacher's theorem).    
\end{Example}

\section{Stationarity and convexity*}

The KKT conditions \eqref{eq:kkt_stationarity}--\eqref{eq:kkt_dual_feasibility}
do not assume anything about convexity of the primal problem
\eqref{eq:primal_problem2}. As conveyed in the theory developed above (Theorem  
\ref{thm:saddle_point_optimality}, Lemma \ref{lem:saddle_point_kkt}, Theorem
and \ref{thm:kkt_optimality}), these conditions characterize primal-dual solutions
in general optimization problems. The arguments that underlie this theory are
elementary; however, they do rely heavily on the theory of Lagrangian duality
developed in the last chapter.   

Convexity is nonetheless a centrally important consideration in most uses of the
KKT optimality conditions. For one, they are only necessary under strong
duality, which is understood to hold most generally for convex problems
(Slater's condition, Theorem \ref{thm:slater_condition}). That is, optimization
problems in which strong duality does not hold can have solutions which violate  
the KKT conditions, and if we rely on the KKT conditions to characterize
optimality in such problems, then we may miss some or all solutions entirely.        

Furthermore, the stationarity condition \eqref{eq:kkt_stationarity} has a
well-known equivalent form under convexity. This condition expresses the fact
that \smash{$\bar{x}$} minimizes the $x$ argument of the Lagrangian with the
other arguments fixed at \smash{$\bar{u}, \bar{v}$}, and it does so using the
language of subgradients. When the primal criterion function $f$ and constraint
functions $h_i$, $i=1,\dots,m$ and $\ell_j$, $j=1,\dots,k$ are each convex, and 
their domains have relative interiors with nonempty intersection, we can split
up the subgradient of the Lagrangian with respect to $x$ into a sum of
subgradients (Property \parref{par:subgradient_sum}), rewriting the stationarity
condition \eqref{eq:kkt_stationarity} as   
\begin{equation}
\label{eq:kkt_stationarity3}
0 \in \partial f(\bar{x}) + \sum_{i=1}^m \bar{u}_i \partial h_i(\bar{x}) +
\sum_{j=1}^k \bar{v}_j \partial \ell_j(\bar{x}).
\end{equation}
If in addition the primal criterion and constraint functions are each
differentiable, then subgradients reduce to gradients (Theorem
\ref{thm:subgradient_uniqueness}), and we can rewrite  
\eqref{eq:kkt_stationarity3} once more as  
\begin{equation}
\label{eq:kkt_stationarity4}
0 = \nabla f(\bar{x}) + \sum_{i=1}^m \bar{u}_i \nabla h_i(\bar{x}) +
\sum_{j=1}^k \bar{v}_j \nabla \ell_j(\bar{x}).
\end{equation}
The forms \eqref{eq:kkt_stationarity3} and especially
\eqref{eq:kkt_stationarity4} are widely-used to express stationarity in
references on the KKT optimality conditions.  

One must be careful when interpreting \eqref{eq:kkt_stationarity3} or
\eqref{eq:kkt_stationarity4} on their own, absent their connection to   
\eqref{eq:kkt_stationarity}. For general $f$, $h_i$, $i=1,\dots,m$, $\ell_j$, 
$j=1,\dots,k$, we only know (Property \parref{par:subgradient_sum}) that   
\[
\partial f(\bar{x}) + \sum_{i=1}^m \bar{u}_i \partial h_i(\bar{x}) +
\sum_{j=1}^k \bar{v}_j \partial \ell_j(\bar{x}) \subseteq \partial_x \bigg( f(x) 
+ \sum_{i=1}^m \bar{u}_i h_i(x) + \sum_{j=1}^k \bar{v}_j \partial \ell_j(x)
\bigg) \bigg|_{x = \bar{x}}, 
\]
and therefore \eqref{eq:kkt_stationarity3} implies \eqref{eq:kkt_stationarity}
but not the other way around. Meanwhile, nonconvex functions can have gradients
that are not subgradients, so \eqref{eq:kkt_stationarity4} is not even
sufficient for \eqref{eq:kkt_stationarity} in general.   

For differentiable nonconvex problems, the alternative stationarity condition
\eqref{eq:kkt_stationarity4}, combined with rest of the KKT conditions 
\eqref{eq:kkt_complementary_slackness}--\eqref{eq:kkt_dual_feasibility}, are 
often examined under what are called \emph{constraint qualification (CQ)} 
conditions. (Slater's condition is an example of CQ, but there are many others.)
CQ is used to ensure that \eqref{eq:kkt_stationarity4} and
\eqref{eq:kkt_complementary_slackness}--\eqref{eq:kkt_dual_feasibility} are   
necessary for optimality. Hence \eqref{eq:kkt_stationarity4} and
\eqref{eq:kkt_complementary_slackness}--\eqref{eq:kkt_dual_feasibility} are 
sometimes called the KKT \emph{necessary} conditions in the literature on
differentiable nonconvex programming. Sufficient conditions in this setting
often involve additional second-order statements. The chapter notes provide some
further discussion. 

\section{Constrained and penalized forms*}

\SkipTocEntry\section*{Chapter notes}

What does 

Bertsekas have to say?
Rockafellar have to say?
Rockafellar and Wets have to say?

Big admission that we are using KKT to mean what others will not call KKT. They
mean 10.19 and the rest.

Describe what we mentioned in 10.4. Boyd and Van is an example of this.

What setting did Kuhn and Tucker study in their original paper? What about
Karush? 
 
Refs....

W. KARUSH (1939), Minima of Functions of Several Variables with Inequalities as
Side Constraints, M. Sc. Dissertation, Dept. of Mathematics, University of
Chicago 

H. W. KUHN and A. W. TUCKER (1951), Nonlinear programming; in
J. Neyman (Ed.), Proceedings of the Second Berkeley Symposium on Mathematical
Statistics and Probability, Univ. of California Press, Berkeley, 481-492. 

Kjeldsen, Tinne Hoff. A contextualized historical analysis of the Kuhn–Tucker
theorem in nonlinear programming: the impact of world war II. Historia
mathematica, 27(4): 331–361, 2000. 

 John (1948) 

% Ideas for advanced topics or exercises: 
% Fritz John conditions
% Invex functions
% Second-order conditions

\begin{xcb}{Exercises}
\begin{enumerate}[label=\thechapter.\arabic*]
\settowidth{\leftmargini}{00.00.\hskip\labelsep}
\item 

\item \label{ex:lasso_dantzig}
Compare these two? And how about basis pursuit?

\item graphical lasso and covariance thresholding

\item \label{ex:simplex_projection}

\end{enumerate}
\end{xcb}