\chapter{Karush-Kuhn-Tucker conditions}
\label{chap:kkt_conditions}

\section{Saddle point condition}
\label{sec:saddle_point_condition}

In this chapter, we will explore conditions which characterize solutions in
constrained optimization problems, the most well-known being a set of conditions
known as the Karush-Kuhn-Tucker (KKT) conditions. This flows naturally from our 
study of Lagrange duality in the last chapter. In fact, the last chapter already
established a key relationship between primal and dual solutions and what 
are called saddle points of the Lagrangian. These arguments were given in 
passing and here we revisit them as they will lay the foundation for the KKT 
conditions, which we cover next.           

As in the last chapter, consider a primal problem
\begin{equation}
\label{eq:primal_problem2}
\begin{alignedat}{2}
&\minimize_x \quad && f(x) \\
&\st \quad && h_i(x) \leq 0, \; i=1,\dots,m \\ 
& && \ell_j(x) = 0, \; j=1,\dots,k,
\end{alignedat}
\end{equation}
whose associated Lagrangian and dual function are
\begin{align*}
L(x,u,v) &= f(x) + \sum_{i=1}^m u_i h_i(x) + \sum_{j=1}^k v_j \ell_j(x), \\ 
g(u,v) &= \inf_x \, L(x,u,v), 
\end{align*}
and whose associated dual problem is 
\begin{equation}
\label{eq:dual_problem2}
\maximize_{u,v} \quad g(u,v) \quad \st \quad u \geq 0.
\end{equation}
We do not assume that \eqref{eq:primal_problem2} is convex, though recall, the 
dual problem \eqref{eq:dual_problem2} is always convex. In general, we say that
a triplet \smash{$\bar{x}, \bar{u}, \bar{v}$} is a \emph{saddle point} of the
Lagrangian if  
\index{Lagrangian function!saddle point} 
\begin{equation}
\label{eq:lagrangian_saddle_point2}
L(\bar{x}, u, v) \leq L(\bar{x}, \bar{u}, \bar{v}) \leq L(x, \bar{u}, \bar{v}),
\quad \text{for any $x$ and any $u \geq 0, \, v$}.
\end{equation}
In other words, starting at \smash{$\bar{x}, \bar{u}, \bar{v}$}, the condition
says: if we move the primal variable away from \smash{$\bar{x}$} then $L$ can
only increase; if we move the dual variables away from \smash{$\bar{u},
  \bar{v}$}, then $L$ can only decrease.    

The following is a useful characterization of primal and dual solutions via
saddle points.  

\begin{Theorem}
\label{thm:saddle_point_optimality}
For any primal problem \eqref{eq:primal_problem2} and its dual
\eqref{eq:dual_problem2}, and any primal feasible \smash{$\bar{x}$} and dual
feasible \smash{$\bar{u}, \bar{v}$}, the triplet \smash{$\bar{x}, \bar{u},
  \bar{v}$} is a saddle point of the Lagrangian
\eqref{eq:lagrangian_saddle_point2} if and only if \smash{$\bar{x}$} is primal
optimal, \smash{$\bar{u}, \bar{v}$} are dual optimal, and strong duality holds. 
\end{Theorem}

Though the arguments behind this result were already given in Chapter 
\ref{chap:lagrangian_duality}, it will be helpful to consolidate them here.
First, note that the saddle point condition
\eqref{eq:lagrangian_saddle_point2} has a couple of equivalent forms:
\begin{equation}
\label{eq:lagrangian_saddle_point3}
\sup_{u \geq 0, \, v} \, L(\bar{x}, u, v) = L(\bar{x}, \bar{u}, \bar{v}) =
\inf_x \, L(x, \bar{u}, \bar{v}), 
\end{equation}
as well as
\begin{equation}
\label{eq:lagrangian_saddle_point4}
f(\bar{x}) = L(\bar{x}, \bar{u}, \bar{v}) = g(\bar{u}, \bar{v}).
\end{equation}
The right-hand side in the above display is due to the definition of the dual
function $g$, whereas the left-hand side uses the representation of $f$ as a
supremum of the Lagrangian, from Property \parref{par:max_min}. To prove    
Theorem \ref{thm:saddle_point_optimality} we simply observe that 
\eqref{eq:lagrangian_saddle_point4} is equivalent to \smash{$\bar{x}, \bar{u}, 
  \bar{v}$} being primal and dual solutions with zero duality gap. 

\section{Karush-Kuhn-Tucker conditions}
\label{sec:kkt_conditions}

We say that a triplet \smash{$\bar{x}, \bar{u}, \bar{v}$} is a
\emph{Karush-Kuhn-Tucker (KKT) point} associated with the primal and 
dual pair \eqref{eq:primal_problem2} and \eqref{eq:dual_problem2} if it
satisfies the following conditions: 
\index{KKT conditions}
\begin{alignat}{2}
\label{eq:kkt_stationarity}
&0 \in \partial_x L(\bar{x}, \bar{u}, \bar{v}) \quad
&& \text{(stationarity)} \\
\label{eq:kkt_complementary_slackness} 
&\bar{u}_i h_i(\bar{x}) = 0, \; i = 1,\dots,m \quad 
&& \text{(complementary slackness)} \\
\label{eq:kkt_primal_feasibility}
&h_i(\bar{x}) \leq 0, \; i = 1,\dots,m, \; \text{and} \; 
\ell_j(\bar{x}) = 0, \; j = 1,\dots,k \quad 
&& \text{(primal feasibility)} \\ 
\label{eq:kkt_dual_feasibility}
&\bar{u}_i \geq 0, \; i = 1,\dots,m. \quad
&& \text{(dual feasibility)}
\end{alignat}
To be clear, in \eqref{eq:kkt_stationarity}, called the stationarity condition,
the subdifferential is taken with respect to the $x$ component of the
Lagrangian.  

The next result shows that the KKT conditions offer yet another equivalent
characterization of saddle point condition (together with feasibility).  

\begin{Lemma}
\label{lem:saddle_point_kkt}
For any primal feasible \smash{$\bar{x}$} and dual feasible \smash{$\bar{u}, 
  \bar{v}$}, the saddle point condition \eqref{eq:lagrangian_saddle_point2} is
equivalent to stationarity and complementary slackness,
\eqref{eq:kkt_stationarity} and \eqref{eq:kkt_complementary_slackness}.
\end{Lemma}

The proof is straightforward, if we use the equivalent form of the saddle point
condition \eqref{eq:lagrangian_saddle_point4}. The second equality in
\eqref{eq:lagrangian_saddle_point4} is equivalent to the fact that
\smash{$\bar{x}$} minimizes \smash{$L(\cdot, \bar{u}, \bar{v})$}, which is
equivalent to stationarity \eqref{eq:kkt_stationarity}. The first equality in
\eqref{eq:lagrangian_saddle_point4} is implied by complementary slackness
\eqref{eq:kkt_complementary_slackness}:
\[
L(\bar{x}, \bar{u}, \bar{v}) = f(\bar{x}) + 
\sum_{i=1}^m \underbrace{\bar{u}_i h_i(\bar{x})}_{=\,0} + 
\sum_{j=1}^k \underbrace{\bar{v}_j \ell_j(\bar{x})}_{=\,0} 
= f(\bar{x}),
\]
where we have used primal feasibility. Furthermore, the first equality in
\eqref{eq:lagrangian_saddle_point4} also implies complementary slackness: if
\smash{$f(\bar{x}) = L(\bar{x}, \bar{u}, \bar{v})$}, then (again using primal
and dual feasibility) we conclude that all summands must be zero in the middle
expression of the last display, which leads to
\eqref{eq:kkt_complementary_slackness}. This completes the proof the theorem.

Combining Theorem \ref{thm:saddle_point_optimality} and Lemma
\ref{lem:saddle_point_kkt} yields the following important result. 

\index{KKT conditions!optimality}
\begin{Theorem}
\label{thm:kkt_optimality}
For any primal problem \eqref{eq:primal_problem2} and its dual
\eqref{eq:dual_problem2}, a triplet \smash{$\bar{x}, \bar{u}, \bar{v}$}
satisfies the KKT conditions
\eqref{eq:kkt_stationarity}--\eqref{eq:kkt_dual_feasibility} if and only if 
\smash{$\bar{x}$} is primal optimal, \smash{$\bar{u}, \bar{v}$} are dual
optimal, and strong duality holds. In other words, the KKT conditions are always
sufficient for optimality, and necessary under strong duality.   
\end{Theorem}

\index{subgradient!optimality condition}
\index{Lagrange multiplier condition} 
It is worth noting that the KKT conditions generalize the subgradient optimality
condition \eqref{eq:subgradient_optimality}. That is, if problem
\eqref{eq:primal_problem2} has no constraints ($m = k = 0$), then the Lagrangian
is simply the primal criterion, $L = f$, and the only non-vacuous KKT condition
is stationarity \eqref{eq:kkt_stationarity}, which reduces to
\eqref{eq:subgradient_optimality}. Similarly, the KKT conditions generalize the
Lagrange multiplier condition \eqref{eq:lagrange_multiplier}, which can seen
either because the subgradient optimality condition does, or directly from
stationarity \eqref{eq:kkt_stationarity} with linear equality constraints of the
form $Ax = b$. Exercise ?? explores further connections. 

\begin{Example}
The following examples discuss the KKT conditions for problems of interest. In
each case, the KKT conditions are necessary and sufficient for optimality, as
the problem in question is convex and strong duality holds by Slater's condition
(Theorem \ref{thm:slater_condition}). 

\begin{enumerate}[label=\alph*., ref=\alph*]
\item Consider a QP with equality constraints only, 
  \begin{alignat*}{2}
  &\minimize_x \quad && c^\T x + \frac{1}{2} x^\T Q x \\
  &\st && Ax = b,
  \end{alignat*}
  where $Q \succeq 0$. Its Lagrangian is 
  \[
  L(x,v) = c^\T x + \frac{1}{2} x^\T Q x + v^\T (Ax - b).
  \]
  Complementary slackness \eqref{eq:kkt_complementary_slackness} and dual
  feasibility \eqref{eq:kkt_dual_feasibility} are vacuous due to the lack of 
  inequality constraints. Stationarity \eqref{eq:kkt_stationarity} and primal
  feasibility \eqref{eq:kkt_primal_feasibility} are each linear equations, which
  can be assembled into one combined linear system:
  \index{Karush-Kuhn-Tucker matrix}
  \[
  \begin{bmatrix} Q & A^\T \\ A & 0 \end{bmatrix}  
  \begin{bmatrix} x \\ v \end{bmatrix} =
  \begin{bmatrix} -c \\ b \end{bmatrix}.
  \]
  The above matrix is often called the \emph{KKT matrix} associated with our
  original QP. 

\item \parlab{xa:lasso_kkt}
  Consider the lasso problem
  \begin{equation}
  \label{eq:lasso_primal2}
  \minimize_\beta \quad \frac{1}{2} \|y - X \beta\|_2^2 + \lambda \|\beta\|_1, 
  \end{equation}
  for a response vector $y \in \R^n$ and feature matrix $X \in \R^{n \times
    d}$. Since this problem has no constraints, the KKT conditions reduce to  
  stationarity (which, as discussed above, is just the subgradient optimality 
  condition):   
  \index{lasso!optimality conditions}
  \begin{equation}
  \label{eq:lasso_optimality}
  X^\T (y - X\beta) = \lambda s, \quad \text{where $s \in \partial
    \|\beta\|_1$}. 
  \end{equation}
  This condition characterizes lasso solutions. We will see later, in Chapter
  \ref{sec:lasso_structure}, that it provides a way to understand the structure
  of lasso solutions.    
  
\item \parlab{xa:svm_kkt}
  Consider the SVM problem
  \begin{equation}
  \label{eq:svm_primal2}
  \begin{alignedat}{2}
  &\minimize_{\beta_0,\beta,\xi} \quad
  && \frac{1}{2} \|\beta\|_2^2 + C \sum_{i=1}^n \xi_i \\ 
  &\st \quad && y_i (\beta_0 + x_i^\T \beta) \geq 1-\xi_i, \;  i=1,\dots,n \\ 
  & && \xi \geq 0,
  \end{alignedat}
  \end{equation}
  for labels $y_i \in \{ -1, 1\}$ and features $x_i \in \R^d$, $i=1,\dots,n$. 
  Its Lagrangian is
  \[
   L(\beta,\beta_0,\xi,\alpha,u) = 
   \frac{1}{2} \|\beta\|_2^2 + C \sum_{i=1}^n \xi_i +  \sum_{i=1}^n \alpha_i 
   \big(1-\xi_i - y_i (\beta_0 + x_i^\T \beta) \big) - u^\T \xi. 
   \]
  The stationarity condition \eqref{eq:kkt_stationarity} reads
  \index{support vector machine!optimality conditions}
  \begin{equation}
  \label{eq:svm_stationarity}
  \sum_{i=1}^n \alpha_i y_i = 0, \quad 
  \sum_{i=1}^n \alpha_i y_i x_i = \beta, \quad
  \alpha_i + u_i = C, \quad i = 1,\dots,n.
  \end{equation}
  Complementary slackness \eqref{eq:kkt_complementary_slackness} reads   
  \begin{equation}
  \label{eq:svm_complementary_slackness}
    \alpha_i \big(1-\xi_i - y_i (\beta_0 + x_i^\T \beta) \big) = 0, \quad
    u_i \xi_i = 0, \quad i=1,\dots,n. 
  \end{equation}
  Together with primal and dual feasibility, the above two conditions
  characterize SVM solutions. We will see later, in Chapter
  \ref{sec:svm_structure}, that these conditions provide a basis for
  understanding the structure of SVM solutions.   
\end{enumerate}
\end{Example}

\section{Primal-dual relationships}

For a primal-dual pair \eqref{eq:primal_problem2}, \eqref{eq:dual_problem2}
which exhibit strong duality, we know (Theorem \ref{thm:kkt_optimality}) that
the KKT conditions are necessary and sufficient for optimality. The stationarity
condition \eqref{eq:kkt_stationarity} allows us to relate primal $x^\star$ and
dual $u^\star, v^\star$ solutions. Rephrased, this condition says that 
\index{KKT conditions!primal-dual relationship}
\begin{equation}
\label{eq:kkt_stationarity2}
\text{$x^\star$ minimizes $L(\cdot, u^\star, v^\star)$}.
\end{equation}
As demonstrated in the examples that follow, we can often rearrange
\eqref{eq:kkt_stationarity2} to form a \emph{primal-dual relationship}, which is
an equation that relates primal and dual solutions. Further, if $L(\cdot,
u^\star, v^\star)$ has a unique minimizer, then this relationship uniquely
specifies $x^\star$ as a function of $u^\star, v^\star$. 

Primal-dual relationships can be useful for two main reasons. First, in the case
that it uniquely specifies the primal solution $x^\star$, a primal-dual
relationship enables us to solve the primal \eqref{eq:primal_problem2} via the
dual \eqref{eq:dual_problem2}, which is useful in practice when the latter is
easier solve than the former. Second, a primal-dual relationship can provide an
avenue for inferring properties of solutions.

\index{support vector machine!primal-dual relationship}
A clear example of this is given by SVM duality, as studied in Example
\parref{xa:svm_kkt}. The stationarity condition \eqref{eq:svm_stationarity}
immediately reveals that the primal solution \smash{$\hbeta$} can be written in
terms of the dual solution via \smash{$\hbeta = \sum_{i=1}^n \halpha_i y_i
  x_i$}. Generally, some coordinates of the dual solution will be equal to
zero, \smash{$\halpha_i = 0$}, by the nonnegativity constraint in
\eqref{eq:svm_dual}; the corresponding feature vectors $x_i$ and labels $y_i$
play no role in shaping \smash{$\hbeta$}, which determines the orientation of
the linear SVM classifier. Meanwhile, the coordinates for which
\smash{$\halpha_i \not= 0$} index what are called \emph{support points} $x_i$
and $y_i$ of the classifier; we revisit this in more detail in Chapter
\ref{sec:svm_structure}.      

For the lasso, as studied in Example \parref{xa:lasso_kkt}, the problem
\eqref{eq:lasso_primal2} has no constraints and thus no associated dual   
variables. As written, the stationarity condition \eqref{eq:lasso_optimality}
relates the primal solution to itself, and not to dual variables. In order to
make progress toward a primal-dual relationship, we can introduce auxiliary
equality constraints in the lasso problem.

\begin{Example}
As in Exercise \ref{ex:lasso_dual}, consider writing the lasso problem
\eqref{eq:lasso_primal2} as 
\[
\minimize_{\beta,z} \quad \frac{1}{2} \|y - z\|_2^2 + \lambda \|\beta\|_1  
\quad \st \quad z = X \beta.
\]
This allows us to write its Lagrangian as     
\[
L(\beta,z,u) = \frac{1}{2} \|y - z\|_2^2 + \lambda \|\beta\|_1 + u^\T (z - X
\beta).
\]
The stationarity condition \eqref{eq:kkt_stationarity} now reads
\[
z = y-u, \quad X^\T u \in \lambda \partial \|\beta\|_1.
\]
By the first equation above, as \smash{$\hat{z} = X \hbeta$} at the solution, 
we have the primal-dual relationship    
\index{lasso!primal-dual relationship}
\begin{equation}
\label{eq:lasso_primal_dual}
X \hbeta = y - \hat{u}.
\end{equation}
Any lasso solution \smash{$\hbeta$} in \eqref{eq:lasso_primal2} (which is not
necessarily unique, if $X$ has linearly dependent columns) and the dual solution 
\smash{$\hat{u}$} in \eqref{eq:lasso_dual} (which is unique since the dual
criterion is strictly concave) must together satisfy
\eqref{eq:lasso_primal_dual}. Furthermore, by straightforward rearrangement, the
dual problem \eqref{eq:lasso_dual} is equivalent to  
\index{lasso!dual problem} 
\[
\minimize_u \quad \|y - u\|_2^2 \quad \st \quad \|X^\T u\|_\infty \leq \lambda.
\]
We can hence write the dual solution as \smash{$\hat{u} = P_C(y)$}, the
projection of $y$ onto the convex set
\[
C = \{ u : \|X^\T u\|_\infty \leq \lambda \}. 
\]
From \eqref{eq:lasso_primal_dual}, we infer that \smash{$X \hbeta =
  (I-P_C)(y)$}, which is the residual from projection onto $C$. As the residual 
from projection onto a convex set is nonexpansive
\eqref{eq:projection_residual_nonexpansiveness}, writing \smash{$X 
\hbeta$}---called the lasso \emph{fitted vector} (or simply the lasso
\emph{fit})---in this form leads to the following conclusion about
smoothness. For any two response vectors $y, y' \in \R^n$ and the corresponding
lasso fitted vectors \smash{$X \hbeta(y), X \hbeta(y') \in \R^n$}, we have
\begin{equation}
\label{eq:lasso_nonexpansive}
\|X \hbeta(y) - X \hbeta(y')\|_2 \leq \|y - y'\|_2.
\end{equation}
That is, the lasso fit \smash{$X \hbeta(y)$} is Lipschitz continuous as a
function of $y$ (with Lipschitz constant $L = 1$), and therefore differentiable
almost everywhere (by Rademacher's theorem).    
\end{Example}

\section{Convexity, stationarity, constraint qualification*} 

The KKT conditions \eqref{eq:kkt_stationarity}--\eqref{eq:kkt_dual_feasibility}
do not assume anything about convexity of the primal problem
\eqref{eq:primal_problem2}. As conveyed in the theory developed above (Theorem  
\ref{thm:saddle_point_optimality}, Lemma \ref{lem:saddle_point_kkt}, Theorem
and \ref{thm:kkt_optimality}), these conditions characterize primal-dual solutions
in general optimization problems. The arguments that underlie this theory are
elementary; however, they do rely heavily on the theory of Lagrangian duality
developed in the last chapter.   

Convexity is nonetheless a centrally important consideration in most uses of the
KKT optimality conditions. For one, we only know (based on the above theory)
that these conditions are necessary under strong duality, which is itself
understood to hold most generally for convex problems (Slater's condition,
Theorem \ref{thm:slater_condition}). That is, optimization problems in which
strong duality does not hold can have solutions which violate the KKT
conditions; if we rely on the KKT conditions to characterize optimality in such
problems, then we may miss some or all solutions entirely. (See the end of this
section for a brief discussion of constraint qualification, which provides
alternate assumptions under which the KKT conditions remain necessary.)

Furthermore, the stationarity condition \eqref{eq:kkt_stationarity} has a
well-known equivalent form under convexity. This condition expresses the fact
that \smash{$\bar{x}$} minimizes the $x$ argument of the Lagrangian with the
other arguments fixed at \smash{$\bar{u}, \bar{v}$}, and it does so using the
language of subgradients. When the primal criterion function $f$ and constraint
functions $h_i$, $i=1,\dots,m$ and $\ell_j$, $j=1,\dots,k$ are each convex, and 
their domains have relative interiors with nonempty intersection, we can split
up the subgradient of the Lagrangian with respect to $x$ into a sum of
subgradients (Property \parref{par:subgradient_sum}), rewriting the stationarity
condition \eqref{eq:kkt_stationarity} as   
\begin{equation}
\label{eq:kkt_stationarity3}
0 \in \partial f(\bar{x}) + \sum_{i=1}^m \bar{u}_i \partial h_i(\bar{x}) +
\sum_{j=1}^k \bar{v}_j \partial \ell_j(\bar{x}).
\end{equation}
If in addition the primal criterion and constraint functions are each
differentiable, then subgradients reduce to gradients (Theorem
\ref{thm:subgradient_uniqueness}), and we can rewrite  
\eqref{eq:kkt_stationarity3} once more as  
\begin{equation}
\label{eq:kkt_stationarity4}
0 = \nabla f(\bar{x}) + \sum_{i=1}^m \bar{u}_i \nabla h_i(\bar{x}) +
\sum_{j=1}^k \bar{v}_j \nabla \ell_j(\bar{x}).
\end{equation}
The forms \eqref{eq:kkt_stationarity3} and especially
\eqref{eq:kkt_stationarity4} are widely-used to express stationarity in
references on the KKT optimality conditions. However, one must be careful when
interpreting \eqref{eq:kkt_stationarity3} or \eqref{eq:kkt_stationarity4} on
their own, absent their connection to \eqref{eq:kkt_stationarity}. In general,
the conditions \eqref{eq:kkt_stationarity}, \eqref{eq:kkt_stationarity3},
\eqref{eq:kkt_stationarity4} are not equivalent, and \eqref{eq:kkt_stationarity}
is the only one which is equivalent to \smash{$\bar{x}$} minimizing
\smash{$L(\cdot, \bar{u}, \bar{v})$}.    

% we only know (Property
% \parref{par:subgradient_sum}) that    
% \[
% \partial f(\bar{x}) + \sum_{i=1}^m \bar{u}_i \partial h_i(\bar{x}) +
% \sum_{j=1}^k \bar{v}_j \partial \ell_j(\bar{x}) \subseteq \partial_x \bigg( f(x) 
% + \sum_{i=1}^m \bar{u}_i h_i(x) + \sum_{j=1}^k \bar{v}_j \partial \ell_j(x)
% \bigg) \bigg|_{x = \bar{x}}, 
% \]
% and therefore \eqref{eq:kkt_stationarity3} implies \eqref{eq:kkt_stationarity}
% but not the other way around. Meanwhile, nonconvex functions can have gradients
% that are not subgradients, so \eqref{eq:kkt_stationarity4} is not even
% sufficient for \eqref{eq:kkt_stationarity} in general.   

For differentiable nonconvex problems, the gradient-based stationarity condition
\eqref{eq:kkt_stationarity4} is often studied in combination with the rest of
the KKT conditions
\eqref{eq:kkt_complementary_slackness}--\eqref{eq:kkt_dual_feasibility}. As
\smash{$\nabla_x L(\bar{x}, \bar{u}, \bar{v}) = 0$} does not imply
\smash{$\bar{x}$} minimizes \smash{$L(\cdot, \bar{u}, \bar{v})$} in this
setting, extra (second-order) conditions are used to certify optimality. On the
other hand, under strong duality, optimality implies
\eqref{eq:kkt_stationarity4}: if \smash{$L(\cdot, \bar{u}, \bar{v})$} attains
its unconstrained infimum at \smash{$\bar{x}$}, then it must have zero gradient
at \smash{$\bar{x}$} (assuming it is continuously differentiable on an open
neighborhood of this point). Complementary slackness and feasibility are still
implied under strong duality, as argued earlier. Hence
\eqref{eq:kkt_stationarity4} and
\eqref{eq:kkt_complementary_slackness}--\eqref{eq:kkt_dual_feasibility} are
sometimes called the KKT \emph{necessary} conditions in the literature on
differentiable nonconvex programming, as they are necessary for optimality under
strong duality. More broadly, they are necessary under what are called
\emph{constraint qualification (CQ)} conditions in this literature. Slater's
condition is an example of CQ, but there are several others, such as
\emph{linear independence CQ (LICQ)}. This states:   
\begin{multline}
\label{eq:linear_independence_cq} 
\text{the gradients of active inequality constraints and linear
  inequality constraints at $\bar{x}$:} \\ \text{$\nabla h_i(\bar{x})$ for all 
  $i$ such that $h_i(\bar{x}) = 0$ and $\nabla \ell_j(\bar{x})$ for $j =
  1,\dots,k$, are linearly independent.}   
\end{multline}
If \smash{$\bar{x}, \bar{u}, \bar{v}$} are primal and dual optimal and LICQ
\eqref{eq:linear_independence_cq} holds at \smash{$\bar{x}$}, then (assuming all
functions are continuously differentiable in an open neighborhood of
\smash{$\bar{x}$}) the conditions \eqref{eq:kkt_stationarity4},
\eqref{eq:kkt_complementary_slackness}--\eqref{eq:kkt_dual_feasibility} must 
hold. The chapter notes provide some further discussion.

% For example, Theorem 2.3.8 and Exercise 3 in Borwein and Lewis (2006) 

\section{Constrained and penalized equivalences*}

In statistics and machine learning, we often encounter penalized problems of the
form  
\begin{equation}
\label{eq:penalized_problem}
\minimize_\theta \quad \ell(\theta) + \lambda r(\theta),
\end{equation}
for a loss function $\ell$ (such as the negative log likelihood in a parametric
model), regularizer $r$ (such as a norm or seminorm), and tuning parameter
$\lambda \geq 0$. A common claim is that \eqref{eq:penalized_problem} is
equivalent to the constrained problem
\begin{equation}
\label{eq:constrained_problem}
\minimize_\theta \quad \ell(\theta) \quad \st \quad r(\theta) \leq t, 
\end{equation}
where $t \in \R$ is another tuning parameter. In what follows, we formalize this
claim using the KKT conditions. Fixing any $\lambda \geq 0$, we first show that   
any solution \smash{$\htheta$} in the penalized problem
\eqref{eq:penalized_problem} is a solution in the constrained problem 
\eqref{eq:constrained_problem}, for an appropriate choice of $t$. In
particular, we define \smash{$t = r(\htheta)$}, which we assume to be finite. We 
now simply check that \smash{$\htheta, \lambda$} satisfy the KKT conditions for  
\eqref{eq:constrained_problem}: stationarity holds because \smash{$\htheta$} 
minimizes the Lagrangian
\begin{equation}
\label{eq:constrained_lagrangian}
L(\theta, \lambda) = \ell(\theta) + \lambda (r(\theta) - t)
\end{equation}
over $\theta$; complementary slackness holds because \smash{$r(\htheta) = t$};
and primal and dual feasibility clearly hold as well. The KKT conditions being
sufficient for optimality, this implies that \smash{$\htheta$} solves
\eqref{eq:constrained_problem}.        

For the other direction: fixing any $t \in \R$, we will show that under strong
duality any solution \smash{$\htheta$} in \eqref{eq:constrained_problem} is a
solution in \eqref{eq:penalized_problem}, for an appropriate choice of
$\lambda$. In particular, we let $\lambda \geq 0$ be any solution---assumed to   
be attained and finite---in the dual of \eqref{eq:constrained_problem}. Since
the KKT conditions are necessary for optimality under strong duality, we know by  
stationarity that \smash{$\htheta$} must minimize $L(\theta, \lambda)$ in
\eqref{eq:constrained_lagrangian} over $\theta$, which implies it must solve    
\eqref{eq:penalized_problem}.   

We summarize the above conclusions into the following theorem.

\begin{Theorem}
For functions $\ell,r : \R^d \to [-\infty, \infty]$, and $\lambda,t \in \R$, the
penalized \eqref{eq:penalized_problem} and constrained
\eqref{eq:constrained_problem} problems admit the following equivalences.     

\begin{enumerate}[label=(\roman*)]
\item If \smash{$\htheta$} solves \eqref{eq:penalized_problem} and \smash{$t =
    r(\htheta) < \infty$}, then \smash{$\htheta$} solves
  \eqref{eq:constrained_problem}. 

\item If strong duality holds, which, for example, is implied by Slater's 
  condition (convexity of $\ell,r$ and $\relint(\dom(\ell) \cap \dom(r)) \cap \{
  \theta : r(\theta) < t\} \not= \emptyset$), and if \smash{$\htheta$} solves
  \eqref{eq:constrained_problem} and $\lambda \in [0,\infty)$ solves its dual
  problem, then \smash{$\htheta$} solves \eqref{eq:penalized_problem}.     
\end{enumerate} 
\end{Theorem}

It is worth noting the asymmetry in the above results---essentially no
conditions are needed to show that penalized solutions are constrained solutions
(only finiteness of the implied constrained tuning parameter value \smash{$t = 
  r(\htheta)$}), whereas strong duality is used to show constrained solutions
are penalized solutions. Exercise ?? [REALLY?] further explores the relationship
between penalized and constrained forms.   

\SkipTocEntry\section*{Chapter notes}

What does 

Bertsekas have to say? Don't really see anything KKT like. Does cover saddle
point theory. 

Rockafellar have to say?
Theorem 28.1 gives necessity of KKT conditions for convex programs
Theorem 28.2 gives sufficiency.
Theorem 28.3 gives the connection to saddle points.
Our treatment is in line with his. 

Rockafellar and Wets have to say? Don't really see anything KKT like. Theorem
8.15 does give general necessary conditions for constraied local minima of a 
proper lower semicontinuous functions in terms of subgradients and normal
cones. Theorem 10.1 is a generalized Fermat's rule. 

(Big admission that we are using KKT to mean what others will not call KKT. They 
mean 10.19 and the rest? Not sure... Our treatment is in line with Rock 1970.) 

Describe what we mentioned in 10.4. Boyd and Van is an example of this.

What setting did Kuhn and Tucker study in their original paper? Saddle points. 
Differentiable functions. Necessary and sufficient conditions for convex
functions. Hint at subgradients and convexity but do not use this language (had
not really been developed yet.)

What about Karush? Calculus of variations focus. But proves the same result.  
 
% Exercise ?? REALLY?] demonstrates that CQ conditions do not necessarily
% imply strong duality.  

% RJT: I don't think CQ implies strong duality in general. I can't find any
% concrete references which suggest that, except Boyd's notes ...

Refs....

W. KARUSH (1939), Minima of Functions of Several Variables with Inequalities as
Side Constraints, M. Sc. Dissertation, Dept. of Mathematics, University of
Chicago 

H. W. KUHN and A. W. TUCKER (1951), Nonlinear programming; in
J. Neyman (Ed.), Proceedings of the Second Berkeley Symposium on Mathematical
Statistics and Probability, Univ. of California Press, Berkeley, 481-492. 

Kuhn, H. W. [1976], Nonlinear programming: a historical view, in Nonlinear
Programming, edited by R. W. Cottle and C. E. Lemke. Found out in 1974 from
Takayama. 

Kjeldsen, Tinne Hoff. A contextualized historical analysis of the Kuhn–Tucker
theorem in nonlinear programming: the impact of world war II. Historia
mathematica, 27(4): 331–361, 2000. 

 John (1948) 

% Ideas for advanced topics or exercises: 
% Fritz John conditions
% Invex functions
% Second-order conditions

\begin{xcb}{Exercises}
\begin{enumerate}[label=\thechapter.\arabic*]
\settowidth{\leftmargini}{00.00.\hskip\labelsep}
\item Use subgradient sum property, under slater, to derive KKT conditions from
  subgradient optimality?

\item 

\item \label{ex:lasso_dantzig}
Compare these two? And how about basis pursuit?

\item graphical lasso and covariance thresholding

\item \label{ex:simplex_projection}

\item Show LICQ does not imply strong duality. 

\end{enumerate}
\end{xcb}